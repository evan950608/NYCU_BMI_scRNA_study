nohup: ignoring input
Representative adata: (57515, 27504) <class 'scipy.sparse._csc.csc_matrix'>
all cell types: ['ASDC', 'B_intermediate', 'B_memory', 'B_naive', 'CD14_Mono', 'CD16_Mono', 'CD4_CTL', 'CD4_Naive', 'CD4_Proliferating', 'CD4_TCM', 'CD4_TEM', 'CD8_Naive', 'CD8_Proliferating', 'CD8_TCM', 'CD8_TEM', 'Doublet', 'Eryth', 'HSPC', 'ILC', 'MAIT', 'NK', 'NK_CD56bright', 'NK_Proliferating', 'Plasmablast', 'Platelet', 'Treg', 'cDC1', 'cDC2', 'dnT', 'gdT', 'pDC']
====================
Queue ['Doublet', 'Eryth', 'HSPC', 'ILC', 'MAIT', 'NK', 'NK_CD56bright', 'NK_Proliferating', 'Plasmablast', 'Platelet', 'Treg', 'cDC1', 'cDC2', 'dnT', 'gdT', 'pDC']
====================
***** Starting tuning
Representative adata: (57515, 27504)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for Doublet
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda:Lambda: Lambda:Lambda:  1e-05 Lambda:Lambda:Lambda:  Lambda:Lambda: Lambda:3.2e-05 Lambda:1.5e-05 Lambda:Lambda:starting at Lambda: Lambda: 0.0001 2.2e-05  0.000215  Lambda: Lambda:starting at Lambda: Lambda:Lambda:Lambda:starting at Lambda: Lambda:Lambda: Lambda:2024-10-02 17:17:36  4.6e-05  6.8e-05 starting at starting at 0.000147 starting at 0.000316 0.003162  2024-10-02 17:17:36  0.000464    2024-10-02 17:17:36  0.000681   0.001  Max_iter: 0.001468 starting at 0.002154 starting at 2024-10-02 17:17:36 2024-10-02 17:17:36 starting at 2024-10-02 17:17:36 starting at starting at 0.004642 Max_iter: 0.006813 starting at 0.01 0.014678 0.021544 Max_iter: 0.031623 starting at 0.046416 0.068129 starting at 0.1 1000starting at 2024-10-02 17:17:36 starting at 2024-10-02 17:17:36 Max_iter: Max_iter: 2024-10-02 17:17:36 Max_iter: 2024-10-02 17:17:36 2024-10-02 17:17:36 starting at 1000
starting at 2024-10-02 17:17:36 starting at starting at starting at 1000
starting at 2024-10-02 17:17:36 starting at starting at 2024-10-02 17:17:36 starting at 
2024-10-02 17:17:36 Max_iter: 2024-10-02 17:17:36 Max_iter: 10001000Max_iter: 1000Max_iter: Max_iter: 2024-10-02 17:17:36 2024-10-02 17:17:36 Max_iter:2024-10-02 17:17:36 2024-10-02 17:17:36 2024-10-02 17:17:36 2024-10-02 17:17:36 Max_iter:2024-10-02 17:17:36 2024-10-02 17:17:36Max_iter:2024-10-02 17:17:36Max_iter: 1000Max_iter:1000

1000
10001000Max_iter: Max_iter:  1000Max_iter: Max_iter: Max_iter: Max_iter:  1000Max_iter:  Max_iter: 1000 Max_iter:1000

 1000



1000
1000
1000
10001000
1000

1000
 
 1000


1000

At iteration 151, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  152 ; minimum lost =  0.11689342558383942 ; diff loss =  5.960464477539062e-07 ; diff weight =  0.00035202503204345703
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.10000000000000002, cost : 30.882 min
==========
At iteration 153, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  154 ; minimum lost =  0.06987133622169495 ; diff loss =  4.470348358154297e-08 ; diff weight =  0.00014019012451171875
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.0316227766016838, cost : 31.146 min
==========
At iteration 159, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  160 ; minimum lost =  0.09595651924610138 ; diff loss =  5.885958671569824e-07 ; diff weight =  0.00041031837463378906
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.0681292069057962, cost : 32.537 min
==========
At iteration 161, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  162 ; minimum lost =  0.08076223731040955 ; diff loss =  7.227063179016113e-07 ; diff weight =  0.0005028843879699707
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.04641588833612786, cost : 32.966 min
==========
At iteration 166, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  167 ; minimum lost =  0.06223186478018761 ; diff loss =  8.903443813323975e-07 ; diff weight =  0.0012932654935866594
lambda is : 0.02154434690031885, cost : 33.664 min
==========
At iteration 168, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  169 ; minimum lost =  0.056766364723443985 ; diff loss =  1.862645149230957e-08 ; diff weight =  6.383886648109183e-05
lambda is : 0.014677992676220709, cost : 33.793 min
==========
At iteration 170, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  171 ; minimum lost =  0.052537355571985245 ; diff loss =  9.872019290924072e-07 ; diff weight =  0.0014072733465582132
lambda is : 0.010000000000000004, cost : 34.666 min
==========
At iteration 242, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  243 ; minimum lost =  0.043909747153520584 ; diff loss =  7.115304470062256e-07 ; diff weight =  0.0014697026927024126
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.004641588833612781, cost : 48.094 min
==========
At iteration 264, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  265 ; minimum lost =  0.04796982556581497 ; diff loss =  9.5367431640625e-07 ; diff weight =  0.001909733982756734
lambda is : 0.006812920690579613, cost : 51.736 min
==========
At iteration 306, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  307 ; minimum lost =  0.03983860835433006 ; diff loss =  9.98377799987793e-07 ; diff weight =  0.0015961735043674707
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.003162277660168382, cost : 60.483 min
==========
At iteration 319, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  320 ; minimum lost =  0.021337401121854782 ; diff loss =  8.139759302139282e-07 ; diff weight =  0.0005616366979666054
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 9.999999999999991e-05, cost : 62.657 min
==========
At iteration 328, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  329 ; minimum lost =  0.03295036405324936 ; diff loss =  7.450580596923828e-07 ; diff weight =  0.0040532322600483894
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0014677992676220694, cost : 64.181 min
==========
At iteration 332, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  333 ; minimum lost =  0.01667691208422184 ; diff loss =  7.860362529754639e-07 ; diff weight =  0.01586761884391308
lambda is : 3.16227766016838e-05, cost : 64.764 min
==========
At iteration 335, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  336 ; minimum lost =  0.036093562841415405 ; diff loss =  9.611248970031738e-07 ; diff weight =  0.005603269208222628
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0021544346900318843, cost : 65.07 min
==========
At iteration 345, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  346 ; minimum lost =  0.024805642664432526 ; diff loss =  5.755573511123657e-07 ; diff weight =  0.004017116967588663
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00031622776601683783, cost : 67.129 min
==========
At iteration 343, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  344 ; minimum lost =  0.028155691921710968 ; diff loss =  7.599592208862305e-07 ; diff weight =  0.0013559957733377814
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0006812920690579617, cost : 67.223 min
==========
At iteration 343, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  344 ; minimum lost =  0.020002074539661407 ; diff loss =  8.512288331985474e-07 ; diff weight =  0.0004411716654431075
lambda is : 6.81292069057961e-05, cost : 67.78 min
==========
At iteration 344, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  345 ; minimum lost =  0.02635628916323185 ; diff loss =  7.7858567237854e-07 ; diff weight =  0.004982678685337305
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00046415888336127795, cost : 68.077 min
==========
At iteration 355, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  356 ; minimum lost =  0.012773239985108376 ; diff loss =  8.558854460716248e-07 ; diff weight =  0.0003765481524169445
lambda is : 1.4677992676220687e-05, cost : 70.134 min
==========
At iteration 361, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  362 ; minimum lost =  0.02219381555914879 ; diff loss =  2.905726432800293e-07 ; diff weight =  0.0018310482846572995
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00014677992676220703, cost : 70.417 min
==========
At iteration 364, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  365 ; minimum lost =  0.023456567898392677 ; diff loss =  9.238719940185547e-07 ; diff weight =  0.0005890977336093783
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 71.817 min
==========
At iteration 383, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  384 ; minimum lost =  0.030262485146522522 ; diff loss =  9.98377799987793e-07 ; diff weight =  0.0024320478551089764
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0010000000000000002, cost : 74.146 min
==========
At iteration 395, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  396 ; minimum lost =  0.014227783307433128 ; diff loss =  2.514570951461792e-07 ; diff weight =  0.016655808314681053
lambda is : 2.1544346900318854e-05, cost : 77.102 min
==========
At iteration 431, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  432 ; minimum lost =  0.017497610300779343 ; diff loss =  5.699694156646729e-07 ; diff weight =  0.000279366213362664
lambda is : 4.6415888336127784e-05, cost : 83.113 min
==========
At iteration 469, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  470 ; minimum lost =  0.009727842174470425 ; diff loss =  9.34116542339325e-07 ; diff weight =  0.0004641944251488894
lambda is : 9.999999999999997e-06, cost : 88.746 min
==========
*** Collecting results ***
Exporting result Dict
Doublet Time elapsed: 88.8198120633761 minutes.
Representative adata: (57515, 27504)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for Eryth
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda: Lambda:Lambda:1e-05 Lambda:Lambda: Lambda:Lambda: Lambda:starting at Lambda:  Lambda:Lambda:Lambda: Lambda:1.5e-05 Lambda: Lambda: Lambda:Lambda:2.2e-05 Lambda: Lambda:Lambda:2024-10-02 18:46:31 Lambda:Lambda:Lambda:Lambda:0.000215 3.2e-05 Lambda:   4.6e-05  0.000464starting at  0.000681  6.8e-05   starting at  0.0001   Max_iter:     starting at starting at  0.068129 0.000147 0.000316 starting at  starting at2024-10-02 18:46:31 0.001 starting at 0.001468 starting at 0.002154 0.003162 2024-10-02 18:46:31 0.004642 starting at 0.006813 0.01 10000.014678 0.021544 0.031623 0.046416 2024-10-02 18:46:31 2024-10-02 18:46:31 0.1 starting at starting at starting at 2024-10-02 18:46:31  2024-10-02 18:46:31Max_iter: starting at 2024-10-02 18:46:31 starting at 2024-10-02 18:46:31 starting at starting at Max_iter:starting at 2024-10-02 18:46:31 starting at starting at 
starting at starting at starting at starting at Max_iter: Max_iter: starting at 2024-10-02 18:46:31 2024-10-02 18:46:31 2024-10-02 18:46:31 Max_iter:  Max_iter:1000
2024-10-02 18:46:31 Max_iter: 2024-10-02 18:46:31 Max_iter: 2024-10-02 18:46:31 2024-10-02 18:46:31  2024-10-02 18:46:31 Max_iter: 2024-10-02 18:46:31 2024-10-02 18:46:31 2024-10-02 18:46:31 2024-10-02 18:46:31 2024-10-02 18:46:31 2024-10-02 18:46:31 100010002024-10-02 18:46:31Max_iter: Max_iter: Max_iter:1000 Max_iter:1000Max_iter:1000Max_iter: Max_iter:1000Max_iter: 1000Max_iter:Max_iter:Max_iter:Max_iter:Max_iter:Max_iter:

 10001000 
1000 
 1000
1000
 
1000
      Max_iter:

1000
1000
1000
100010001000
1000
1000
1000
 




1000
At iteration 2, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  3 ; minimum lost =  0.008460495620965958 ; diff loss =  8.093193173408508e-07 ; diff weight =  0.015298466198146343
At iteration 2, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  3 ; minimum lost =  0.007681493181735277 ; diff loss =  2.7567148208618164e-07 ; diff weight =  0.006515446584671736
At iteration 2, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  3 ; minimum lost =  0.008001953363418579 ; diff loss =  4.5262277126312256e-07 ; diff weight =  0.009885609149932861
lambda is : 2.1544346900318854e-05, cost : 1.537 min
==========
lambda is : 9.999999999999997e-06, cost : 1.561 min
==========
lambda is : 1.4677992676220687e-05, cost : 1.592 min
==========
At iteration 99, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  100 ; minimum lost =  0.019718637689948082 ; diff loss =  6.51925802230835e-08 ; diff weight =  0.0004894115845672786
lambda is : 0.010000000000000004, cost : 19.704 min
==========
At iteration 108, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  109 ; minimum lost =  0.05265303701162338 ; diff loss =  6.109476089477539e-07 ; diff weight =  0.0005273222923278809
lambda is : 0.04641588833612786, cost : 21.529 min
==========
At iteration 118, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  119 ; minimum lost =  0.02446679212152958 ; diff loss =  1.471489667892456e-07 ; diff weight =  0.0005044303834438324
lambda is : 0.014677992676220709, cost : 23.378 min
==========
At iteration 123, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  124 ; minimum lost =  0.040146615356206894 ; diff loss =  1.8998980522155762e-07 ; diff weight =  0.0003617405891418457
lambda is : 0.0316227766016838, cost : 24.153 min
==========
At iteration 128, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  129 ; minimum lost =  0.015381032600998878 ; diff loss =  1.7601996660232544e-07 ; diff weight =  0.00039723629015497863
lambda is : 0.006812920690579613, cost : 25.213 min
==========
At iteration 128, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  129 ; minimum lost =  0.06966355443000793 ; diff loss =  4.991888999938965e-07 ; diff weight =  0.0004221796989440918
lambda is : 0.0681292069057962, cost : 25.289 min
==========
At iteration 128, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  129 ; minimum lost =  0.031030725687742233 ; diff loss =  5.587935447692871e-09 ; diff weight =  4.2319297790527344e-05
lambda is : 0.02154434690031885, cost : 25.666 min
==========
At iteration 132, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  133 ; minimum lost =  0.09259235113859177 ; diff loss =  3.7997961044311523e-07 ; diff weight =  0.00028228759765625
lambda is : 0.10000000000000002, cost : 25.924 min
==========
At iteration 158, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  159 ; minimum lost =  0.011686403304338455 ; diff loss =  5.038455128669739e-07 ; diff weight =  0.0010251592611894011
lambda is : 0.004641588833612781, cost : 30.49 min
==========
At iteration 161, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  162 ; minimum lost =  0.00886581465601921 ; diff loss =  4.842877388000488e-07 ; diff weight =  0.0025226641446352005
lambda is : 0.003162277660168382, cost : 31.169 min
==========
At iteration 188, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  189 ; minimum lost =  0.0067089619114995 ; diff loss =  6.919726729393005e-07 ; diff weight =  0.003356823930516839
lambda is : 0.0021544346900318843, cost : 36.702 min
==========
At iteration 201, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  202 ; minimum lost =  0.004111009184271097 ; diff loss =  8.582137525081635e-07 ; diff weight =  0.0012511815875768661
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0010000000000000002, cost : 38.479 min
==========
At iteration 235, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  236 ; minimum lost =  0.005131996236741543 ; diff loss =  4.0978193283081055e-07 ; diff weight =  0.0015289000002667308
lambda is : 0.0014677992676220694, cost : 45.997 min
==========
At iteration 246, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  247 ; minimum lost =  0.002159328665584326 ; diff loss =  8.640345185995102e-07 ; diff weight =  0.006403769366443157
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00031622776601683783, cost : 47.49 min
==========
At iteration 256, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  257 ; minimum lost =  0.002578738844022155 ; diff loss =  8.05361196398735e-07 ; diff weight =  0.005815417971462011
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00046415888336127795, cost : 50.034 min
==========
At iteration 259, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  260 ; minimum lost =  0.00317554734647274 ; diff loss =  8.153729140758514e-07 ; diff weight =  0.0016124240355566144
lambda is : 0.0006812920690579617, cost : 50.728 min
==========
At iteration 266, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  267 ; minimum lost =  0.0014475405914708972 ; diff loss =  9.480863809585571e-07 ; diff weight =  0.013960697688162327
lambda is : 9.999999999999991e-05, cost : 51.438 min
==========
At iteration 273, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  274 ; minimum lost =  0.0016151999589055777 ; diff loss =  8.00006091594696e-07 ; diff weight =  0.006659016944468021
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00014677992676220703, cost : 52.674 min
==========
At iteration 271, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  272 ; minimum lost =  0.001834049355238676 ; diff loss =  9.816139936447144e-07 ; diff weight =  0.011829930357635021
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 53.613 min
==========
At iteration 283, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  284 ; minimum lost =  0.0011370953870937228 ; diff loss =  9.260838851332664e-07 ; diff weight =  0.00712742330506444
lambda is : 4.6415888336127784e-05, cost : 55.515 min
==========
At iteration 302, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  303 ; minimum lost =  0.0012554123532027006 ; diff loss =  8.523929864168167e-07 ; diff weight =  0.00858152937144041
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 6.81292069057961e-05, cost : 57.687 min
==========
At iteration 314, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  315 ; minimum lost =  0.0009641972137615085 ; diff loss =  9.727664291858673e-07 ; diff weight =  0.012085292488336563
lambda is : 3.16227766016838e-05, cost : 58.998 min
==========
*** Collecting results ***
Exporting result Dict
Eryth Time elapsed: 59.078141740957896 minutes.
Representative adata: (57515, 27504)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for HSPC
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda:Lambda:Lambda:   1e-05Lambda:1.5e-05 2.2e-05Lambda: starting at Lambda:Lambda:starting at  starting atLambda:Lambda: Lambda:Lambda:Lambda:Lambda: 2024-10-02 19:45:42Lambda:3.2e-05 Lambda: Lambda:  Lambda:Lambda:2024-10-02 19:45:42 Lambda:Lambda: 2024-10-02 19:45:42 Lambda: 0.0001 Lambda:  Lambda: Lambda:Lambda:  Max_iter: starting at  4.6e-05 0.001 6.8e-05   Max_iter:    Max_iter:0.000147  0.006813 starting at  0.014678 0.000215  0.031623   0.000316  10000.000464 2024-10-02 19:45:42 0.000681 starting at starting at starting at 0.001468 0.002154 1000
0.003162 0.004642  1000starting at 0.01 starting at 2024-10-02 19:45:42 0.021544 starting at starting at 0.046416 starting at 0.068129 0.1 starting at 
starting at Max_iter: starting at 2024-10-02 19:45:42 2024-10-02 19:45:42 2024-10-02 19:45:42 starting at starting at starting at starting at 
2024-10-02 19:45:42starting at2024-10-02 19:45:42Max_iter: starting at2024-10-02 19:45:42 2024-10-02 19:45:42starting at2024-10-02 19:45:42starting atstarting at2024-10-02 19:45:422024-10-02 19:45:4210002024-10-02 19:45:42Max_iter:Max_iter:Max_iter:2024-10-02 19:45:422024-10-02 19:45:422024-10-02 19:45:422024-10-02 19:45:42   1000 Max_iter:       
        Max_iter:2024-10-02 19:45:42Max_iter:
2024-10-02 19:45:42 Max_iter: 2024-10-02 19:45:42 Max_iter:2024-10-02 19:45:42 2024-10-02 19:45:42 Max_iter:Max_iter: Max_iter:1000
10001000
Max_iter: Max_iter:Max_iter:Max_iter:     10001000Max_iter: Max_iter: Max_iter: 1000 
1000  10001000Max_iter:1000Max_iter:

 10001000 1000
1000
10001000

 
 1000

1000



10001000



At iteration 125, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  126 ; minimum lost =  0.035472769290208817 ; diff loss =  5.029141902923584e-07 ; diff weight =  0.0003359914699103683
lambda is : 0.010000000000000004, cost : 25.435 min
==========
At iteration 132, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  133 ; minimum lost =  0.041129522025585175 ; diff loss =  8.642673492431641e-07 ; diff weight =  0.0006658727652393281
lambda is : 0.014677992676220709, cost : 26.712 min
==========
At iteration 140, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  141 ; minimum lost =  0.04785846546292305 ; diff loss =  3.0174851417541504e-07 ; diff weight =  0.00026128129684366286
lambda is : 0.02154434690031885, cost : 28.375 min
==========
At iteration 143, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  144 ; minimum lost =  0.05632999539375305 ; diff loss =  1.1175870895385742e-08 ; diff weight =  8.481740951538086e-05
lambda is : 0.0316227766016838, cost : 29.101 min
==========
At iteration 147, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  147 ; minimum lost =  0.08380357176065445 ; diff loss =  -7.450580596923828e-09 ; diff weight =  0.002570507349446416
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.0681292069057962, cost : 29.739 min
==========
At iteration 149, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  150 ; minimum lost =  0.10559959709644318 ; diff loss =  5.21540641784668e-08 ; diff weight =  0.00010961294174194336
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.10000000000000002, cost : 29.838 min
==========
At iteration 163, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  164 ; minimum lost =  0.06784944236278534 ; diff loss =  8.940696716308594e-08 ; diff weight =  0.00016897916793823242
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.04641588833612786, cost : 32.815 min
==========
At iteration 178, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  179 ; minimum lost =  0.025815408676862717 ; diff loss =  9.98377799987793e-07 ; diff weight =  0.0016236911760643125
lambda is : 0.004641588833612781, cost : 35.748 min
==========
At iteration 204, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  205 ; minimum lost =  0.031022613868117332 ; diff loss =  5.159527063369751e-07 ; diff weight =  0.0007105171680450439
lambda is : 0.006812920690579613, cost : 40.755 min
==========
At iteration 273, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  274 ; minimum lost =  0.016204440966248512 ; diff loss =  7.022172212600708e-07 ; diff weight =  0.002244026865810156
lambda is : 0.0021544346900318843, cost : 53.593 min
==========
At iteration 281, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  282 ; minimum lost =  0.020590748637914658 ; diff loss =  9.741634130477905e-07 ; diff weight =  0.0038372769486159086
lambda is : 0.003162277660168382, cost : 54.838 min
==========
At iteration 293, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  294 ; minimum lost =  0.009597314521670341 ; diff loss =  9.34116542339325e-07 ; diff weight =  0.004622236825525761
lambda is : 0.0010000000000000002, cost : 56.576 min
==========
At iteration 292, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  293 ; minimum lost =  0.012513212859630585 ; diff loss =  5.62518835067749e-07 ; diff weight =  0.0035295598208904266
lambda is : 0.0014677992676220694, cost : 57.442 min
==========
At iteration 308, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  309 ; minimum lost =  0.007264802232384682 ; diff loss =  5.308538675308228e-07 ; diff weight =  0.00409428495913744
lambda is : 0.0006812920690579617, cost : 59.933 min
==========
At iteration 326, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  327 ; minimum lost =  0.005470780655741692 ; diff loss =  9.30391252040863e-07 ; diff weight =  0.007885514758527279
lambda is : 0.00046415888336127795, cost : 63.587 min
==========
At iteration 343, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  344 ; minimum lost =  0.00312592601403594 ; diff loss =  9.988434612751007e-07 ; diff weight =  0.01008638460189104
lambda is : 0.0002154434690031884, cost : 66.253 min
==========
At iteration 352, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  353 ; minimum lost =  0.004134706687182188 ; diff loss =  4.670582711696625e-07 ; diff weight =  0.004539592657238245
lambda is : 0.00031622776601683783, cost : 67.756 min
==========
At iteration 348, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  349 ; minimum lost =  0.0023313499987125397 ; diff loss =  9.704381227493286e-07 ; diff weight =  0.011681275442242622
lambda is : 0.00014677992676220703, cost : 67.855 min
==========
At iteration 408, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  409 ; minimum lost =  0.001743633416481316 ; diff loss =  6.671762093901634e-07 ; diff weight =  0.009667242877185345
lambda is : 9.999999999999991e-05, cost : 79.204 min
==========
At iteration 415, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  416 ; minimum lost =  0.0013233913341537118 ; diff loss =  9.953510016202927e-07 ; diff weight =  0.014803051948547363
lambda is : 6.81292069057961e-05, cost : 80.025 min
==========
At iteration 440, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  441 ; minimum lost =  0.0009812056086957455 ; diff loss =  8.954666554927826e-07 ; diff weight =  0.013848538510501385
lambda is : 4.6415888336127784e-05, cost : 84.323 min
==========
At iteration 451, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  452 ; minimum lost =  0.0007511648000217974 ; diff loss =  9.114737622439861e-07 ; diff weight =  0.019437039270997047
lambda is : 3.16227766016838e-05, cost : 87.901 min
==========
At iteration 475, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  476 ; minimum lost =  0.00044471005094237626 ; diff loss =  9.904906619340181e-07 ; diff weight =  0.029135365039110184
lambda is : 1.4677992676220687e-05, cost : 89.905 min
==========
At iteration 464, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  465 ; minimum lost =  0.0005750790587626398 ; diff loss =  9.435461834073067e-07 ; diff weight =  0.03165407478809357
lambda is : 2.1544346900318854e-05, cost : 90.267 min
==========
At iteration 499, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  500 ; minimum lost =  0.0003322384145576507 ; diff loss =  9.043433237820864e-07 ; diff weight =  0.08016958832740784
lambda is : 9.999999999999997e-06, cost : 94.168 min
==========
*** Collecting results ***
Exporting result Dict
/home/jovyan/work/GitHub/EvanPys/Progress/ADlasso2/AD2_w_utils_lossdiff_noZ.py:929: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.
  fig, ax1 = plt.subplots(figsize = (fig_width,fig_height))
HSPC Time elapsed: 94.24282083511352 minutes.
Representative adata: (57515, 27504)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for ILC
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda:Lambda:Lambda: Lambda:  Lambda: Lambda:Lambda:1e-05Lambda: Lambda:Lambda:Lambda:3.2e-05 Lambda:2.2e-05 Lambda:Lambda: Lambda: 1.5e-05   Lambda:Lambda: starting atLambda: Lambda:0.000147 Lambda: Lambda: Lambda: Lambda: Lambda: Lambda:starting at  starting at   4.6e-05 0.001468 starting at 6.8e-05 0.0001    2024-10-02 21:20:020.004642  starting at  0.000215  0.014678  0.000316 0.046416 0.068129  2024-10-02 21:20:02 0.000464 2024-10-02 21:20:02 0.000681 0.001 starting at starting at 2024-10-02 21:20:02 starting at starting at 0.003162 0.002154  Max_iter:starting at 0.006813 2024-10-02 21:20:02 0.01 starting at 0.021544 starting at 0.031623 starting at starting at starting at 0.1 Max_iter: starting at Max_iter: starting at starting at 2024-10-02 21:20:02 2024-10-02 21:20:02 Max_iter: 2024-10-02 21:20:02 2024-10-02 21:20:02 starting at starting at  10002024-10-02 21:20:02 starting at Max_iter: starting at 2024-10-02 21:20:02 starting at 2024-10-02 21:20:02 starting at 2024-10-02 21:20:02 2024-10-02 21:20:02 2024-10-02 21:20:02 starting at 10002024-10-02 21:20:02 10002024-10-02 21:20:02 2024-10-02 21:20:02 Max_iter: Max_iter: 1000Max_iter: Max_iter: 2024-10-02 21:20:02 2024-10-02 21:20:02 
Max_iter: 2024-10-02 21:20:02 10002024-10-02 21:20:02 Max_iter: 2024-10-02 21:20:02 Max_iter: 2024-10-02 21:20:02 Max_iter: Max_iter: Max_iter: 2024-10-02 21:20:02 
Max_iter: 
Max_iter: Max_iter: 10001000
10001000Max_iter:Max_iter:1000Max_iter:
Max_iter: 1000Max_iter:1000Max_iter:100010001000Max_iter:100010001000



 1000 1000
 10001000

 
 


 1000





1000
1000

At iteration 128, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  129 ; minimum lost =  0.026632405817508698 ; diff loss =  1.7881393432617188e-07 ; diff weight =  0.00035649538040161133
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.010000000000000004, cost : 26.425 min
==========
At iteration 136, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  137 ; minimum lost =  0.0581960454583168 ; diff loss =  5.438923835754395e-07 ; diff weight =  0.0005193948745727539
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.04641588833612786, cost : 27.612 min
==========
At iteration 136, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  137 ; minimum lost =  0.03115106001496315 ; diff loss =  7.841736078262329e-07 ; diff weight =  0.0008757720934227109
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.014677992676220709, cost : 27.86 min
==========
At iteration 138, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  139 ; minimum lost =  0.07529133558273315 ; diff loss =  9.015202522277832e-07 ; diff weight =  0.000844709575176239
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.0681292069057962, cost : 27.986 min
==========
At iteration 137, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  138 ; minimum lost =  0.09729494899511337 ; diff loss =  6.705522537231445e-08 ; diff weight =  0.0001239776611328125
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.10000000000000002, cost : 28.038 min
==========
At iteration 139, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  140 ; minimum lost =  0.04607703909277916 ; diff loss =  4.842877388000488e-08 ; diff weight =  0.0001671314239501953
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.0316227766016838, cost : 28.357 min
==========
At iteration 141, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  142 ; minimum lost =  0.02108946070075035 ; diff loss =  2.644956111907959e-07 ; diff weight =  0.0011689243838191032
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.004641588833612781, cost : 28.892 min
==========
At iteration 146, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  147 ; minimum lost =  0.01775636151432991 ; diff loss =  8.139759302139282e-07 ; diff weight =  0.001564888283610344
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0021544346900318843, cost : 29.499 min
==========
At iteration 146, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  147 ; minimum lost =  0.037319134920835495 ; diff loss =  5.103647708892822e-07 ; diff weight =  0.0005709528923034668
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.02154434690031885, cost : 29.933 min
==========
At iteration 152, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  153 ; minimum lost =  0.02341235987842083 ; diff loss =  6.50063157081604e-07 ; diff weight =  0.0016479776240885258
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.006812920690579613, cost : 30.78 min
==========
At iteration 162, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  163 ; minimum lost =  0.019369222223758698 ; diff loss =  3.129243850708008e-07 ; diff weight =  0.0012011401122435927
lambda is : 0.003162277660168382, cost : 32.521 min
==========
At iteration 184, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  185 ; minimum lost =  0.016240274533629417 ; diff loss =  8.847564458847046e-07 ; diff weight =  0.002214938635006547
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0014677992676220694, cost : 36.642 min
==========
At iteration 279, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  280 ; minimum lost =  0.013799477368593216 ; diff loss =  9.98377799987793e-07 ; diff weight =  0.0045737470500171185
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0010000000000000002, cost : 54.684 min
==========
At iteration 282, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  283 ; minimum lost =  0.0075318291783332825 ; diff loss =  8.33999365568161e-07 ; diff weight =  0.007092304993420839
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00031622776601683783, cost : 54.868 min
==========
At iteration 296, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  297 ; minimum lost =  0.011426208540797234 ; diff loss =  9.834766387939453e-07 ; diff weight =  0.004947975277900696
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0006812920690579617, cost : 56.481 min
==========
At iteration 307, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  308 ; minimum lost =  0.00931796059012413 ; diff loss =  9.648501873016357e-07 ; diff weight =  0.004630868323147297
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00046415888336127795, cost : 60.119 min
==========
At iteration 323, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  324 ; minimum lost =  0.006054132245481014 ; diff loss =  9.667128324508667e-07 ; diff weight =  0.005629858002066612
At iteration 319, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  320 ; minimum lost =  0.004892798140645027 ; diff loss =  9.76957380771637e-07 ; diff weight =  0.008079993538558483
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 62.592 min
==========
lambda is : 0.00014677992676220703, cost : 62.632 min
==========
At iteration 352, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  353 ; minimum lost =  0.0039047629106789827 ; diff loss =  9.431969374418259e-07 ; diff weight =  0.006439373828470707
lambda is : 9.999999999999991e-05, cost : 68.46 min
==========
At iteration 355, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  356 ; minimum lost =  0.0025488748215138912 ; diff loss =  9.406358003616333e-07 ; diff weight =  0.015620913356542587
lambda is : 4.6415888336127784e-05, cost : 68.528 min
==========
At iteration 357, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  358 ; minimum lost =  0.003136838786303997 ; diff loss =  9.974464774131775e-07 ; diff weight =  0.007202334236353636
lambda is : 6.81292069057961e-05, cost : 68.706 min
==========
At iteration 394, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  395 ; minimum lost =  0.0015199147164821625 ; diff loss =  8.877832442522049e-07 ; diff weight =  0.002742270240560174
lambda is : 2.1544346900318854e-05, cost : 74.976 min
==========
At iteration 400, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  401 ; minimum lost =  0.0011883644619956613 ; diff loss =  7.022172212600708e-07 ; diff weight =  0.00400400348007679
lambda is : 1.4677992676220687e-05, cost : 76.013 min
==========
At iteration 402, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  403 ; minimum lost =  0.001937237335368991 ; diff loss =  9.615905582904816e-07 ; diff weight =  0.010212515480816364
lambda is : 3.16227766016838e-05, cost : 76.943 min
==========
At iteration 408, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  409 ; minimum lost =  0.0009242289233952761 ; diff loss =  9.66014340519905e-07 ; diff weight =  0.012749183923006058
lambda is : 9.999999999999997e-06, cost : 77.86 min
==========
*** Collecting results ***
Exporting result Dict
ILC Time elapsed: 77.9287144223849 minutes.
Representative adata: (57515, 27504)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for MAIT
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda:Lambda:Lambda: Lambda: Lambda:Lambda:Lambda: Lambda:1e-05 Lambda: Lambda:Lambda:1.5e-05  Lambda: Lambda:Lambda: Lambda:3.2e-05 Lambda: Lambda:Lambda:starting at Lambda: 2.2e-05  Lambda:Lambda: Lambda: Lambda:starting at Lambda:Lambda:0.000316  4.6e-05   6.8e-05  starting at  0.0001   2024-10-02 22:38:04  0.000147 starting at 0.01   0.000215 0.031623  2024-10-02 22:38:04   starting at 0.000464 starting at 0.000681 0.001 starting at 0.001468 2024-10-02 22:38:04 0.002154 starting at 0.003162 0.004642 Max_iter: 0.006813 starting at 2024-10-02 22:38:04 starting at 0.014678 0.021544 starting at starting at 0.046416 Max_iter: 0.068129 0.1 2024-10-02 22:38:04 starting at 2024-10-02 22:38:04 starting at starting at 2024-10-02 22:38:04 starting at Max_iter: starting at 2024-10-02 22:38:04 starting at starting at 1000starting at 2024-10-02 22:38:04 Max_iter: 2024-10-02 22:38:04 starting at starting at 2024-10-02 22:38:04 2024-10-02 22:38:04 starting at 1000starting at starting at Max_iter: 2024-10-02 22:38:04 Max_iter: 2024-10-02 22:38:04 2024-10-02 22:38:04 Max_iter: 2024-10-02 22:38:0410002024-10-02 22:38:04 Max_iter: 2024-10-02 22:38:04 2024-10-02 22:38:04 
2024-10-02 22:38:04 Max_iter: 1000Max_iter: 2024-10-02 22:38:04 2024-10-02 22:38:04 Max_iter: Max_iter: 2024-10-02 22:38:04 
2024-10-02 22:38:04 2024-10-02 22:38:04 1000Max_iter: 1000Max_iter: Max_iter: 1000
 Max_iter:
Max_iter: 1000Max_iter: Max_iter: Max_iter: 1000
1000Max_iter:Max_iter: 10001000Max_iter:Max_iter:Max_iter:
1000
10001000 1000
1000
1000
1000


 10001000

 1000 1000 1000


1000






At iteration 205, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  206 ; minimum lost =  0.16640841960906982 ; diff loss =  6.407499313354492e-07 ; diff weight =  0.0003457327838987112
lambda is : 0.0681292069057962, cost : 41.438 min
==========
At iteration 209, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  210 ; minimum lost =  0.1834516078233719 ; diff loss =  3.8743019104003906e-07 ; diff weight =  9.578466415405273e-05
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.10000000000000002, cost : 42.291 min
==========
At iteration 221, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  222 ; minimum lost =  0.15193942189216614 ; diff loss =  5.960464477539063e-08 ; diff weight =  8.958693069871515e-05
lambda is : 0.04641588833612786, cost : 44.482 min
==========
At iteration 263, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  264 ; minimum lost =  0.1330917924642563 ; diff loss =  3.2782554626464844e-07 ; diff weight =  0.0002771881700027734
lambda is : 0.0316227766016838, cost : 52.475 min
==========
At iteration 318, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  319 ; minimum lost =  0.11322026699781418 ; diff loss =  8.195638656616211e-08 ; diff weight =  0.00017530682089272887
lambda is : 0.02154434690031885, cost : 62.73 min
==========
At iteration 349, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  350 ; minimum lost =  0.09490444511175156 ; diff loss =  1.564621925354004e-07 ; diff weight =  0.0002607705828268081
lambda is : 0.014677992676220709, cost : 68.845 min
==========
At iteration 352, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  353 ; minimum lost =  0.07955826818943024 ; diff loss =  5.960464477539063e-08 ; diff weight =  0.00019945952226407826
lambda is : 0.010000000000000004, cost : 69.381 min
==========
At iteration 382, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  383 ; minimum lost =  0.057416126132011414 ; diff loss =  2.7567148208618164e-07 ; diff weight =  0.002133547328412533
lambda is : 0.004641588833612781, cost : 75.259 min
==========
At iteration 397, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  398 ; minimum lost =  0.06722071766853333 ; diff loss =  9.238719940185547e-07 ; diff weight =  0.0010728612542152405
lambda is : 0.006812920690579613, cost : 78.268 min
==========
At iteration 430, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  430 ; minimum lost =  0.023413822054862976 ; diff loss =  -6.668269634246826e-07 ; diff weight =  0.0189768448472023
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00046415888336127795, cost : 85.505 min
==========
At iteration 484, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  485 ; minimum lost =  0.035860396921634674 ; diff loss =  9.834766387939453e-07 ; diff weight =  0.002318637678399682
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0014677992676220694, cost : 95.19 min
==========
At iteration 511, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  511 ; minimum lost =  0.041706833988428116 ; diff loss =  -1.0803341865539551e-07 ; diff weight =  0.00479467585682869
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0021544346900318843, cost : 98.927 min
==========
At iteration 516, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  517 ; minimum lost =  0.026635872200131416 ; diff loss =  9.853392839431763e-07 ; diff weight =  0.002311242278665304
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0006812920690579617, cost : 101.02 min
==========
At iteration 520, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  521 ; minimum lost =  0.017214836552739143 ; diff loss =  8.810311555862427e-07 ; diff weight =  0.0010162763064727187
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 101.464 min
==========
At iteration 527, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  527 ; minimum lost =  0.009598957374691963 ; diff loss =  -1.0617077350616455e-07 ; diff weight =  0.010169435292482376
lambda is : 4.6415888336127784e-05, cost : 102.307 min
==========
At iteration 534, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  535 ; minimum lost =  0.006942560896277428 ; diff loss =  7.506459951400757e-07 ; diff weight =  0.0030250572599470615
lambda is : 2.1544346900318854e-05, cost : 103.306 min
==========
At iteration 532, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  532 ; minimum lost =  0.030844900757074356 ; diff loss =  -7.82310962677002e-08 ; diff weight =  0.004293996375054121
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0010000000000000002, cost : 103.808 min
==========
At iteration 538, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  539 ; minimum lost =  0.01290197018533945 ; diff loss =  2.952292561531067e-07 ; diff weight =  0.0008883225964382291
At iteration 536, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  537 ; minimum lost =  0.048659153282642365 ; diff loss =  6.332993507385254e-07 ; diff weight =  0.0003826719184871763
lambda is : 0.003162277660168382, cost : 104.829 min
==========
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 9.999999999999991e-05, cost : 104.835 min
==========
At iteration 542, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  543 ; minimum lost =  0.019882749766111374 ; diff loss =  4.209578037261963e-07 ; diff weight =  0.002489106496796012
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00031622776601683783, cost : 105.111 min
==========
At iteration 545, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  546 ; minimum lost =  0.011139489710330963 ; diff loss =  3.8370490074157715e-07 ; diff weight =  0.007062689866870642
lambda is : 6.81292069057961e-05, cost : 105.77 min
==========
At iteration 547, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  548 ; minimum lost =  0.005689516663551331 ; diff loss =  7.282942533493042e-07 ; diff weight =  0.00974952057003975
lambda is : 1.4677992676220687e-05, cost : 106.345 min
==========
At iteration 559, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  560 ; minimum lost =  0.014772893860936165 ; diff loss =  9.248033165931702e-07 ; diff weight =  0.0031697717495262623
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00014677992676220703, cost : 107.705 min
==========
At iteration 577, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  578 ; minimum lost =  0.008060515858232975 ; diff loss =  7.925555109977722e-07 ; diff weight =  0.0006340141408145428
At iteration 576, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  577 ; minimum lost =  0.004586428869515657 ; diff loss =  9.336508810520172e-07 ; diff weight =  0.001970737474039197
lambda is : 3.16227766016838e-05, cost : 111.591 min
==========
lambda is : 9.999999999999997e-06, cost : 111.657 min
==========
*** Collecting results ***
Exporting result Dict
MAIT Time elapsed: 111.73432369232178 minutes.
Representative adata: (57515, 27504)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for NK
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda:Lambda:Lambda: Lambda: Lambda:Lambda:Lambda: Lambda: Lambda:1e-05  2.2e-05 Lambda: Lambda: Lambda:  Lambda:Lambda:  Lambda:1.5e-05 Lambda:Lambda: 0.0001 Lambda: Lambda:Lambda:starting at Lambda: 3.2e-05 Lambda:Lambda:starting at Lambda:Lambda: 4.6e-05 0.000464 0.000681 0.000215  0.001468 6.8e-05  starting at  0.004642 starting at  0.000147   2024-10-03 00:29:53 0.021544 starting at   2024-10-03 00:29:53   0.000316 starting at starting at starting at starting at 0.001 starting at starting at 0.002154 2024-10-03 00:29:53 0.003162 starting at 2024-10-03 00:29:53 0.006813 starting at 0.01 0.014678 Max_iter: starting at 2024-10-03 00:29:53 0.031623 0.046416 Max_iter: 0.1 0.068129 starting at 2024-10-03 00:29:53 2024-10-03 00:29:53 2024-10-03 00:29:53 2024-10-03 00:29:53 starting at 2024-10-03 00:29:53 2024-10-03 00:29:53 starting at Max_iter: starting at 2024-10-03 00:29:53 Max_iter: starting at 2024-10-03 00:29:53 starting at starting at 10002024-10-03 00:29:53 Max_iter: starting at starting at 1000
starting at starting at 2024-10-03 00:29:53 Max_iter: Max_iter: Max_iter: Max_iter: 2024-10-03 00:29:53 Max_iter: Max_iter: 2024-10-03 00:29:53 10002024-10-03 00:29:53 Max_iter: 10002024-10-03 00:29:53 Max_iter: 2024-10-03 00:29:53 2024-10-03 00:29:53 
Max_iter: 10002024-10-03 00:29:53 2024-10-03 00:29:53 2024-10-03 00:29:53 2024-10-03 00:29:53 Max_iter: 1000100010001000Max_iter: 10001000Max_iter: 
Max_iter: 1000
Max_iter: 1000Max_iter:Max_iter: 1000
Max_iter: Max_iter: Max_iter: Max_iter:1000



1000


1000
1000
1000

 1000

100010001000 

1000


1000

At iteration 337, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  338 ; minimum lost =  0.2353195995092392 ; diff loss =  1.043081283569336e-07 ; diff weight =  5.3423606004798785e-05
lambda is : 0.04641588833612786, cost : 67.791 min
==========
At iteration 355, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  356 ; minimum lost =  0.3140639066696167 ; diff loss =  6.854534149169922e-07 ; diff weight =  0.0002925295557361096
lambda is : 0.10000000000000002, cost : 71.052 min
==========
At iteration 374, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  375 ; minimum lost =  0.2725161612033844 ; diff loss =  9.834766387939453e-07 ; diff weight =  0.00032838471815921366
lambda is : 0.0681292069057962, cost : 74.636 min
==========
At iteration 392, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  393 ; minimum lost =  0.17099201679229736 ; diff loss =  7.450580596923828e-07 ; diff weight =  0.0015191038837656379
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.02154434690031885, cost : 78.444 min
==========
At iteration 429, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  430 ; minimum lost =  0.14448028802871704 ; diff loss =  8.493661880493164e-07 ; diff weight =  0.0010950893629342318
lambda is : 0.014677992676220709, cost : 85.285 min
==========
At iteration 443, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  444 ; minimum lost =  0.2011643350124359 ; diff loss =  3.2782554626464844e-07 ; diff weight =  0.00039989102515392005
lambda is : 0.0316227766016838, cost : 88.381 min
==========
At iteration 465, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  466 ; minimum lost =  0.12234356999397278 ; diff loss =  8.940696716308594e-07 ; diff weight =  0.0008388159913010895
lambda is : 0.010000000000000004, cost : 92.121 min
==========
At iteration 481, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  482 ; minimum lost =  0.10429343581199646 ; diff loss =  8.866190910339355e-07 ; diff weight =  0.0015389325562864542
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.006812920690579613, cost : 95.419 min
==========
At iteration 504, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  504 ; minimum lost =  0.08992314338684082 ; diff loss =  -6.258487701416016e-07 ; diff weight =  0.016720203682780266
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.004641588833612781, cost : 99.425 min
==========
At iteration 536, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  537 ; minimum lost =  0.059580087661743164 ; diff loss =  9.387731552124023e-07 ; diff weight =  0.0020634152460843325
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0014677992676220694, cost : 105.194 min
==========
At iteration 541, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  542 ; minimum lost =  0.0784430131316185 ; diff loss =  8.568167686462402e-07 ; diff weight =  0.001503074192442
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.003162277660168382, cost : 106.239 min
==========
At iteration 544, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  545 ; minimum lost =  0.022348515689373016 ; diff loss =  9.909272193908691e-07 ; diff weight =  0.000579560874029994
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 9.999999999999991e-05, cost : 106.488 min
==========
At iteration 553, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  554 ; minimum lost =  0.012629471719264984 ; diff loss =  9.98377799987793e-07 ; diff weight =  0.0005268696113489568
lambda is : 2.1544346900318854e-05, cost : 108.326 min
==========
At iteration 579, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  580 ; minimum lost =  0.0685131847858429 ; diff loss =  9.238719940185547e-07 ; diff weight =  0.001457684556953609
lambda is : 0.0021544346900318843, cost : 113.098 min
==========
At iteration 577, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  578 ; minimum lost =  0.016549929976463318 ; diff loss =  4.991888999938965e-07 ; diff weight =  0.008234419859945774
At iteration 578, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  579 ; minimum lost =  0.051732078194618225 ; diff loss =  6.556510925292969e-07 ; diff weight =  0.00300803710706532
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0010000000000000002, cost : 113.249 min
==========
lambda is : 4.6415888336127784e-05, cost : 113.372 min
==========
At iteration 578, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  579 ; minimum lost =  0.02540593035519123 ; diff loss =  5.047768354415894e-07 ; diff weight =  0.0025559216737747192
At iteration 582, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  583 ; minimum lost =  0.019143573939800262 ; diff loss =  5.383044481277466e-07 ; diff weight =  0.00042332298471592367
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00014677992676220703, cost : 113.629 min
==========
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 6.81292069057961e-05, cost : 113.883 min
==========
At iteration 595, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  596 ; minimum lost =  0.029098298400640488 ; diff loss =  7.860362529754639e-07 ; diff weight =  0.0011407291749492288
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 116.581 min
==========
At iteration 596, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  597 ; minimum lost =  0.04483860731124878 ; diff loss =  9.238719940185547e-07 ; diff weight =  0.003629022976383567
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0006812920690579617, cost : 117.052 min
==========
At iteration 612, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  613 ; minimum lost =  0.008219299837946892 ; diff loss =  9.778887033462524e-07 ; diff weight =  0.019664952531456947
lambda is : 9.999999999999997e-06, cost : 119.737 min
==========
At iteration 612, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  612 ; minimum lost =  0.03356747701764107 ; diff loss =  -9.015202522277832e-07 ; diff weight =  0.028652414679527283
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00031622776601683783, cost : 119.908 min
==========
At iteration 628, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  629 ; minimum lost =  0.01379470992833376 ; diff loss =  5.029141902923584e-07 ; diff weight =  0.00027334029437042773
lambda is : 3.16227766016838e-05, cost : 122.014 min
==========
At iteration 629, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  630 ; minimum lost =  0.03872506693005562 ; diff loss =  5.960464477539062e-07 ; diff weight =  0.0029027857817709446
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00046415888336127795, cost : 122.811 min
==========
At iteration 667, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  668 ; minimum lost =  0.00935225561261177 ; diff loss =  8.437782526016235e-07 ; diff weight =  0.00034389400389045477
lambda is : 1.4677992676220687e-05, cost : 129.006 min
==========
*** Collecting results ***
Exporting result Dict
NK Time elapsed: 129.08457930882773 minutes.
Representative adata: (57515, 27504)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for NK_CD56bright
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda:Lambda:Lambda:Lambda: Lambda:Lambda:Lambda: Lambda: Lambda:Lambda: Lambda:Lambda:1e-05 Lambda: Lambda:  Lambda: Lambda:1.5e-05 Lambda: Lambda: Lambda:2.2e-05 Lambda: Lambda: Lambda:3.2e-05 Lambda: Lambda:Lambda:  starting at  4.6e-05 0.001 0.000681  6.8e-05  starting at  0.0001 0.004642  starting at  0.000147  0.000215  starting at  0.031623  0.1 0.000316 2024-10-03 02:39:04 0.000464 starting at starting at starting at 0.001468 starting at 0.002154 2024-10-03 02:39:04 0.003162 starting at starting at 0.006813 2024-10-03 02:39:04 0.01 starting at 0.014678 starting at 0.0215442024-10-03 02:39:04 0.046416 starting at 0.068129 starting at starting at Max_iter: starting at 2024-10-03 02:39:04 2024-10-03 02:39:04 2024-10-03 02:39:04 starting at 2024-10-03 02:39:04 starting at Max_iter: starting at 2024-10-03 02:39:04 2024-10-03 02:39:04 starting at Max_iter: starting at 2024-10-03 02:39:04 starting at 2024-10-03 02:39:04  starting atMax_iter: starting at 2024-10-03 02:39:04 starting at 2024-10-03 02:39:04 2024-10-03 02:39:04 10002024-10-03 02:39:04 Max_iter: Max_iter: Max_iter: 2024-10-03 02:39:04 Max_iter: 2024-10-03 02:39:04 10002024-10-03 02:39:04 Max_iter: Max_iter: 2024-10-03 02:39:04 10002024-10-03 02:39:04 Max_iter: 2024-10-03 02:39:04 Max_iter:  2024-10-03 02:39:0410002024-10-03 02:39:04 Max_iter: 2024-10-03 02:39:04 Max_iter: Max_iter: 
Max_iter: 100010001000Max_iter: 1000Max_iter: 
Max_iter: 10001000Max_iter: 
Max_iter: 1000Max_iter: 1000 Max_iter:
Max_iter: 1000Max_iter: 100010001000


1000
1000
1000

10001000
1000
 1000
1000








1000

At iteration 164, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  165 ; minimum lost =  0.1115340143442154 ; diff loss =  3.8743019104003906e-07 ; diff weight =  1.7021686289808713e-05
lambda is : 0.0681292069057962, cost : 33.193 min
==========
At iteration 169, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  170 ; minimum lost =  0.13076350092887878 ; diff loss =  7.152557373046875e-07 ; diff weight =  0.0003746151924133301
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.10000000000000002, cost : 34.24 min
==========
At iteration 174, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  175 ; minimum lost =  0.0963665172457695 ; diff loss =  1.9371509552001953e-07 ; diff weight =  0.0001628398895263672
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.04641588833612786, cost : 35.165 min
==========
At iteration 178, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  179 ; minimum lost =  0.08628855645656586 ; diff loss =  4.470348358154297e-08 ; diff weight =  0.0002870663593057543
lambda is : 0.0316227766016838, cost : 36.072 min
==========
At iteration 201, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  202 ; minimum lost =  0.07652349770069122 ; diff loss =  8.940696716308594e-07 ; diff weight =  0.0003067713987547904
lambda is : 0.02154434690031885, cost : 40.546 min
==========
At iteration 272, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  273 ; minimum lost =  0.06764198839664459 ; diff loss =  9.834766387939453e-07 ; diff weight =  0.0009228775161318481
lambda is : 0.014677992676220709, cost : 53.687 min
==========
At iteration 281, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  282 ; minimum lost =  0.05957380309700966 ; diff loss =  5.848705768585205e-07 ; diff weight =  0.0010406575165688992
lambda is : 0.010000000000000004, cost : 55.503 min
==========
At iteration 313, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  313 ; minimum lost =  0.03514818102121353 ; diff loss =  -2.1606683731079102e-07 ; diff weight =  0.04519428685307503
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.003162277660168382, cost : 61.908 min
==========
At iteration 346, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  347 ; minimum lost =  0.05060340464115143 ; diff loss =  1.9371509552001953e-07 ; diff weight =  0.00023885681002866477
lambda is : 0.006812920690579613, cost : 67.321 min
==========
At iteration 355, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  355 ; minimum lost =  0.016535408794879913 ; diff loss =  -2.738088369369507e-07 ; diff weight =  0.010365616530179977
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0006812920690579617, cost : 69.78 min
==========
At iteration 360, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  361 ; minimum lost =  0.02395029552280903 ; diff loss =  8.717179298400879e-07 ; diff weight =  0.0017537630628794432
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0014677992676220694, cost : 70.194 min
==========
At iteration 364, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  365 ; minimum lost =  0.02894231118261814 ; diff loss =  9.592622518539429e-07 ; diff weight =  0.0018393469508737326
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0021544346900318843, cost : 71.069 min
==========
At iteration 401, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  402 ; minimum lost =  0.01980050653219223 ; diff loss =  9.499490261077881e-07 ; diff weight =  0.0024640876799821854
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0010000000000000002, cost : 77.538 min
==========
At iteration 410, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  411 ; minimum lost =  0.04216793179512024 ; diff loss =  5.401670932769775e-07 ; diff weight =  0.001537226140499115
lambda is : 0.004641588833612781, cost : 80.798 min
==========
At iteration 414, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  415 ; minimum lost =  0.013857354409992695 ; diff loss =  6.770715117454529e-07 ; diff weight =  0.002611299278214574
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00046415888336127795, cost : 80.912 min
==========
At iteration 417, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  418 ; minimum lost =  0.009999366477131844 ; diff loss =  9.760260581970215e-07 ; diff weight =  0.0033203589264303446
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 81.107 min
==========
At iteration 439, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  440 ; minimum lost =  0.006375953555107117 ; diff loss =  8.391216397285461e-07 ; diff weight =  0.001296142814680934
lambda is : 6.81292069057961e-05, cost : 85.207 min
==========
At iteration 444, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  445 ; minimum lost =  0.0034538106992840767 ; diff loss =  9.951181709766388e-07 ; diff weight =  0.0010689840419217944
lambda is : 1.4677992676220687e-05, cost : 86.387 min
==========
At iteration 457, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  458 ; minimum lost =  0.005327225662767887 ; diff loss =  8.89413058757782e-07 ; diff weight =  0.0016791915986686945
lambda is : 4.6415888336127784e-05, cost : 88.406 min
==========
At iteration 455, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  456 ; minimum lost =  0.011723875999450684 ; diff loss =  9.527429938316345e-07 ; diff weight =  0.002961857942864299
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00031622776601683783, cost : 88.909 min
==========
At iteration 461, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  462 ; minimum lost =  0.008562702685594559 ; diff loss =  5.112960934638977e-07 ; diff weight =  0.0009888934437185526
lambda is : 0.00014677992676220703, cost : 89.064 min
==========
At iteration 472, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  473 ; minimum lost =  0.004480684641748667 ; diff loss =  6.402842700481415e-07 ; diff weight =  0.0017281454056501389
At iteration 470, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  471 ; minimum lost =  0.0037188990972936153 ; diff loss =  9.168870747089386e-07 ; diff weight =  0.011308537796139717
lambda is : 3.16227766016838e-05, cost : 91.137 min
==========
lambda is : 2.1544346900318854e-05, cost : 91.162 min
==========
At iteration 475, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  476 ; minimum lost =  0.007310398388653994 ; diff loss =  2.0954757928848267e-07 ; diff weight =  0.005328840110450983
lambda is : 9.999999999999991e-05, cost : 91.564 min
==========
At iteration 479, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  480 ; minimum lost =  0.0026724673807621002 ; diff loss =  9.82079654932022e-07 ; diff weight =  0.0017471503233537078
lambda is : 9.999999999999997e-06, cost : 92.636 min
==========
*** Collecting results ***
Exporting result Dict
NK_CD56bright Time elapsed: 92.70704656044641 minutes.
Representative adata: (57515, 27504)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for NK_Proliferating
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda:Lambda:Lambda:Lambda:Lambda: Lambda:  Lambda:Lambda:Lambda:Lambda: Lambda:  Lambda:Lambda:1e-05  1.5e-05 2.2e-05 Lambda:Lambda:  Lambda: Lambda: Lambda:3.2e-05 Lambda:Lambda:Lambda:Lambda:0.000215 Lambda:4.6e-05 Lambda:Lambda:  starting at 6.8e-05 starting at starting at   0.001 0.0001  0.002154  0.000147  starting at     starting at  starting at   0.000316 0.000464 2024-10-03 04:11:52 starting at 2024-10-03 04:11:52 2024-10-03 04:11:52 0.000681 0.001468 starting at starting at 0.003162 starting at 0.004642 starting at 0.006813 2024-10-03 04:11:52 0.01 0.014678 0.021544 0.031623 2024-10-03 04:11:52 0.046416 2024-10-03 04:11:52 0.068129 0.1 starting at starting at Max_iter: 2024-10-03 04:11:52 Max_iter: Max_iter: starting at starting at 2024-10-03 04:11:52 2024-10-03 04:11:52 starting at 2024-10-03 04:11:52 starting at 2024-10-03 04:11:52 starting at Max_iter: starting at starting at starting at starting at Max_iter: starting at Max_iter: starting at starting at 2024-10-03 04:11:52 2024-10-03 04:11:52 1000Max_iter: 1000
10002024-10-03 04:11:52 2024-10-03 04:11:52 Max_iter: Max_iter: 2024-10-03 04:11:52 Max_iter: 2024-10-03 04:11:52 Max_iter: 2024-10-03 04:11:52 10002024-10-03 04:11:52 2024-10-03 04:11:52 2024-10-03 04:11:52 2024-10-03 04:11:52 10002024-10-03 04:11:52 10002024-10-03 04:11:52 2024-10-03 04:11:52 Max_iter: Max_iter: 
1000
Max_iter:Max_iter: 10001000Max_iter:1000Max_iter: 1000Max_iter:
Max_iter:Max_iter: Max_iter:Max_iter:
Max_iter:
Max_iter: Max_iter: 10001000

 1000


 1000
1000

  10001000
  1000 100010001000

1000
1000

1000




At iteration 142, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  143 ; minimum lost =  0.09323471784591675 ; diff loss =  2.9802322387695312e-08 ; diff weight =  7.677078247070312e-05
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.0681292069057962, cost : 29.138 min
==========
At iteration 145, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  146 ; minimum lost =  0.11435697227716446 ; diff loss =  8.419156074523926e-07 ; diff weight =  0.00040978193283081055
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.10000000000000002, cost : 29.581 min
==========
At iteration 148, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  149 ; minimum lost =  0.06692535430192947 ; diff loss =  9.015202522277832e-07 ; diff weight =  0.00021502922754734755
lambda is : 0.0316227766016838, cost : 30.141 min
==========
At iteration 148, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  149 ; minimum lost =  0.07787993550300598 ; diff loss =  7.450580596923828e-08 ; diff weight =  0.0001391768455505371
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.04641588833612786, cost : 30.286 min
==========
At iteration 158, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  159 ; minimum lost =  0.05914665013551712 ; diff loss =  2.5331974029541016e-07 ; diff weight =  9.093599510379136e-05
lambda is : 0.02154434690031885, cost : 31.944 min
==========
At iteration 225, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  226 ; minimum lost =  0.052606672048568726 ; diff loss =  9.797513484954834e-07 ; diff weight =  0.0017341412603855133
lambda is : 0.014677992676220709, cost : 44.499 min
==========
At iteration 255, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  256 ; minimum lost =  0.04348349571228027 ; diff loss =  7.450580596923828e-08 ; diff weight =  0.0004106131964363158
lambda is : 0.010000000000000004, cost : 50.645 min
==========
At iteration 270, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  271 ; minimum lost =  0.028401972725987434 ; diff loss =  8.884817361831665e-07 ; diff weight =  0.002431065309792757
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.004641588833612781, cost : 53.374 min
==========
At iteration 281, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  282 ; minimum lost =  0.03508191928267479 ; diff loss =  8.158385753631592e-07 ; diff weight =  0.0013328294735401869
lambda is : 0.006812920690579613, cost : 55.28 min
==========
At iteration 303, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  304 ; minimum lost =  0.022860310971736908 ; diff loss =  9.071081876754761e-07 ; diff weight =  0.0024961205199360847
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.003162277660168382, cost : 59.666 min
==========
At iteration 315, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  316 ; minimum lost =  0.015519103035330772 ; diff loss =  8.121132850646973e-07 ; diff weight =  0.0026943988632410765
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0014677992676220694, cost : 61.326 min
==========
At iteration 335, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  336 ; minimum lost =  0.018705731257796288 ; diff loss =  9.667128324508667e-07 ; diff weight =  0.003089717822149396
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0021544346900318843, cost : 65.269 min
==========
At iteration 352, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  353 ; minimum lost =  0.008099480532109737 ; diff loss =  8.177012205123901e-07 ; diff weight =  0.00643337843939662
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00031622776601683783, cost : 68.162 min
==========
At iteration 366, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  367 ; minimum lost =  0.010928554460406303 ; diff loss =  9.629875421524048e-07 ; diff weight =  0.002992598107084632
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0006812920690579617, cost : 71.254 min
==========
At iteration 371, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  372 ; minimum lost =  0.006978197488933802 ; diff loss =  7.874332368373871e-07 ; diff weight =  0.002232827479019761
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 71.819 min
==========
At iteration 379, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  380 ; minimum lost =  0.012923887930810452 ; diff loss =  9.797513484954834e-07 ; diff weight =  0.00197432329878211
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0010000000000000002, cost : 73.683 min
==========
At iteration 379, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  380 ; minimum lost =  0.0061026266776025295 ; diff loss =  6.41215592622757e-07 ; diff weight =  0.006201182492077351
lambda is : 0.00014677992676220703, cost : 74.343 min
==========
At iteration 395, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  396 ; minimum lost =  0.00930098257958889 ; diff loss =  9.974464774131775e-07 ; diff weight =  0.0044121635146439075
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00046415888336127795, cost : 76.171 min
==========
At iteration 407, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  408 ; minimum lost =  0.003951608203351498 ; diff loss =  9.629875421524048e-07 ; diff weight =  0.003682251088321209
lambda is : 4.6415888336127784e-05, cost : 79.601 min
==========
At iteration 414, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  415 ; minimum lost =  0.005285133607685566 ; diff loss =  9.96515154838562e-07 ; diff weight =  0.0016626110300421715
lambda is : 9.999999999999991e-05, cost : 81.556 min
==========
At iteration 436, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  437 ; minimum lost =  0.004489829298108816 ; diff loss =  5.592592060565948e-07 ; diff weight =  0.008681029081344604
lambda is : 6.81292069057961e-05, cost : 84.338 min
==========
At iteration 465, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  466 ; minimum lost =  0.0018588306847959757 ; diff loss =  6.927875801920891e-07 ; diff weight =  0.002691201865673065
lambda is : 9.999999999999997e-06, cost : 89.263 min
==========
At iteration 481, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  482 ; minimum lost =  0.0025805302429944277 ; diff loss =  4.84054908156395e-07 ; diff weight =  0.01035495474934578
lambda is : 2.1544346900318854e-05, cost : 91.265 min
==========
At iteration 494, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  495 ; minimum lost =  0.003076958702877164 ; diff loss =  9.362120181322098e-07 ; diff weight =  0.0007347030914388597
lambda is : 3.16227766016838e-05, cost : 94.405 min
==========
At iteration 502, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  503 ; minimum lost =  0.0020724330097436905 ; diff loss =  4.23286110162735e-07 ; diff weight =  0.0025791916996240616
lambda is : 1.4677992676220687e-05, cost : 94.967 min
==========
*** Collecting results ***
Exporting result Dict
NK_Proliferating Time elapsed: 95.04690709511439 minutes.
Representative adata: (57515, 27504)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for Plasmablast
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda: Lambda:Lambda:1e-05   Lambda:Lambda:Lambda:Lambda:Lambda:Lambda:starting at Lambda:1.5e-05 Lambda:2.2e-05 Lambda:Lambda: Lambda: Lambda: Lambda: Lambda:Lambda: Lambda:Lambda: Lambda: Lambda:Lambda:2024-10-03 05:47:01 Lambda:Lambda: starting at  starting at   3.2e-05  4.6e-05 0.001468  6.8e-05   0.0001   0.000147  0.014678   Max_iter:   0.000215 2024-10-03 05:47:01 0.000316 2024-10-03 05:47:01 0.000464 0.000681 starting at 0.001 starting at starting at 0.002154 starting at 0.003162 0.004642 starting at 0.006813 0.01 starting at 0.021544 starting at 0.031623 0.046416 10000.068129 0.1 starting at Max_iter: starting at Max_iter: starting at starting at 2024-10-03 05:47:01 starting at 2024-10-03 05:47:01 2024-10-03 05:47:01 starting at 2024-10-03 05:47:01 starting at starting at 2024-10-03 05:47:01 starting at starting at 2024-10-03 05:47:01 starting at 2024-10-03 05:47:01 starting at starting at 
starting at starting at 2024-10-03 05:47:01 10002024-10-03 05:47:01 10002024-10-03 05:47:01 2024-10-03 05:47:01 Max_iter: 2024-10-03 05:47:01 Max_iter: Max_iter: 2024-10-03 05:47:01 Max_iter: 2024-10-03 05:47:01 2024-10-03 05:47:01 Max_iter: 2024-10-03 05:47:01 2024-10-03 05:47:01 Max_iter: 2024-10-03 05:47:01 Max_iter: 2024-10-03 05:47:01 2024-10-03 05:47:012024-10-03 05:47:01 2024-10-03 05:47:01Max_iter: 
Max_iter: 
Max_iter:Max_iter:1000Max_iter: 10001000Max_iter: 1000Max_iter: Max_iter:1000
Max_iter: Max_iter:1000Max_iter: 1000Max_iter:  Max_iter:  10001000  
1000

1000
1000
 10001000 
1000
1000Max_iter:1000Max_iter: 

1000
1000




1000

 1000
1000


At iteration 137, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  138 ; minimum lost =  0.05643240362405777 ; diff loss =  4.3213367462158203e-07 ; diff weight =  0.0007242300780490041
lambda is : 0.0316227766016838, cost : 27.959 min
==========
At iteration 142, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  143 ; minimum lost =  0.047608572989702225 ; diff loss =  4.0978193283081055e-08 ; diff weight =  0.0004626430163625628
lambda is : 0.02154434690031885, cost : 28.831 min
==========
At iteration 143, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  144 ; minimum lost =  0.06771603226661682 ; diff loss =  4.470348358154297e-08 ; diff weight =  9.783809946384281e-05
lambda is : 0.04641588833612786, cost : 29.718 min
==========
At iteration 148, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  149 ; minimum lost =  0.10529609769582748 ; diff loss =  2.7567148208618164e-07 ; diff weight =  0.00023800134658813477
lambda is : 0.10000000000000002, cost : 29.985 min
==========
At iteration 156, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  157 ; minimum lost =  0.08347614109516144 ; diff loss =  8.195638656616211e-07 ; diff weight =  0.00041925907135009766
lambda is : 0.0681292069057962, cost : 31.491 min
==========
At iteration 171, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  172 ; minimum lost =  0.038435548543930054 ; diff loss =  8.940696716308594e-08 ; diff weight =  0.0003665500262286514
lambda is : 0.014677992676220709, cost : 34.387 min
==========
At iteration 202, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  203 ; minimum lost =  0.03002525120973587 ; diff loss =  1.043081283569336e-07 ; diff weight =  0.0003503439074847847
lambda is : 0.010000000000000004, cost : 40.128 min
==========
At iteration 219, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  220 ; minimum lost =  0.023246631026268005 ; diff loss =  1.0803341865539551e-07 ; diff weight =  0.00030072047957219183
lambda is : 0.006812920690579613, cost : 43.612 min
==========
At iteration 251, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  252 ; minimum lost =  0.010998876765370369 ; diff loss =  9.816139936447144e-07 ; diff weight =  0.0025528830010443926
lambda is : 0.0021544346900318843, cost : 49.642 min
==========
At iteration 274, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  275 ; minimum lost =  0.014093923382461071 ; diff loss =  9.76957380771637e-07 ; diff weight =  0.0030683644581586123
lambda is : 0.003162277660168382, cost : 53.294 min
==========
At iteration 271, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  272 ; minimum lost =  0.008546038530766964 ; diff loss =  7.580965757369995e-07 ; diff weight =  0.0027398914098739624
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0014677992676220694, cost : 53.513 min
==========
At iteration 273, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  274 ; minimum lost =  0.018058793619275093 ; diff loss =  2.0675361156463623e-07 ; diff weight =  0.0009250890580005944
lambda is : 0.004641588833612781, cost : 53.939 min
==========
At iteration 288, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  289 ; minimum lost =  0.006586426869034767 ; diff loss =  7.767230272293091e-07 ; diff weight =  0.004669700283557177
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0010000000000000002, cost : 56.04 min
==========
At iteration 295, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  296 ; minimum lost =  0.0038462416268885136 ; diff loss =  9.236391633749008e-07 ; diff weight =  0.006941098719835281
lambda is : 0.00046415888336127795, cost : 58.071 min
==========
At iteration 299, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  300 ; minimum lost =  0.005051386542618275 ; diff loss =  8.00006091594696e-07 ; diff weight =  0.008775413036346436
lambda is : 0.0006812920690579617, cost : 59.126 min
==========
At iteration 326, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  327 ; minimum lost =  0.0029071213211864233 ; diff loss =  6.33997842669487e-07 ; diff weight =  0.006875739898532629
lambda is : 0.00031622776601683783, cost : 63.473 min
==========
At iteration 333, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  334 ; minimum lost =  0.001753196120262146 ; diff loss =  9.748619049787521e-07 ; diff weight =  0.010559187270700932
lambda is : 0.00014677992676220703, cost : 64.755 min
==========
At iteration 342, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  343 ; minimum lost =  0.002240315079689026 ; diff loss =  9.231735020875931e-07 ; diff weight =  0.008533068001270294
lambda is : 0.0002154434690031884, cost : 66.443 min
==========
At iteration 373, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  374 ; minimum lost =  0.0013232355704531074 ; diff loss =  7.761409506201744e-07 ; diff weight =  0.008240720257163048
lambda is : 9.999999999999991e-05, cost : 73.077 min
==========
At iteration 400, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  401 ; minimum lost =  0.0010482906363904476 ; diff loss =  8.53673554956913e-07 ; diff weight =  0.0229018721729517
lambda is : 6.81292069057961e-05, cost : 77.082 min
==========
At iteration 417, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  418 ; minimum lost =  0.0008124522864818573 ; diff loss =  8.981442078948021e-07 ; diff weight =  0.013936702162027359
lambda is : 4.6415888336127784e-05, cost : 80.142 min
==========
At iteration 442, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  443 ; minimum lost =  0.000625264598056674 ; diff loss =  9.575160220265388e-07 ; diff weight =  0.01776963844895363
lambda is : 3.16227766016838e-05, cost : 84.651 min
==========
At iteration 459, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  460 ; minimum lost =  0.000482154602650553 ; diff loss =  8.845236152410507e-07 ; diff weight =  0.019472802057862282
lambda is : 2.1544346900318854e-05, cost : 87.073 min
==========
At iteration 473, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  474 ; minimum lost =  0.0003713613550644368 ; diff loss =  9.456416592001915e-07 ; diff weight =  0.023447252810001373
lambda is : 1.4677992676220687e-05, cost : 89.952 min
==========
At iteration 487, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  488 ; minimum lost =  0.00029371955315582454 ; diff loss =  8.808274287730455e-07 ; diff weight =  0.021125663071870804
lambda is : 9.999999999999997e-06, cost : 91.528 min
==========
*** Collecting results ***
Exporting result Dict
Plasmablast Time elapsed: 91.6005426843961 minutes.
Representative adata: (57515, 27504)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for Platelet
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda: Lambda:Lambda:Lambda:1e-05 Lambda: Lambda:Lambda: Lambda:Lambda: Lambda:Lambda:starting atLambda:Lambda: Lambda:2.2e-05 Lambda: Lambda: Lambda:4.6e-05 Lambda: Lambda:Lambda: Lambda:1.5e-05 Lambda: Lambda:Lambda:  2024-10-03 07:18:42Lambda:  3.2e-05  starting at  0.001  6.8e-05  starting at  0.0001   0.000147  starting at  0.000215   0.000316  Max_iter: 0.068129 0.000464 starting at 0.000681 2024-10-03 07:18:42 0.001468 starting at 0.002154 starting at 0.003162 2024-10-03 07:18:42 0.004642 starting at 0.006813 0.01 starting at 0.014678 2024-10-03 07:18:42 0.021544 starting at 0.031623 0.046416 starting at  10000.1 starting at starting at 2024-10-03 07:18:42 starting at Max_iter: starting at 2024-10-03 07:18:42 starting at 2024-10-03 07:18:42 starting at Max_iter: starting at 2024-10-03 07:18:42 starting at starting at 2024-10-03 07:18:42 starting at Max_iter: starting at 2024-10-03 07:18:42 starting at starting at 2024-10-03 07:18:42 
starting at 2024-10-03 07:18:42 2024-10-03 07:18:42 Max_iter: 2024-10-03 07:18:42 10002024-10-03 07:18:42 Max_iter: 2024-10-03 07:18:42 Max_iter: 2024-10-03 07:18:42 10002024-10-03 07:18:42 Max_iter: 2024-10-03 07:18:42 2024-10-03 07:18:42 Max_iter: 2024-10-03 07:18:42 10002024-10-03 07:18:42 Max_iter: 2024-10-03 07:18:42 2024-10-03 07:18:42 Max_iter: 2024-10-03 07:18:42 Max_iter: Max_iter: 1000Max_iter: 
Max_iter:1000Max_iter: 1000Max_iter:
Max_iter:1000Max_iter:Max_iter:1000Max_iter:
Max_iter:1000Max_iter:Max_iter:1000Max_iter:10001000
1000 
1000
 1000 1000
 1000 1000
 1000 1000
 1000 1000
 1000


1000










At iteration 134, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  135 ; minimum lost =  0.14489656686782837 ; diff loss =  4.470348358154297e-08 ; diff weight =  0.00010412931442260742
lambda is : 0.10000000000000002, cost : 27.413 min
==========
At iteration 179, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  180 ; minimum lost =  0.12231236696243286 ; diff loss =  7.301568984985352e-07 ; diff weight =  0.0003660619258880615
lambda is : 0.0681292069057962, cost : 36.242 min
==========
At iteration 209, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  210 ; minimum lost =  0.0982474684715271 ; diff loss =  5.513429641723633e-07 ; diff weight =  0.00035549464519135654
lambda is : 0.04641588833612786, cost : 41.871 min
==========
At iteration 247, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  248 ; minimum lost =  0.04813515394926071 ; diff loss =  5.662441253662109e-07 ; diff weight =  0.007764968555420637
lambda is : 0.014677992676220709, cost : 49.378 min
==========
At iteration 249, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  250 ; minimum lost =  0.06129712611436844 ; diff loss =  2.60770320892334e-07 ; diff weight =  0.00043129618279635906
lambda is : 0.02154434690031885, cost : 49.771 min
==========
At iteration 266, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  267 ; minimum lost =  0.03031500242650509 ; diff loss =  9.387731552124023e-07 ; diff weight =  0.0018188192043453455
lambda is : 0.006812920690579613, cost : 52.838 min
==========
At iteration 283, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  284 ; minimum lost =  0.02457336336374283 ; diff loss =  5.494803190231323e-07 ; diff weight =  0.0007540577207691967
lambda is : 0.004641588833612781, cost : 55.871 min
==========
At iteration 290, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  291 ; minimum lost =  0.03782055899500847 ; diff loss =  3.6135315895080566e-07 ; diff weight =  0.0008816584595479071
lambda is : 0.010000000000000004, cost : 57.721 min
==========
At iteration 312, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  313 ; minimum lost =  0.0201849564909935 ; diff loss =  7.562339305877686e-07 ; diff weight =  0.00356885464861989
lambda is : 0.003162277660168382, cost : 61.763 min
==========
At iteration 320, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  321 ; minimum lost =  0.01693733036518097 ; diff loss =  9.164214134216309e-07 ; diff weight =  0.002764899982139468
lambda is : 0.0021544346900318843, cost : 63.536 min
==========
At iteration 351, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  352 ; minimum lost =  0.07756327092647552 ; diff loss =  5.960464477539063e-08 ; diff weight =  0.00030819198582321405
lambda is : 0.0316227766016838, cost : 68.869 min
==========
At iteration 377, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  378 ; minimum lost =  0.014426002278923988 ; diff loss =  5.811452865600586e-07 ; diff weight =  0.0012546859215945005
lambda is : 0.0014677992676220694, cost : 73.911 min
==========
At iteration 380, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  381 ; minimum lost =  0.012604089453816414 ; diff loss =  9.890645742416382e-07 ; diff weight =  0.0025043163914233446
lambda is : 0.0010000000000000002, cost : 73.961 min
==========
At iteration 404, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  405 ; minimum lost =  0.010028593242168427 ; diff loss =  9.294599294662476e-07 ; diff weight =  0.0029691678937524557
lambda is : 0.00046415888336127795, cost : 79.101 min
==========
At iteration 416, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  417 ; minimum lost =  0.011129196733236313 ; diff loss =  7.413327693939209e-07 ; diff weight =  0.0028818605933338404
lambda is : 0.0006812920690579617, cost : 80.842 min
==========
At iteration 463, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  464 ; minimum lost =  0.008979209698736668 ; diff loss =  9.210780262947083e-07 ; diff weight =  0.0036024455912411213
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00031622776601683783, cost : 89.744 min
==========
At iteration 480, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  481 ; minimum lost =  0.008002351969480515 ; diff loss =  9.96515154838562e-07 ; diff weight =  0.0038146022707223892
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 93.28 min
==========
At iteration 487, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  488 ; minimum lost =  0.0038906384725123644 ; diff loss =  7.038470357656479e-07 ; diff weight =  0.00507316505536437
lambda is : 2.1544346900318854e-05, cost : 93.988 min
==========
At iteration 485, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  486 ; minimum lost =  0.004976604133844376 ; diff loss =  8.223578333854675e-07 ; diff weight =  0.00383214233443141
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 4.6415888336127784e-05, cost : 94.337 min
==========
At iteration 498, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  499 ; minimum lost =  0.0028889807872474194 ; diff loss =  9.220093488693237e-07 ; diff weight =  0.002566214418038726
lambda is : 9.999999999999997e-06, cost : 96.777 min
==========
At iteration 505, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  506 ; minimum lost =  0.003298883093520999 ; diff loss =  5.713663995265961e-07 ; diff weight =  0.0016656122170388699
lambda is : 1.4677992676220687e-05, cost : 97.491 min
==========
At iteration 519, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  520 ; minimum lost =  0.007055189460515976 ; diff loss =  9.75094735622406e-07 ; diff weight =  0.00475976150482893
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00014677992676220703, cost : 99.941 min
==========
At iteration 539, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  540 ; minimum lost =  0.00418598297983408 ; diff loss =  9.564682841300964e-07 ; diff weight =  0.008335375227034092
lambda is : 3.16227766016838e-05, cost : 103.954 min
==========
At iteration 550, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  551 ; minimum lost =  0.005435187369585037 ; diff loss =  9.87667590379715e-07 ; diff weight =  0.006317564751952887
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 6.81292069057961e-05, cost : 105.074 min
==========
At iteration 559, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  560 ; minimum lost =  0.0061689759604632854 ; diff loss =  9.676441550254822e-07 ; diff weight =  0.005857111886143684
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 9.999999999999991e-05, cost : 105.168 min
==========
*** Collecting results ***
Exporting result Dict
Platelet Time elapsed: 105.24955523411433 minutes.
Representative adata: (57515, 27504)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for Treg
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda: Lambda: Lambda: Lambda:1e-05 Lambda:Lambda:1.5e-05 Lambda: Lambda:2.2e-05 Lambda: Lambda:Lambda:Lambda:starting at Lambda: Lambda: Lambda:Lambda:starting at Lambda:Lambda: Lambda:0.0001 Lambda: Lambda:Lambda:starting at Lambda:  Lambda:Lambda:3.2e-05    2024-10-03 09:04:03  6.8e-05  4.6e-05   2024-10-03 09:04:03  0.006813  starting at  0.000147   2024-10-03 09:04:03 0.046416 0.000215   starting at 0.000316 0.000464 0.000681 Max_iter: 0.001 starting at 0.001468 starting at 0.002154 0.003162 Max_iter: 0.004642 starting at 0.01 2024-10-03 09:04:03 0.014678 starting at 0.021544 0.031623 Max_iter: starting at starting at 0.068129 0.1 2024-10-03 09:04:03 starting at starting at starting at 1000
starting at 2024-10-03 09:04:03 starting at 2024-10-03 09:04:03 starting at starting at 1000starting at 2024-10-03 09:04:03 starting at Max_iter: starting at 2024-10-03 09:04:03 starting at starting at 10002024-10-03 09:04:03 2024-10-03 09:04:03 starting at starting at Max_iter: 2024-10-03 09:04:03 2024-10-03 09:04:03 2024-10-03 09:04:03 2024-10-03 09:04:03 Max_iter: 2024-10-03 09:04:03 Max_iter: 2024-10-03 09:04:032024-10-03 09:04:03
2024-10-03 09:04:03 Max_iter: 2024-10-03 09:04:0310002024-10-03 09:04:03 Max_iter:2024-10-03 09:04:032024-10-03 09:04:03
Max_iter:Max_iter: 2024-10-03 09:04:032024-10-03 09:04:031000Max_iter:Max_iter:Max_iter:Max_iter: 1000Max_iter: 1000  Max_iter:1000 
Max_iter:    1000  
   1000
1000

Max_iter: Max_iter:  1000
Max_iter:  10001000
Max_iter:Max_iter: 1000
Max_iter:Max_iter:100010001000
10001000
1000
 1000

  





1000
10001000

At iteration 181, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  182 ; minimum lost =  0.17947518825531006 ; diff loss =  3.725290298461914e-07 ; diff weight =  3.3175572752952576e-05
lambda is : 0.10000000000000002, cost : 36.56 min
==========
At iteration 184, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  185 ; minimum lost =  0.1479622721672058 ; diff loss =  8.940696716308594e-08 ; diff weight =  0.0001162808621302247
lambda is : 0.04641588833612786, cost : 37.454 min
==========
At iteration 203, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  204 ; minimum lost =  0.16193972527980804 ; diff loss =  7.450580596923828e-07 ; diff weight =  0.0003925022028852254
lambda is : 0.0681292069057962, cost : 41.222 min
==========
At iteration 235, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  236 ; minimum lost =  0.127463698387146 ; diff loss =  9.5367431640625e-07 ; diff weight =  0.001802393700927496
lambda is : 0.02154434690031885, cost : 46.859 min
==========
At iteration 248, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  249 ; minimum lost =  0.1165112853050232 ; diff loss =  5.513429641723633e-07 ; diff weight =  0.0004319551517255604
lambda is : 0.010000000000000004, cost : 49.794 min
==========
At iteration 251, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  252 ; minimum lost =  0.13639818131923676 ; diff loss =  4.917383193969727e-07 ; diff weight =  0.0016447282396256924
lambda is : 0.0316227766016838, cost : 50.32 min
==========
At iteration 269, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  270 ; minimum lost =  0.12099693715572357 ; diff loss =  1.862645149230957e-07 ; diff weight =  4.768464805238182e-06
lambda is : 0.014677992676220709, cost : 54.187 min
==========
At iteration 422, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  423 ; minimum lost =  0.10927371680736542 ; diff loss =  9.462237358093262e-07 ; diff weight =  0.0016477282624691725
lambda is : 0.006812920690579613, cost : 82.126 min
==========
At iteration 443, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  444 ; minimum lost =  0.021821916103363037 ; diff loss =  8.586794137954712e-07 ; diff weight =  0.0005696173757314682
lambda is : 6.81292069057961e-05, cost : 86.306 min
==========
At iteration 471, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  472 ; minimum lost =  0.08849677443504333 ; diff loss =  9.834766387939453e-07 ; diff weight =  0.0018072808161377907
lambda is : 0.003162277660168382, cost : 90.65 min
==========
At iteration 470, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  470 ; minimum lost =  0.018742011860013008 ; diff loss =  -2.7008354663848877e-07 ; diff weight =  0.010644013993442059
lambda is : 4.6415888336127784e-05, cost : 91.089 min
==========
At iteration 481, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  481 ; minimum lost =  0.06773867458105087 ; diff loss =  -1.7881393432617188e-07 ; diff weight =  0.006997892167419195
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0014677992676220694, cost : 93.541 min
==========
At iteration 480, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  481 ; minimum lost =  0.09916255623102188 ; diff loss =  9.238719940185547e-07 ; diff weight =  0.0016985067632049322
lambda is : 0.004641588833612781, cost : 93.654 min
==========
At iteration 488, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  489 ; minimum lost =  0.03212643042206764 ; diff loss =  5.438923835754395e-07 ; diff weight =  0.002049216767773032
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 94.183 min
==========
At iteration 487, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  488 ; minimum lost =  0.013889783062040806 ; diff loss =  9.639188647270203e-07 ; diff weight =  0.0010969673749059439
lambda is : 2.1544346900318854e-05, cost : 94.765 min
==========
At iteration 481, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  481 ; minimum lost =  0.058377817273139954 ; diff loss =  -1.0058283805847168e-07 ; diff weight =  0.005069685634225607
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0010000000000000002, cost : 94.807 min
==========
At iteration 493, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  494 ; minimum lost =  0.05004081875085831 ; diff loss =  9.387731552124023e-07 ; diff weight =  0.0030960782896727324
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0006812920690579617, cost : 95.607 min
==========
At iteration 499, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  500 ; minimum lost =  0.02447563037276268 ; diff loss =  5.848705768585205e-07 ; diff weight =  0.00020058799418620765
At iteration 497, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  498 ; minimum lost =  0.015942132100462914 ; diff loss =  6.351619958877563e-07 ; diff weight =  0.0007844381034374237
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 9.999999999999991e-05, cost : 96.758 min
==========
lambda is : 3.16227766016838e-05, cost : 96.911 min
==========
At iteration 497, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  498 ; minimum lost =  0.011723486706614494 ; diff loss =  8.530914783477783e-07 ; diff weight =  0.003171408548951149
lambda is : 1.4677992676220687e-05, cost : 97.382 min
==========
At iteration 520, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  521 ; minimum lost =  0.02806280180811882 ; diff loss =  6.407499313354492e-07 ; diff weight =  0.0012400202685967088
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00014677992676220703, cost : 99.984 min
==========
At iteration 520, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  521 ; minimum lost =  0.03690091148018837 ; diff loss =  4.0978193283081055e-07 ; diff weight =  0.0036529162898659706
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00031622776601683783, cost : 101.011 min
==========
At iteration 519, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  520 ; minimum lost =  0.07793498039245605 ; diff loss =  9.98377799987793e-07 ; diff weight =  0.001991309691220522
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0021544346900318843, cost : 101.391 min
==========
At iteration 538, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  539 ; minimum lost =  0.04283618554472923 ; diff loss =  7.115304470062256e-07 ; diff weight =  0.001577305723913014
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00046415888336127795, cost : 103.415 min
==========
At iteration 558, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  559 ; minimum lost =  0.009129717014729977 ; diff loss =  8.475035429000854e-07 ; diff weight =  0.0018316525965929031
lambda is : 9.999999999999997e-06, cost : 106.221 min
==========
*** Collecting results ***
Exporting result Dict
Treg Time elapsed: 106.29287745952607 minutes.
Representative adata: (57515, 27504)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for cDC1
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda:Lambda:Lambda:Lambda:Lambda:Lambda: Lambda:   Lambda: Lambda:Lambda:Lambda: Lambda: Lambda:1e-05 Lambda: Lambda:1.5e-05 Lambda: Lambda: 2.2e-05 3.2e-05  Lambda:4.6e-05 Lambda: Lambda: Lambda: Lambda: Lambda: Lambda: Lambda:0.000215 0.000464  starting at  6.8e-05  starting at 0.002154 0.003162 starting at starting at 0.000147  starting at 0.014678  0.01  0.0001 0.031623  0.000316 0.068129 starting at starting at 0.000681 2024-10-03 10:50:27 0.001 starting at 0.001468 2024-10-03 10:50:27 starting at starting at 2024-10-03 10:50:27 2024-10-03 10:50:27 starting at 0.006813 2024-10-03 10:50:27 starting at 0.004642 starting at 0.021544 starting at starting at 0.046416 starting at  starting at0.1 2024-10-03 10:50:27 2024-10-03 10:50:27 starting at Max_iter: starting at 2024-10-03 10:50:27 starting at Max_iter: 2024-10-03 10:50:27 2024-10-03 10:50:27 Max_iter: Max_iter: 2024-10-03 10:50:27 starting at Max_iter: 2024-10-03 10:50:27 starting at 2024-10-03 10:50:27 starting at 2024-10-03 10:50:27 2024-10-03 10:50:27 starting at 2024-10-03 10:50:27  2024-10-03 10:50:27starting at Max_iter: Max_iter: 2024-10-03 10:50:27 10002024-10-03 10:50:27 Max_iter: 2024-10-03 10:50:27 1000Max_iter: Max_iter: 10001000Max_iter: 2024-10-03 10:50:27 1000Max_iter: 2024-10-03 10:50:27 Max_iter: 2024-10-03 10:50:27 Max_iter: Max_iter: 2024-10-03 10:50:27 Max_iter:  Max_iter:2024-10-03 10:50:27 10001000Max_iter: 
Max_iter: 1000Max_iter: 
10001000

1000Max_iter:
1000Max_iter: 1000Max_iter: 10001000Max_iter:1000 Max_iter:

1000
1000

1000


 1000
1000

1000


 
1000 1000

1000


At iteration 104, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  105 ; minimum lost =  0.020255856215953827 ; diff loss =  7.115304470062256e-07 ; diff weight =  0.001109725795686245
lambda is : 0.004641588833612781, cost : 21.662 min
==========
At iteration 107, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  108 ; minimum lost =  0.030401602387428284 ; diff loss =  5.438923835754395e-07 ; diff weight =  0.0007362727192230523
lambda is : 0.014677992676220709, cost : 21.907 min
==========
At iteration 112, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  113 ; minimum lost =  0.02587314508855343 ; diff loss =  1.955777406692505e-07 ; diff weight =  9.761021647136658e-05
lambda is : 0.010000000000000004, cost : 22.701 min
==========
At iteration 115, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  116 ; minimum lost =  0.022671138867735863 ; diff loss =  1.862645149230957e-09 ; diff weight =  2.2292137145996094e-05
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.006812920690579613, cost : 23.605 min
==========
At iteration 125, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  126 ; minimum lost =  0.036607369780540466 ; diff loss =  8.158385753631592e-07 ; diff weight =  0.000805974006652832
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.02154434690031885, cost : 25.505 min
==========
At iteration 126, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  127 ; minimum lost =  0.04565233737230301 ; diff loss =  6.295740604400635e-07 ; diff weight =  0.0005935001536272466
lambda is : 0.0316227766016838, cost : 25.606 min
==========
At iteration 133, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  134 ; minimum lost =  0.0742085874080658 ; diff loss =  2.0116567611694336e-07 ; diff weight =  0.00027304887771606445
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.0681292069057962, cost : 26.763 min
==========
At iteration 133, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  134 ; minimum lost =  0.057564012706279755 ; diff loss =  7.189810276031494e-07 ; diff weight =  0.0005959272384643555
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.04641588833612786, cost : 27.098 min
==========
At iteration 143, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  144 ; minimum lost =  0.09675657749176025 ; diff loss =  7.674098014831543e-07 ; diff weight =  0.0004673004150390625
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.10000000000000002, cost : 28.881 min
==========
At iteration 209, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  210 ; minimum lost =  0.01660171151161194 ; diff loss =  9.08970832824707e-07 ; diff weight =  0.001989633310586214
lambda is : 0.003162277660168382, cost : 41.202 min
==========
At iteration 239, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  240 ; minimum lost =  0.013054190203547478 ; diff loss =  1.0337680578231812e-07 ; diff weight =  0.0008659696904942393
lambda is : 0.0021544346900318843, cost : 47.653 min
==========
At iteration 264, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  265 ; minimum lost =  0.009993896819651127 ; diff loss =  8.698552846908569e-07 ; diff weight =  0.005752322729676962
lambda is : 0.0014677992676220694, cost : 51.864 min
==========
At iteration 262, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  263 ; minimum lost =  0.0075761135667562485 ; diff loss =  6.193295121192932e-07 ; diff weight =  0.0010955639882013202
lambda is : 0.0010000000000000002, cost : 51.966 min
==========
At iteration 273, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  274 ; minimum lost =  0.005713778547942638 ; diff loss =  5.168840289115906e-07 ; diff weight =  0.004068922717124224
lambda is : 0.0006812920690579617, cost : 53.806 min
==========
At iteration 310, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  311 ; minimum lost =  0.004289241507649422 ; diff loss =  7.208436727523804e-07 ; diff weight =  0.00522665586322546
lambda is : 0.00046415888336127795, cost : 59.813 min
==========
At iteration 320, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  321 ; minimum lost =  0.0032100435346364975 ; diff loss =  7.909256964921951e-07 ; diff weight =  0.008634975180029869
lambda is : 0.00031622776601683783, cost : 62.671 min
==========
At iteration 325, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  326 ; minimum lost =  0.0023956135846674442 ; diff loss =  7.671769708395004e-07 ; diff weight =  0.009759416803717613
lambda is : 0.0002154434690031884, cost : 63.327 min
==========
At iteration 331, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  332 ; minimum lost =  0.0018058344721794128 ; diff loss =  6.640329957008362e-07 ; diff weight =  0.008321357890963554
lambda is : 0.00014677992676220703, cost : 63.66 min
==========
At iteration 343, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  344 ; minimum lost =  0.001355569576844573 ; diff loss =  8.649658411741257e-07 ; diff weight =  0.013014904223382473
lambda is : 9.999999999999991e-05, cost : 66.054 min
==========
At iteration 356, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  357 ; minimum lost =  0.00103893899358809 ; diff loss =  9.960494935512543e-07 ; diff weight =  0.01918591558933258
lambda is : 6.81292069057961e-05, cost : 68.216 min
==========
At iteration 376, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  377 ; minimum lost =  0.0007722381269559264 ; diff loss =  8.798670023679733e-07 ; diff weight =  0.014089789241552353
lambda is : 4.6415888336127784e-05, cost : 72.585 min
==========
At iteration 384, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  385 ; minimum lost =  0.0006040699081495404 ; diff loss =  9.656650945544243e-07 ; diff weight =  0.025337206199765205
lambda is : 3.16227766016838e-05, cost : 73.918 min
==========
At iteration 410, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  411 ; minimum lost =  0.00045759417116642 ; diff loss =  8.477945812046528e-07 ; diff weight =  0.04587696120142937
lambda is : 2.1544346900318854e-05, cost : 79.335 min
==========
At iteration 424, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  425 ; minimum lost =  0.0003598573384806514 ; diff loss =  9.877549018710852e-07 ; diff weight =  0.05146867409348488
lambda is : 1.4677992676220687e-05, cost : 80.874 min
==========
At iteration 444, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  445 ; minimum lost =  0.0002853379992302507 ; diff loss =  9.21980245038867e-07 ; diff weight =  0.043917037546634674
lambda is : 9.999999999999997e-06, cost : 83.558 min
==========
*** Collecting results ***
Exporting result Dict
cDC1 Time elapsed: 83.6321184237798 minutes.
Representative adata: (57515, 27504)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for cDC2
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda: Lambda:Lambda:Lambda:Lambda: Lambda:Lambda:1e-05 Lambda: Lambda: Lambda:Lambda: Lambda:Lambda:Lambda:3.2e-05 Lambda:  Lambda:Lambda:starting at Lambda: Lambda:1.5e-05 Lambda: Lambda:2.2e-05 Lambda: Lambda: Lambda:Lambda: 0.000215    starting at  4.6e-05 6.8e-05   2024-10-03 12:14:10  0.0001  starting at  0.000147  starting at  0.014678  0.000316  0.1 starting at 0.068129 0.000464 0.000681 2024-10-03 12:14:10 0.001 starting at starting at 0.001468 0.002154 Max_iter: 0.003162 starting at 0.004642 2024-10-03 12:14:10 0.006813 starting at 0.01 2024-10-03 12:14:10 0.021544 starting at 0.031623 starting at 0.046416 starting at 2024-10-03 12:14:10 starting at starting at starting at Max_iter: starting at 2024-10-03 12:14:10 2024-10-03 12:14:10 starting at starting at 1000starting at 2024-10-03 12:14:10 starting at Max_iter: starting at 2024-10-03 12:14:10 starting at Max_iter: starting at 2024-10-03 12:14:10 starting at 2024-10-03 12:14:10 starting at 2024-10-03 12:14:10 Max_iter: 2024-10-03 12:14:10 2024-10-03 12:14:10 2024-10-03 12:14:10 10002024-10-03 12:14:10 Max_iter: Max_iter: 2024-10-03 12:14:10 2024-10-03 12:14:10 
2024-10-03 12:14:10 Max_iter: 2024-10-03 12:14:10 10002024-10-03 12:14:10 Max_iter: 2024-10-03 12:14:10 1000
2024-10-03 12:14:10 Max_iter: 2024-10-03 12:14:10 Max_iter: 2024-10-03 12:14:10 Max_iter: 1000
Max_iter: Max_iter: Max_iter: 
Max_iter: 10001000Max_iter:Max_iter: Max_iter:1000Max_iter:
Max_iter: 1000Max_iter: Max_iter: 1000Max_iter: 1000
Max_iter: 1000
1000
10001000
1000

 10001000
 1000
 10001000
10001000
1000
1000








At iteration 174, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  175 ; minimum lost =  0.146035835146904 ; diff loss =  1.341104507446289e-07 ; diff weight =  0.00010544061660766602
lambda is : 0.0681292069057962, cost : 35.338 min
==========
At iteration 174, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  175 ; minimum lost =  0.1641390323638916 ; diff loss =  5.662441253662109e-07 ; diff weight =  0.00029402971267700195
lambda is : 0.10000000000000002, cost : 35.467 min
==========
At iteration 191, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  191 ; minimum lost =  0.13205191493034363 ; diff loss =  -5.066394805908203e-07 ; diff weight =  0.006799876689910889
lambda is : 0.04641588833612786, cost : 38.694 min
==========
At iteration 245, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  246 ; minimum lost =  0.11585349589586258 ; diff loss =  5.21540641784668e-08 ; diff weight =  0.00010113186726812273
lambda is : 0.0316227766016838, cost : 49.19 min
==========
At iteration 260, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  260 ; minimum lost =  0.09754785895347595 ; diff loss =  -4.3958425521850586e-07 ; diff weight =  0.006704062223434448
lambda is : 0.02154434690031885, cost : 51.997 min
==========
At iteration 334, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  335 ; minimum lost =  0.07893803715705872 ; diff loss =  9.685754776000977e-07 ; diff weight =  0.0012401011772453785
lambda is : 0.014677992676220709, cost : 65.771 min
==========
At iteration 370, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  371 ; minimum lost =  0.04133287072181702 ; diff loss =  9.834766387939453e-07 ; diff weight =  0.0017376263858750463
lambda is : 0.004641588833612781, cost : 72.142 min
==========
At iteration 379, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  380 ; minimum lost =  0.06372174620628357 ; diff loss =  8.642673492431641e-07 ; diff weight =  0.00041704034083522856
lambda is : 0.010000000000000004, cost : 74.687 min
==========
At iteration 382, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  383 ; minimum lost =  0.0335782989859581 ; diff loss =  2.868473529815674e-07 ; diff weight =  0.001110521494410932
lambda is : 0.003162277660168382, cost : 75.046 min
==========
At iteration 412, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  413 ; minimum lost =  0.027656536549329758 ; diff loss =  8.344650268554688e-07 ; diff weight =  0.0021484652534127235
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0021544346900318843, cost : 80.498 min
==========
At iteration 443, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  444 ; minimum lost =  0.05115766078233719 ; diff loss =  8.940696716308594e-07 ; diff weight =  0.002469428349286318
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.006812920690579613, cost : 85.557 min
==========
At iteration 446, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  447 ; minimum lost =  0.014014477841556072 ; diff loss =  9.033828973770142e-07 ; diff weight =  0.00755859212949872
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00046415888336127795, cost : 87.091 min
==========
At iteration 463, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  464 ; minimum lost =  0.022935088723897934 ; diff loss =  4.6193599700927734e-07 ; diff weight =  0.001513731898739934
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0014677992676220694, cost : 90.362 min
==========
At iteration 470, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  471 ; minimum lost =  0.019266270101070404 ; diff loss =  8.791685104370117e-07 ; diff weight =  0.0009123932104557753
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0010000000000000002, cost : 91.618 min
==========
At iteration 480, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  481 ; minimum lost =  0.016325190663337708 ; diff loss =  5.476176738739014e-07 ; diff weight =  0.001877772156149149
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0006812920690579617, cost : 92.688 min
==========
At iteration 500, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  501 ; minimum lost =  0.009081506170332432 ; diff loss =  8.381903171539307e-07 ; diff weight =  0.0009206043323501945
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00014677992676220703, cost : 97.05 min
==========
At iteration 508, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  509 ; minimum lost =  0.01192439068108797 ; diff loss =  8.074566721916199e-07 ; diff weight =  0.004182663280516863
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00031622776601683783, cost : 98.905 min
==========
At iteration 519, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  520 ; minimum lost =  0.005034064874053001 ; diff loss =  6.528571248054504e-07 ; diff weight =  0.0008440372766926885
At iteration 527, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  528 ; minimum lost =  0.0103016197681427 ; diff loss =  6.742775440216064e-07 ; diff weight =  0.0006083591724745929
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 101.317 min
==========
lambda is : 3.16227766016838e-05, cost : 101.469 min
==========
At iteration 533, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  534 ; minimum lost =  0.00780413206666708 ; diff loss =  6.905756890773773e-07 ; diff weight =  0.0037448653019964695
lambda is : 9.999999999999991e-05, cost : 102.005 min
==========
At iteration 541, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  542 ; minimum lost =  0.0035772621631622314 ; diff loss =  9.830109775066376e-07 ; diff weight =  0.004124223720282316
lambda is : 1.4677992676220687e-05, cost : 104.201 min
==========
At iteration 554, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  555 ; minimum lost =  0.0066511863842606544 ; diff loss =  3.9674341678619385e-07 ; diff weight =  0.0013341674348339438
lambda is : 6.81292069057961e-05, cost : 106.491 min
==========
At iteration 548, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  549 ; minimum lost =  0.003007340943440795 ; diff loss =  5.962792783975601e-07 ; diff weight =  0.0008681760518811643
lambda is : 9.999999999999997e-06, cost : 107.112 min
==========
At iteration 560, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  561 ; minimum lost =  0.004074374213814735 ; diff loss =  8.074566721916199e-07 ; diff weight =  0.0015800349647179246
lambda is : 2.1544346900318854e-05, cost : 108.668 min
==========
At iteration 563, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  564 ; minimum lost =  0.005703120492398739 ; diff loss =  8.484348654747009e-07 ; diff weight =  0.004910193849354982
lambda is : 4.6415888336127784e-05, cost : 109.211 min
==========
*** Collecting results ***
Exporting result Dict
cDC2 Time elapsed: 109.30081791877747 minutes.
Representative adata: (57515, 27504)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for dnT
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda:Lambda: Lambda: Lambda:Lambda:Lambda:1.5e-05 Lambda: Lambda:Lambda:1e-05 Lambda: Lambda: Lambda: Lambda: Lambda:starting at Lambda: Lambda: 2.2e-05  Lambda:Lambda:Lambda: Lambda: Lambda:starting at Lambda:Lambda: Lambda: Lambda:3.2e-05  4.6e-05  6.8e-05 0.000681  2024-10-03 14:03:34  0.0001 0.002154 starting at 0.003162    0.000147 0.014678  2024-10-03 14:03:34   0.000215 0.068129  starting at 0.000316 starting at 0.000464 starting at starting at 0.001 Max_iter: 0.001468 starting at starting at 2024-10-03 14:03:34 starting at 0.004642 0.006813 0.01 starting at starting at 0.021544 Max_iter: 0.031623 0.046416 starting at starting at 0.1 2024-10-03 14:03:34 starting at 2024-10-03 14:03:34 starting at 2024-10-03 14:03:34 2024-10-03 14:03:34 starting at 1000starting at 2024-10-03 14:03:34 2024-10-03 14:03:34 Max_iter: 2024-10-03 14:03:34 starting at starting at starting at 2024-10-03 14:03:34 2024-10-03 14:03:34 starting at 1000starting at starting at 2024-10-03 14:03:34 2024-10-03 14:03:34 starting at Max_iter: 2024-10-03 14:03:34 Max_iter: 2024-10-03 14:03:34 Max_iter: Max_iter: 2024-10-03 14:03:34 
2024-10-03 14:03:34 Max_iter: Max_iter: 1000Max_iter: 2024-10-03 14:03:34 2024-10-03 14:03:34 2024-10-03 14:03:34 Max_iter: Max_iter:2024-10-03 14:03:34 
2024-10-03 14:03:34 2024-10-03 14:03:34 Max_iter: Max_iter: 2024-10-03 14:03:34 1000Max_iter: 1000Max_iter: 10001000Max_iter: Max_iter: 10001000
1000Max_iter: Max_iter: Max_iter: 1000 Max_iter: Max_iter: Max_iter: 10001000Max_iter:
1000
1000

10001000


100010001000
1000100010001000

 










1000
At iteration 149, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  150 ; minimum lost =  0.08798205852508545 ; diff loss =  7.525086402893066e-07 ; diff weight =  0.00046247243881225586
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.0681292069057962, cost : 30.304 min
==========
At iteration 151, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  152 ; minimum lost =  0.10904888808727264 ; diff loss =  7.003545761108398e-07 ; diff weight =  0.00039845705032348633
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.10000000000000002, cost : 30.492 min
==========
At iteration 155, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  156 ; minimum lost =  0.06064155697822571 ; diff loss =  1.5273690223693848e-07 ; diff weight =  0.00010874222789425403
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.0316227766016838, cost : 31.465 min
==========
At iteration 156, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  157 ; minimum lost =  0.07181675732135773 ; diff loss =  7.599592208862305e-07 ; diff weight =  0.0005398392677307129
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.04641588833612786, cost : 31.555 min
==========
At iteration 158, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  159 ; minimum lost =  0.0467224046587944 ; diff loss =  8.642673492431641e-07 ; diff weight =  0.00180981180164963
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.014677992676220709, cost : 31.95 min
==========
At iteration 162, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  163 ; minimum lost =  0.05249333381652832 ; diff loss =  4.470348358154297e-08 ; diff weight =  0.0001390578836435452
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.02154434690031885, cost : 32.796 min
==========
At iteration 179, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  180 ; minimum lost =  0.03905779868364334 ; diff loss =  8.940696716308594e-07 ; diff weight =  0.0015020871069282293
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.006812920690579613, cost : 36.222 min
==========
At iteration 184, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  185 ; minimum lost =  0.042250894010066986 ; diff loss =  8.270144462585449e-07 ; diff weight =  5.757411418017e-05
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.010000000000000004, cost : 36.877 min
==========
At iteration 268, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  269 ; minimum lost =  0.02774411253631115 ; diff loss =  9.10833477973938e-07 ; diff weight =  0.0019607499707490206
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0021544346900318843, cost : 52.952 min
==========
At iteration 287, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  288 ; minimum lost =  0.03158272057771683 ; diff loss =  7.711350917816162e-07 ; diff weight =  0.0010676851961761713
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.003162277660168382, cost : 56.515 min
==========
At iteration 293, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  294 ; minimum lost =  0.03547615185379982 ; diff loss =  7.189810276031494e-07 ; diff weight =  0.0016639785608276725
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.004641588833612781, cost : 56.883 min
==========
At iteration 299, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  300 ; minimum lost =  0.02078845351934433 ; diff loss =  9.98377799987793e-07 ; diff weight =  0.0025252762716263533
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0010000000000000002, cost : 58.549 min
==========
At iteration 321, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  322 ; minimum lost =  0.02406543679535389 ; diff loss =  9.872019290924072e-07 ; diff weight =  0.0026990342885255814
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0014677992676220694, cost : 62.296 min
==========
At iteration 328, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  329 ; minimum lost =  0.017965640872716904 ; diff loss =  8.922070264816284e-07 ; diff weight =  0.002609111601486802
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0006812920690579617, cost : 63.931 min
==========
At iteration 358, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  359 ; minimum lost =  0.015610912814736366 ; diff loss =  9.834766387939453e-07 ; diff weight =  0.0031024201307445765
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00046415888336127795, cost : 69.588 min
==========
At iteration 383, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  384 ; minimum lost =  0.005141247995197773 ; diff loss =  9.299255907535553e-07 ; diff weight =  0.0011437818175181746
lambda is : 2.1544346900318854e-05, cost : 74.313 min
==========
At iteration 386, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  387 ; minimum lost =  0.012074457481503487 ; diff loss =  9.480863809585571e-07 ; diff weight =  0.007696183398365974
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 75.056 min
==========
At iteration 387, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  388 ; minimum lost =  0.013679055497050285 ; diff loss =  9.741634130477905e-07 ; diff weight =  0.0034268114250153303
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00031622776601683783, cost : 75.742 min
==========
At iteration 404, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  405 ; minimum lost =  0.00825190357863903 ; diff loss =  8.996576070785522e-07 ; diff weight =  0.0016706599853932858
lambda is : 6.81292069057961e-05, cost : 78.178 min
==========
At iteration 412, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  413 ; minimum lost =  0.009445203468203545 ; diff loss =  7.776543498039246e-07 ; diff weight =  0.0039066290482878685
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 9.999999999999991e-05, cost : 79.004 min
==========
At iteration 407, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  408 ; minimum lost =  0.00712144561111927 ; diff loss =  7.292255759239197e-07 ; diff weight =  0.0009728763834573328
lambda is : 4.6415888336127784e-05, cost : 79.36 min
==========
At iteration 416, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  417 ; minimum lost =  0.01068542804569006 ; diff loss =  9.71369445323944e-07 ; diff weight =  0.0042501469142735004
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00014677992676220703, cost : 80.075 min
==========
At iteration 421, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  422 ; minimum lost =  0.005964655429124832 ; diff loss =  8.577480912208557e-07 ; diff weight =  0.0013071979628875852
lambda is : 3.16227766016838e-05, cost : 80.855 min
==========
At iteration 450, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  451 ; minimum lost =  0.003145696595311165 ; diff loss =  7.543712854385376e-07 ; diff weight =  0.0018205081578344107
lambda is : 9.999999999999997e-06, cost : 86.65 min
==========
At iteration 493, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  494 ; minimum lost =  0.003772587515413761 ; diff loss =  7.534399628639221e-07 ; diff weight =  0.011709415353834629
lambda is : 1.4677992676220687e-05, cost : 93.403 min
==========
*** Collecting results ***
Exporting result Dict
dnT Time elapsed: 93.47565737565358 minutes.
Representative adata: (57515, 27504)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for gdT
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda:Lambda:Lambda:Lambda:Lambda:  Lambda: Lambda:Lambda:Lambda: Lambda: Lambda:Lambda:1e-05 Lambda:Lambda:3.2e-05 Lambda: Lambda:1.5e-05 Lambda:   Lambda:2.2e-05 Lambda: Lambda:0.0001  Lambda: Lambda:Lambda:Lambda:starting at Lambda:   starting at  4.6e-05  starting at  0.001 6.8e-05 0.002154  starting at  0.000147  starting at 0.01  0.000215    2024-10-03 15:37:08 0.068129 0.000316 0.1 2024-10-03 15:37:08 0.000464 starting at 0.000681 2024-10-03 15:37:08 0.001468 starting at starting at starting at 0.003162 2024-10-03 15:37:08 0.004642 starting at 0.006813 2024-10-03 15:37:08 starting at 0.014678 starting at 0.021544 0.031623 0.046416 Max_iter: starting at starting at starting at Max_iter: starting at 2024-10-03 15:37:08 starting at Max_iter: starting at 2024-10-03 15:37:08 2024-10-03 15:37:08 2024-10-03 15:37:08 starting at Max_iter: starting at 2024-10-03 15:37:08 starting at Max_iter: 2024-10-03 15:37:08 starting at 2024-10-03 15:37:08 starting at starting at starting at 10002024-10-03 15:37:08 2024-10-03 15:37:08 2024-10-03 15:37:08 10002024-10-03 15:37:08 Max_iter: 2024-10-03 15:37:08 10002024-10-03 15:37:08 Max_iter: Max_iter: Max_iter: 2024-10-03 15:37:08 10002024-10-03 15:37:08 Max_iter: 2024-10-03 15:37:08 1000Max_iter: 2024-10-03 15:37:08 Max_iter: 2024-10-03 15:37:08 2024-10-03 15:37:08 2024-10-03 15:37:08 
Max_iter: Max_iter: Max_iter: 
Max_iter: 1000
Max_iter: 
Max_iter: 100010001000Max_iter: 
Max_iter: 1000Max_iter: 
1000Max_iter:1000
Max_iter: Max_iter: Max_iter: 1000
1000
1000
1000
10001000


10001000
1000
 100010001000
1000







At iteration 194, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  195 ; minimum lost =  0.20646724104881287 ; diff loss =  6.854534149169922e-07 ; diff weight =  0.014344786293804646
lambda is : 0.10000000000000002, cost : 39.443 min
==========
At iteration 228, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  229 ; minimum lost =  0.18994703888893127 ; diff loss =  1.1920928955078125e-07 ; diff weight =  7.053336594253778e-05
lambda is : 0.0681292069057962, cost : 45.882 min
==========
At iteration 316, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  317 ; minimum lost =  0.17582590878009796 ; diff loss =  6.407499313354492e-07 ; diff weight =  0.0002785807882901281
lambda is : 0.04641588833612786, cost : 63.275 min
==========
At iteration 397, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  398 ; minimum lost =  0.13344547152519226 ; diff loss =  9.5367431640625e-07 ; diff weight =  0.0007299847202375531
lambda is : 0.02154434690031885, cost : 78.471 min
==========
At iteration 408, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  408 ; minimum lost =  0.062344666570425034 ; diff loss =  -4.4330954551696777e-07 ; diff weight =  0.005666561424732208
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.003162277660168382, cost : 81.135 min
==========
At iteration 419, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  420 ; minimum lost =  0.1541973352432251 ; diff loss =  3.8743019104003906e-07 ; diff weight =  0.0003228172427043319
lambda is : 0.0316227766016838, cost : 82.751 min
==========
At iteration 425, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  425 ; minimum lost =  0.04323993995785713 ; diff loss =  -7.040798664093018e-07 ; diff weight =  0.00834609568119049
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0010000000000000002, cost : 84.113 min
==========
At iteration 435, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  436 ; minimum lost =  0.11448465287685394 ; diff loss =  9.760260581970215e-07 ; diff weight =  0.0007935523171909153
lambda is : 0.014677992676220709, cost : 86.163 min
==========
At iteration 461, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  462 ; minimum lost =  0.072054922580719 ; diff loss =  8.940696716308594e-07 ; diff weight =  0.0010853457497432828
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.004641588833612781, cost : 90.294 min
==========
At iteration 471, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  472 ; minimum lost =  0.08389253914356232 ; diff loss =  8.493661880493164e-07 ; diff weight =  0.0011763483053073287
At iteration 470, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  470 ; minimum lost =  0.047994352877140045 ; diff loss =  -1.564621925354004e-07 ; diff weight =  0.004781653173267841
lambda is : 0.006812920690579613, cost : 92.665 min
==========
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0014677992676220694, cost : 92.678 min
==========
At iteration 473, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  474 ; minimum lost =  0.09799844771623611 ; diff loss =  3.7997961044311523e-07 ; diff weight =  0.00022239939426071942
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.010000000000000004, cost : 93.488 min
==========
At iteration 489, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  489 ; minimum lost =  0.05416528135538101 ; diff loss =  -7.82310962677002e-08 ; diff weight =  0.0023839056957513094
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0021544346900318843, cost : 97.405 min
==========
At iteration 505, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  505 ; minimum lost =  0.03086133301258087 ; diff loss =  -5.085021257400513e-07 ; diff weight =  0.016230640932917595
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 100.192 min
==========
At iteration 518, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  519 ; minimum lost =  0.028061959892511368 ; diff loss =  8.046627044677734e-07 ; diff weight =  0.004081546328961849
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00014677992676220703, cost : 100.777 min
==========
At iteration 512, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  512 ; minimum lost =  0.02536756731569767 ; diff loss =  -7.692724466323853e-07 ; diff weight =  0.009278564713895321
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 9.999999999999991e-05, cost : 101.002 min
==========
At iteration 519, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  519 ; minimum lost =  0.0359647162258625 ; diff loss =  -2.1979212760925293e-07 ; diff weight =  0.006338528357446194
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00046415888336127795, cost : 101.968 min
==========
At iteration 518, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  519 ; minimum lost =  0.03901177644729614 ; diff loss =  8.568167686462402e-07 ; diff weight =  0.0015696247573941946
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0006812920690579617, cost : 102.396 min
==========
At iteration 529, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  530 ; minimum lost =  0.022281896322965622 ; diff loss =  5.662441253662109e-07 ; diff weight =  0.0005603810423053801
lambda is : 6.81292069057961e-05, cost : 103.617 min
==========
At iteration 529, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  529 ; minimum lost =  0.01281594019383192 ; diff loss =  -3.417953848838806e-07 ; diff weight =  0.01427843701094389
At iteration 533, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  534 ; minimum lost =  0.01963643729686737 ; diff loss =  7.413327693939209e-07 ; diff weight =  0.0007708812481723726
lambda is : 1.4677992676220687e-05, cost : 104.794 min
==========
lambda is : 4.6415888336127784e-05, cost : 104.89 min
==========
At iteration 542, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  543 ; minimum lost =  0.01466208416968584 ; diff loss =  7.832422852516174e-07 ; diff weight =  0.00035593812935985625
lambda is : 2.1544346900318854e-05, cost : 105.719 min
==========
At iteration 549, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  549 ; minimum lost =  0.0168635081499815 ; diff loss =  -9.145587682723999e-07 ; diff weight =  0.009765053167939186
lambda is : 3.16227766016838e-05, cost : 106.751 min
==========
At iteration 556, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  557 ; minimum lost =  0.01010992843657732 ; diff loss =  7.795169949531555e-07 ; diff weight =  0.0005453184130601585
lambda is : 9.999999999999997e-06, cost : 108.39 min
==========
At iteration 568, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  569 ; minimum lost =  0.03333134204149246 ; diff loss =  8.754432201385498e-07 ; diff weight =  0.0022867554798722267
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00031622776601683783, cost : 110.317 min
==========
*** Collecting results ***
Exporting result Dict
gdT Time elapsed: 110.39287401835124 minutes.
Representative adata: (57515, 27504)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for pDC
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda:Lambda: Lambda: Lambda:1e-05 Lambda: Lambda:Lambda: Lambda:Lambda:1.5e-05 Lambda: Lambda:Lambda:Lambda:Lambda:starting at Lambda:4.6e-05 Lambda: Lambda: 2.2e-05 Lambda: Lambda: Lambda:Lambda:starting at Lambda: Lambda:3.2e-05 Lambda: Lambda:   2024-10-03 17:27:38  starting at  6.8e-05  0.001468 starting at  0.003162  0.0001   2024-10-03 17:27:38  0.000147  starting at  0.000215  0.068129 0.000316 0.000464 Max_iter: 0.000681 2024-10-03 17:27:38 0.001 starting at 0.002154 starting at 2024-10-03 17:27:38 0.004642 starting at 0.006813 starting at 0.01 0.014678 Max_iter: 0.021544 starting at 0.031623 2024-10-03 17:27:38 0.046416 starting at 0.1 starting at starting at starting at 1000starting at Max_iter: starting at 2024-10-03 17:27:38 starting at 2024-10-03 17:27:38 Max_iter: starting at 2024-10-03 17:27:38 starting at 2024-10-03 17:27:38 starting at starting at 1000starting at 2024-10-03 17:27:38 starting at Max_iter: starting at 2024-10-03 17:27:38 starting at 2024-10-03 17:27:38 2024-10-03 17:27:38 2024-10-03 17:27:38 
2024-10-03 17:27:38 1000
2024-10-03 17:27:38 Max_iter: 2024-10-03 17:27:38 Max_iter: 1000
2024-10-03 17:27:38 Max_iter: 2024-10-03 17:27:38 Max_iter: 2024-10-03 17:27:38 2024-10-03 17:27:38 
2024-10-03 17:27:38 Max_iter: 2024-10-03 17:27:3810002024-10-03 17:27:38 Max_iter: 2024-10-03 17:27:38Max_iter: Max_iter: Max_iter: Max_iter: Max_iter:1000Max_iter:1000Max_iter:1000Max_iter:1000Max_iter:Max_iter:Max_iter:1000 
Max_iter:1000 1000100010001000 
 1000
 1000
 1000
 1000 1000 1000
Max_iter:  1000
Max_iter:



1000






1000
 
1000
At iteration 164, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  165 ; minimum lost =  0.12749053537845612 ; diff loss =  1.4901161193847656e-08 ; diff weight =  1.9669532775878906e-05
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.10000000000000002, cost : 33.51 min
==========
At iteration 169, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  169 ; minimum lost =  0.1072821319103241 ; diff loss =  0.0 ; diff weight =  2.199411392211914e-05
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.0681292069057962, cost : 34.348 min
==========
At iteration 177, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  178 ; minimum lost =  0.09271353483200073 ; diff loss =  6.854534149169922e-07 ; diff weight =  0.00043100141920149326
lambda is : 0.04641588833612786, cost : 35.939 min
==========
At iteration 196, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  197 ; minimum lost =  0.08220898360013962 ; diff loss =  6.705522537231445e-07 ; diff weight =  3.710480814334005e-05
lambda is : 0.0316227766016838, cost : 39.432 min
==========
At iteration 231, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  232 ; minimum lost =  0.0736723467707634 ; diff loss =  9.760260581970215e-07 ; diff weight =  0.0011605541221797466
lambda is : 0.02154434690031885, cost : 45.9 min
==========
At iteration 237, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  238 ; minimum lost =  0.04694000631570816 ; diff loss =  9.126961231231689e-07 ; diff weight =  0.0014585376484319568
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.010000000000000004, cost : 47.453 min
==========
At iteration 283, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  284 ; minimum lost =  0.027127504348754883 ; diff loss =  9.51811671257019e-07 ; diff weight =  0.002240378875285387
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.004641588833612781, cost : 55.808 min
==========
At iteration 286, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  287 ; minimum lost =  0.035814739763736725 ; diff loss =  2.3096799850463867e-07 ; diff weight =  0.00036904047010466456
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.006812920690579613, cost : 56.625 min
==========
At iteration 295, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  296 ; minimum lost =  0.01163344644010067 ; diff loss =  9.10833477973938e-07 ; diff weight =  0.002195262350142002
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0014677992676220694, cost : 57.859 min
==========
At iteration 319, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  320 ; minimum lost =  0.020359020680189133 ; diff loss =  5.550682544708252e-07 ; diff weight =  0.0011004769476130605
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.003162277660168382, cost : 62.846 min
==========
At iteration 328, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  329 ; minimum lost =  0.015347111970186234 ; diff loss =  9.480863809585571e-07 ; diff weight =  0.0040573482401669025
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0021544346900318843, cost : 63.935 min
==========
At iteration 323, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  324 ; minimum lost =  0.008690040558576584 ; diff loss =  8.996576070785522e-07 ; diff weight =  0.0062692961655557156
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0010000000000000002, cost : 64.115 min
==========
At iteration 331, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  332 ; minimum lost =  0.059945035725831985 ; diff loss =  3.6135315895080566e-07 ; diff weight =  0.0006573142600245774
lambda is : 0.014677992676220709, cost : 65.003 min
==========
At iteration 334, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  335 ; minimum lost =  0.006526299752295017 ; diff loss =  2.6170164346694946e-07 ; diff weight =  0.009371881373226643
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0006812920690579617, cost : 65.191 min
==========
At iteration 360, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  361 ; minimum lost =  0.0029848648700863123 ; diff loss =  6.889458745718002e-07 ; diff weight =  0.00821672286838293
lambda is : 0.0002154434690031884, cost : 69.917 min
==========
At iteration 359, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  360 ; minimum lost =  0.0049795592203736305 ; diff loss =  8.884817361831665e-07 ; diff weight =  0.00533892260864377
lambda is : 0.00046415888336127795, cost : 70.745 min
==========
At iteration 375, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  376 ; minimum lost =  0.003777103265747428 ; diff loss =  9.806826710700989e-07 ; diff weight =  0.005858050659298897
lambda is : 0.00031622776601683783, cost : 72.888 min
==========
At iteration 389, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  390 ; minimum lost =  0.002271983539685607 ; diff loss =  9.231735020875931e-07 ; diff weight =  0.00974627211689949
lambda is : 0.00014677992676220703, cost : 75.396 min
==========
At iteration 414, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  415 ; minimum lost =  0.0017792530125007033 ; diff loss =  9.416835382580757e-07 ; diff weight =  0.021168285980820656
lambda is : 9.999999999999991e-05, cost : 80.511 min
==========
At iteration 453, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  454 ; minimum lost =  0.001386634772643447 ; diff loss =  9.438954293727875e-07 ; diff weight =  0.012214000336825848
lambda is : 6.81292069057961e-05, cost : 87.931 min
==========
At iteration 485, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  486 ; minimum lost =  0.001086636446416378 ; diff loss =  9.492505341768265e-07 ; diff weight =  0.0282635148614645
lambda is : 4.6415888336127784e-05, cost : 92.76 min
==========
At iteration 498, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  499 ; minimum lost =  0.0008608443313278258 ; diff loss =  9.802170097827911e-07 ; diff weight =  0.017047259956598282
lambda is : 3.16227766016838e-05, cost : 94.504 min
==========
At iteration 519, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  520 ; minimum lost =  0.000686400628183037 ; diff loss =  9.821378625929356e-07 ; diff weight =  0.01616436429321766
lambda is : 2.1544346900318854e-05, cost : 98.314 min
==========
At iteration 514, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  515 ; minimum lost =  0.0005677064182236791 ; diff loss =  9.798677638173103e-07 ; diff weight =  0.016154274344444275
lambda is : 1.4677992676220687e-05, cost : 98.681 min
==========
At iteration 543, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  544 ; minimum lost =  0.0004297097912058234 ; diff loss =  8.485803846269846e-07 ; diff weight =  0.02371187135577202
lambda is : 9.999999999999997e-06, cost : 102.549 min
==========
*** Collecting results ***
Exporting result Dict
pDC Time elapsed: 102.64222592512766 minutes.
***** Finished lambda tuning
====================
