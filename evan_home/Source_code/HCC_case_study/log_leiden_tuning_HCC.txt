nohup: ignoring input
Shape: (71915, 25712) <class 'scipy.sparse._csc.csc_matrix'>
all leiden clusters: ['Leiden_0', 'Leiden_1', 'Leiden_10', 'Leiden_11', 'Leiden_12', 'Leiden_13', 'Leiden_14', 'Leiden_15', 'Leiden_16', 'Leiden_17', 'Leiden_18', 'Leiden_19', 'Leiden_2', 'Leiden_20', 'Leiden_21', 'Leiden_22', 'Leiden_23', 'Leiden_3', 'Leiden_4', 'Leiden_5', 'Leiden_6', 'Leiden_7', 'Leiden_8', 'Leiden_9']
====================
***** Starting tuning
Shape: (71915, 25712)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for Leiden_0
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda:Lambda: Lambda: Lambda:Lambda: Lambda:Lambda:1.5e-05 Lambda: Lambda:Lambda:Lambda:1e-05 Lambda: Lambda:Lambda:Lambda:3.2e-05 Lambda: Lambda: Lambda:Lambda:Lambda:starting at Lambda: Lambda:2.2e-05 Lambda: Lambda:Lambda:  starting at  0.000215    starting at  4.6e-05  6.8e-05    2024-11-22 15:09:14  0.0001  starting at  0.021544   0.000147 0.1 2024-11-22 15:09:14 0.000316 starting at 0.000464 0.000681 0.0012024-11-22 15:09:14 0.001468 starting at 0.002154 starting at 0.003162 0.004642 0.006813 Max_iter: 0.01 starting at 0.014678 2024-11-22 15:09:14 0.031623 starting at 0.046416 0.068129 starting at starting at Max_iter: starting at 2024-11-22 15:09:14 starting at starting at  starting atMax_iter: starting at 2024-11-22 15:09:14 starting at 2024-11-22 15:09:14 starting at starting at starting at 1000
starting at 2024-11-22 15:09:14 starting at Max_iter: starting at 2024-11-22 15:09:14 starting at starting at 2024-11-22 15:09:14 2024-11-22 15:09:14 10002024-11-22 15:09:14 Max_iter: 2024-11-22 15:09:14 2024-11-22 15:09:14  2024-11-22 15:09:1410002024-11-22 15:09:14 Max_iter: 2024-11-22 15:09:14 Max_iter: 2024-11-22 15:09:14 2024-11-22 15:09:14 2024-11-22 15:09:14 2024-11-22 15:09:14 Max_iter: 2024-11-22 15:09:14 10002024-11-22 15:09:14 Max_iter:2024-11-22 15:09:14 2024-11-22 15:09:14Max_iter: Max_iter:
Max_iter:1000Max_iter: Max_iter:  Max_iter:
Max_iter: 1000Max_iter:1000Max_iter: Max_iter:Max_iter: Max_iter:1000Max_iter:
Max_iter: Max_iter: 1000  
10001000 1000
 
1000
 10001000
 1000
 1000 1000
 1000Max_iter:
10001000

1000
1000


1000
 




1000
At iteration 240, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  241 ; minimum lost =  0.37302911281585693 ; diff loss =  5.364418029785156e-07 ; diff weight =  8.93276373972185e-05
lambda is : 0.10000000000000002, cost : 60.796 min
==========
At iteration 313, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  314 ; minimum lost =  0.33438146114349365 ; diff loss =  6.556510925292969e-07 ; diff weight =  0.00010483331425348297
lambda is : 0.0681292069057962, cost : 78.171 min
==========
At iteration 391, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  392 ; minimum lost =  0.27323177456855774 ; diff loss =  2.086162567138672e-07 ; diff weight =  0.0002362533996347338
lambda is : 0.0316227766016838, cost : 97.008 min
==========
At iteration 400, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  401 ; minimum lost =  0.24658872187137604 ; diff loss =  9.08970832824707e-07 ; diff weight =  0.0005300629418343306
lambda is : 0.02154434690031885, cost : 99.706 min
==========
At iteration 431, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  432 ; minimum lost =  0.30224764347076416 ; diff loss =  7.152557373046875e-07 ; diff weight =  0.0006304612616077065
lambda is : 0.04641588833612786, cost : 105.669 min
==========
At iteration 434, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  435 ; minimum lost =  0.22321811318397522 ; diff loss =  9.98377799987793e-07 ; diff weight =  0.0004768499347846955
lambda is : 0.014677992676220709, cost : 107.036 min
==========
At iteration 478, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  479 ; minimum lost =  0.10142259299755096 ; diff loss =  9.98377799987793e-07 ; diff weight =  0.00011147574696224183
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 117.485 min
==========
At iteration 520, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  521 ; minimum lost =  0.1859445869922638 ; diff loss =  9.98377799987793e-07 ; diff weight =  0.0017923663835972548
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.006812920690579613, cost : 124.864 min
==========
At iteration 524, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  525 ; minimum lost =  0.20344282686710358 ; diff loss =  5.811452865600586e-07 ; diff weight =  0.0004234429507050663
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.010000000000000004, cost : 127.918 min
==========
At iteration 523, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  524 ; minimum lost =  0.17037352919578552 ; diff loss =  9.685754776000977e-07 ; diff weight =  0.0013074924936518073
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.004641588833612781, cost : 128.148 min
==========
At iteration 519, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  520 ; minimum lost =  0.08425027132034302 ; diff loss =  9.834766387939453e-07 ; diff weight =  0.00010987362475134432
At iteration 529, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  529 ; minimum lost =  0.10499469935894012 ; diff loss =  -4.3958425521850586e-07 ; diff weight =  0.004967646673321724
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00031622776601683783, cost : 128.882 min
==========
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 6.81292069057961e-05, cost : 129.139 min
==========
At iteration 529, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  530 ; minimum lost =  0.11213221400976181 ; diff loss =  9.5367431640625e-07 ; diff weight =  0.0010891553247347474
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00046415888336127795, cost : 129.821 min
==========
At iteration 532, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  533 ; minimum lost =  0.07559210807085037 ; diff loss =  7.748603820800781e-07 ; diff weight =  0.016635196283459663
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 3.16227766016838e-05, cost : 131.469 min
==========
At iteration 547, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  547 ; minimum lost =  0.08857402205467224 ; diff loss =  -3.725290298461914e-07 ; diff weight =  0.00864356104284525
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 9.999999999999991e-05, cost : 134.226 min
==========
At iteration 543, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  544 ; minimum lost =  0.09334263205528259 ; diff loss =  9.164214134216309e-07 ; diff weight =  0.00013009340909775347
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00014677992676220703, cost : 134.947 min
==========
At iteration 571, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  572 ; minimum lost =  0.1568460762500763 ; diff loss =  8.791685104370117e-07 ; diff weight =  0.0010993044124916196
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.003162277660168382, cost : 138.233 min
==========
At iteration 581, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  582 ; minimum lost =  0.13529570400714874 ; diff loss =  9.98377799987793e-07 ; diff weight =  0.001462881569750607
At iteration 568, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  569 ; minimum lost =  0.11886174976825714 ; diff loss =  4.470348358154297e-07 ; diff weight =  0.00019822095055133104
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0014677992676220694, cost : 139.118 min
==========
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0006812920690579617, cost : 139.277 min
==========
At iteration 576, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  577 ; minimum lost =  0.14534062147140503 ; diff loss =  8.195638656616211e-07 ; diff weight =  0.000446281919721514
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0021544346900318843, cost : 139.919 min
==========
At iteration 594, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  595 ; minimum lost =  0.12660157680511475 ; diff loss =  5.513429641723633e-07 ; diff weight =  0.0009770876495167613
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0010000000000000002, cost : 143.406 min
==========
At iteration 606, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  607 ; minimum lost =  0.07678154855966568 ; diff loss =  8.121132850646973e-07 ; diff weight =  0.0015657634939998388
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 4.6415888336127784e-05, cost : 145.179 min
==========
At iteration 618, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  619 ; minimum lost =  0.060662198811769485 ; diff loss =  9.648501873016357e-07 ; diff weight =  0.0004522851959336549
At iteration 615, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  616 ; minimum lost =  0.06514547765254974 ; diff loss =  8.195638656616211e-07 ; diff weight =  0.00031966259120963514
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 9.999999999999997e-06, cost : 147.834 min
==========
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 1.4677992676220687e-05, cost : 147.909 min
==========
At iteration 665, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  666 ; minimum lost =  0.06604431569576263 ; diff loss =  8.270144462585449e-07 ; diff weight =  0.0002069735201075673
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 2.1544346900318854e-05, cost : 158.767 min
==========
*** Collecting results ***
Exporting result Dict
Leiden_0 Time elapsed: 158.8445312142372 minutes.
Shape: (71915, 25712)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for Leiden_1
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda:Lambda:Lambda: Lambda:Lambda:Lambda:  Lambda:1e-05 Lambda: Lambda: Lambda:Lambda: Lambda:Lambda:1.5e-05 Lambda:2.2e-05 Lambda: Lambda:Lambda:starting at Lambda:Lambda: Lambda:3.2e-05  Lambda:Lambda:4.6e-05  Lambda: Lambda:0.000215 Lambda:  starting at  starting at  6.8e-05   2024-11-22 17:48:13   0.0001  starting at 0.000147  0.01 starting at 0.000316  0.031623  starting at  0.068129 0.000464 2024-11-22 17:48:13 0.000681 2024-11-22 17:48:13 0.001 starting at 0.001468 0.002154 Max_iter: 0.003162 0.004642 starting at 0.0068132024-11-22 17:48:13 starting at  starting at0.014678 2024-11-22 17:48:13 starting at 0.021544 starting at 0.1 2024-11-22 17:48:13 0.046416starting at starting at Max_iter: starting at Max_iter: starting at 2024-11-22 17:48:13 starting at starting at 1000starting at starting at 2024-11-22 17:48:13  starting atMax_iter: 2024-11-22 17:48:13  2024-11-22 17:48:13starting at Max_iter: 2024-11-22 17:48:13 starting at 2024-11-22 17:48:13 starting at Max_iter:  starting at2024-11-22 17:48:13 2024-11-22 17:48:13 10002024-11-22 17:48:13 10002024-11-22 17:48:13 Max_iter: 2024-11-22 17:48:13 2024-11-22 17:48:13 
2024-11-22 17:48:13 2024-11-22 17:48:13 Max_iter:  2024-11-22 17:48:131000Max_iter:  Max_iter:2024-11-22 17:48:13 1000
Max_iter: 2024-11-22 17:48:13 Max_iter: 2024-11-22 17:48:13 1000 2024-11-22 17:48:13Max_iter: Max_iter: 
Max_iter:
Max_iter: 1000Max_iter: Max_iter: Max_iter:Max_iter:1000 
1000 Max_iter:1000Max_iter:1000Max_iter:
 10001000 1000
10001000
 1000 
Max_iter:
1000 
 
 Max_iter:

1000


1000
 1000
1000
10001000 



1000
At iteration 215, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  216 ; minimum lost =  0.3953481614589691 ; diff loss =  9.238719940185547e-07 ; diff weight =  0.00020596821559593081
lambda is : 0.10000000000000002, cost : 52.854 min
==========
At iteration 365, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  366 ; minimum lost =  0.36054763197898865 ; diff loss =  9.834766387939453e-07 ; diff weight =  0.00026744327624328434
lambda is : 0.0681292069057962, cost : 88.424 min
==========
At iteration 406, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  407 ; minimum lost =  0.2930485010147095 ; diff loss =  9.834766387939453e-07 ; diff weight =  0.00036325983819551766
lambda is : 0.0316227766016838, cost : 98.352 min
==========
At iteration 417, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  418 ; minimum lost =  0.3253861665725708 ; diff loss =  8.344650268554688e-07 ; diff weight =  0.00028593812021426857
lambda is : 0.04641588833612786, cost : 101.089 min
==========
At iteration 431, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  432 ; minimum lost =  0.240151509642601 ; diff loss =  8.940696716308594e-07 ; diff weight =  0.00047246506437659264
lambda is : 0.014677992676220709, cost : 105.278 min
==========
At iteration 435, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  436 ; minimum lost =  0.2178187519311905 ; diff loss =  9.387731552124023e-07 ; diff weight =  0.0009915942791849375
lambda is : 0.010000000000000004, cost : 106.713 min
==========
At iteration 439, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  440 ; minimum lost =  0.2646488845348358 ; diff loss =  5.066394805908203e-07 ; diff weight =  0.00028449183446355164
lambda is : 0.02154434690031885, cost : 106.813 min
==========
At iteration 468, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  469 ; minimum lost =  0.1974351704120636 ; diff loss =  9.834766387939453e-07 ; diff weight =  0.0008127460605464876
lambda is : 0.006812920690579613, cost : 113.994 min
==========
At iteration 469, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  470 ; minimum lost =  0.10862810164690018 ; diff loss =  7.748603820800781e-07 ; diff weight =  0.00149213382974267
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00046415888336127795, cost : 114.849 min
==========
At iteration 479, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  480 ; minimum lost =  0.12522882223129272 ; diff loss =  4.470348358154297e-07 ; diff weight =  0.001044194563291967
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0010000000000000002, cost : 115.654 min
==========
At iteration 477, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  478 ; minimum lost =  0.10192611813545227 ; diff loss =  7.525086402893066e-07 ; diff weight =  0.00036253881989978254
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00031622776601683783, cost : 116.157 min
==========
At iteration 476, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  477 ; minimum lost =  0.13565264642238617 ; diff loss =  3.129243850708008e-07 ; diff weight =  0.00014463889237958938
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0014677992676220694, cost : 116.437 min
==========
At iteration 487, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  488 ; minimum lost =  0.17867209017276764 ; diff loss =  9.98377799987793e-07 ; diff weight =  0.0010008906247094274
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.004641588833612781, cost : 118.121 min
==========
At iteration 493, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  494 ; minimum lost =  0.07601724565029144 ; diff loss =  9.685754776000977e-07 ; diff weight =  0.0001858360192272812
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 3.16227766016838e-05, cost : 119.53 min
==========
At iteration 504, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  505 ; minimum lost =  0.14761996269226074 ; diff loss =  7.748603820800781e-07 ; diff weight =  0.0009199198102578521
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0021544346900318843, cost : 120.717 min
==========
At iteration 503, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  504 ; minimum lost =  0.11618614196777344 ; diff loss =  9.387731552124023e-07 ; diff weight =  0.0013576858909800649
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0006812920690579617, cost : 121.218 min
==========
At iteration 516, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  517 ; minimum lost =  0.08653882145881653 ; diff loss =  8.493661880493164e-07 ; diff weight =  0.00045038331882096827
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 9.999999999999991e-05, cost : 123.757 min
==========
At iteration 524, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  525 ; minimum lost =  0.0957416445016861 ; diff loss =  9.08970832824707e-07 ; diff weight =  0.0021066777408123016
At iteration 522, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  523 ; minimum lost =  0.16190612316131592 ; diff loss =  9.98377799987793e-07 ; diff weight =  0.0008664437918923795
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 124.477 min
==========
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.003162277660168382, cost : 124.499 min
==========
At iteration 522, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  523 ; minimum lost =  0.08256329596042633 ; diff loss =  9.313225746154785e-07 ; diff weight =  0.0035483213141560555
At iteration 522, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  523 ; minimum lost =  0.09080879390239716 ; diff loss =  3.725290298461914e-07 ; diff weight =  0.00015110061212908477
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 6.81292069057961e-05, cost : 125.24 min
==========
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00014677992676220703, cost : 125.409 min
==========
At iteration 523, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  524 ; minimum lost =  0.06822696328163147 ; diff loss =  9.387731552124023e-07 ; diff weight =  0.00025590587756596506
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 1.4677992676220687e-05, cost : 126.748 min
==========
At iteration 541, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  542 ; minimum lost =  0.07893890887498856 ; diff loss =  9.313225746154785e-07 ; diff weight =  0.00021674616436939687
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 4.6415888336127784e-05, cost : 129.03 min
==========
At iteration 600, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  601 ; minimum lost =  0.06361478567123413 ; diff loss =  9.5367431640625e-07 ; diff weight =  0.00025713458308018744
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 9.999999999999997e-06, cost : 141.496 min
==========
At iteration 626, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  627 ; minimum lost =  0.07053263485431671 ; diff loss =  5.960464477539062e-07 ; diff weight =  9.684528049547225e-05
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 2.1544346900318854e-05, cost : 146.922 min
==========
*** Collecting results ***
Exporting result Dict
Leiden_1 Time elapsed: 147.00486875772475 minutes.
Shape: (71915, 25712)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for Leiden_10
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda: Lambda:Lambda:Lambda:Lambda:1e-05  Lambda: Lambda: Lambda:Lambda:Lambda:  Lambda:Lambda:Lambda:starting at Lambda: Lambda: 1.5e-05  Lambda:Lambda: 4.6e-05  Lambda:2.2e-05  Lambda:  Lambda:Lambda:Lambda: 0.000215 Lambda:3.2e-05 Lambda:Lambda:   2024-11-22 20:15:19 0.000681 0.001 starting at 6.8e-05  0.002154 starting at 0.0001  starting at 0.004642 0.01 0.000147   0.021544 starting at  starting at   0.046416 0.000316 0.000464 Max_iter: starting at starting at 2024-11-22 20:15:19 starting at 0.001468 starting at 2024-11-22 20:15:19 starting at 0.003162 2024-11-22 20:15:19 starting at starting at starting at 0.014678 0.006813 starting at 2024-11-22 20:15:19 0.031623 2024-11-22 20:15:19 0.068129 0.1 starting at starting at starting at 10002024-11-22 20:15:19 2024-11-22 20:15:19 Max_iter: 2024-11-22 20:15:19 starting at 2024-11-22 20:15:19 Max_iter: 2024-11-22 20:15:19 starting at Max_iter: 2024-11-22 20:15:19 2024-11-22 20:15:19 2024-11-22 20:15:19 starting at starting at 2024-11-22 20:15:19 Max_iter: starting at Max_iter: starting at starting at 2024-11-22 20:15:19 2024-11-22 20:15:19 2024-11-22 20:15:19 
Max_iter: Max_iter: 1000Max_iter: 2024-11-22 20:15:19 Max_iter: 1000Max_iter: 2024-11-22 20:15:19 1000Max_iter: Max_iter: Max_iter: 2024-11-22 20:15:19 2024-11-22 20:15:19 Max_iter: 10002024-11-22 20:15:19 10002024-11-22 20:15:19 2024-11-22 20:15:19 Max_iter: Max_iter: Max_iter: 10001000
1000Max_iter: 1000
1000Max_iter: 
100010001000Max_iter: Max_iter:1000
Max_iter:
Max_iter:Max_iter:100010001000


1000

1000



1000 1000
 1000 1000 1000








At iteration 208, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  209 ; minimum lost =  0.1593015491962433 ; diff loss =  4.470348358154297e-07 ; diff weight =  0.0006064680055715144
lambda is : 0.0681292069057962, cost : 52.075 min
==========
At iteration 251, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  252 ; minimum lost =  0.19691604375839233 ; diff loss =  8.940696716308594e-08 ; diff weight =  9.357886301586404e-05
lambda is : 0.10000000000000002, cost : 61.181 min
==========
At iteration 254, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  255 ; minimum lost =  0.06337956339120865 ; diff loss =  5.587935447692871e-07 ; diff weight =  0.0009883076418191195
lambda is : 0.014677992676220709, cost : 62.139 min
==========
At iteration 259, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  260 ; minimum lost =  0.12691403925418854 ; diff loss =  9.98377799987793e-07 ; diff weight =  0.0009077273425646126
lambda is : 0.04641588833612786, cost : 63.212 min
==========
At iteration 258, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  259 ; minimum lost =  0.10094919800758362 ; diff loss =  9.238719940185547e-07 ; diff weight =  0.0005975706153549254
lambda is : 0.0316227766016838, cost : 63.276 min
==========
At iteration 304, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  305 ; minimum lost =  0.08014146983623505 ; diff loss =  6.332993507385254e-07 ; diff weight =  0.0006388697074726224
lambda is : 0.02154434690031885, cost : 74.616 min
==========
At iteration 311, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  312 ; minimum lost =  0.03992488235235214 ; diff loss =  9.462237358093262e-07 ; diff weight =  0.0006697491044178605
lambda is : 0.006812920690579613, cost : 76.196 min
==========
At iteration 334, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  335 ; minimum lost =  0.05003172159194946 ; diff loss =  2.0116567611694336e-07 ; diff weight =  0.00042418352677486837
lambda is : 0.010000000000000004, cost : 81.48 min
==========
At iteration 346, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  347 ; minimum lost =  0.03190590441226959 ; diff loss =  8.121132850646973e-07 ; diff weight =  0.0012760077370330691
lambda is : 0.004641588833612781, cost : 83.099 min
==========
At iteration 417, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  418 ; minimum lost =  0.021051904186606407 ; diff loss =  6.686896085739136e-07 ; diff weight =  0.0023015961050987244
At iteration 416, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  417 ; minimum lost =  0.01750953495502472 ; diff loss =  9.424984455108643e-07 ; diff weight =  0.002794527681544423
lambda is : 0.0021544346900318843, cost : 101.771 min
==========
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0014677992676220694, cost : 101.776 min
==========
At iteration 426, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  427 ; minimum lost =  0.025744937360286713 ; diff loss =  9.94652509689331e-07 ; diff weight =  0.0023269671946763992
lambda is : 0.003162277660168382, cost : 101.817 min
==========
At iteration 435, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  436 ; minimum lost =  0.014816896989941597 ; diff loss =  9.96515154838562e-07 ; diff weight =  0.005338574294000864
lambda is : 0.0010000000000000002, cost : 103.479 min
==========
At iteration 469, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  470 ; minimum lost =  0.012779701501131058 ; diff loss =  9.685754776000977e-07 ; diff weight =  0.004527118522673845
lambda is : 0.0006812920690579617, cost : 109.045 min
==========
At iteration 486, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  487 ; minimum lost =  0.009817676618695259 ; diff loss =  9.043142199516296e-07 ; diff weight =  0.004023776855319738
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00031622776601683783, cost : 113.303 min
==========
At iteration 485, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  486 ; minimum lost =  0.011147252283990383 ; diff loss =  9.51811671257019e-07 ; diff weight =  0.0043412186205387115
lambda is : 0.00046415888336127795, cost : 114.161 min
==========
At iteration 515, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  516 ; minimum lost =  0.0077240196987986565 ; diff loss =  9.695068001747131e-07 ; diff weight =  0.003916759043931961
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00014677992676220703, cost : 119.233 min
==========
At iteration 513, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  514 ; minimum lost =  0.008686370216310024 ; diff loss =  8.707866072654724e-07 ; diff weight =  0.0038921604864299297
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 122.319 min
==========
At iteration 550, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  551 ; minimum lost =  0.006805292330682278 ; diff loss =  9.294599294662476e-07 ; diff weight =  0.004104335326701403
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 9.999999999999991e-05, cost : 127.52 min
==========
At iteration 547, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  548 ; minimum lost =  0.004391145892441273 ; diff loss =  6.94766640663147e-07 ; diff weight =  0.00807323306798935
lambda is : 3.16227766016838e-05, cost : 128.3 min
==========
At iteration 561, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  562 ; minimum lost =  0.005938267335295677 ; diff loss =  9.899958968162537e-07 ; diff weight =  0.007604246959090233
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 6.81292069057961e-05, cost : 130.944 min
==========
At iteration 564, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  565 ; minimum lost =  0.0051033031195402145 ; diff loss =  9.979121387004852e-07 ; diff weight =  0.00343324919231236
lambda is : 4.6415888336127784e-05, cost : 131.376 min
==========
At iteration 561, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  562 ; minimum lost =  0.0025675026699900627 ; diff loss =  8.433125913143158e-07 ; diff weight =  0.014305259101092815
lambda is : 9.999999999999997e-06, cost : 132.416 min
==========
At iteration 584, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  585 ; minimum lost =  0.0035913537722080946 ; diff loss =  8.891802281141281e-07 ; diff weight =  0.009058043360710144
lambda is : 2.1544346900318854e-05, cost : 135.457 min
==========
At iteration 591, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  592 ; minimum lost =  0.0029991664923727512 ; diff loss =  5.881302058696747e-07 ; diff weight =  0.01503862626850605
lambda is : 1.4677992676220687e-05, cost : 136.393 min
==========
*** Collecting results ***
Exporting result Dict
/home/jovyan/work/GitHub/EvanPys/Progress/ADlasso2/AD2_w_utils_lossdiff_noZ.py:929: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.
  fig, ax1 = plt.subplots(figsize = (fig_width,fig_height))
Leiden_10 Time elapsed: 136.46761178970337 minutes.
Shape: (71915, 25712)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for Leiden_11
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda:Lambda:Lambda: Lambda:Lambda: Lambda:Lambda:  Lambda:Lambda:Lambda:1.5e-05   Lambda:2.2e-05Lambda:Lambda: Lambda: Lambda:Lambda:4.6e-05 Lambda:Lambda:1e-05  Lambda:  Lambda:Lambda:Lambda:starting at Lambda:3.2e-05 0.000147 Lambda: Lambda: starting at 0.000316  0.000215   starting at   starting at 6.8e-05  0.0001 0.004642    2024-11-22 22:31:54  starting at starting at  0.031623   2024-11-22 22:31:540.1 starting at 0.000464 starting at 0.000681 0.001 2024-11-22 22:31:54 0.001468 0.002154 2024-11-22 22:31:54 starting at 0.003162 starting at starting at 0.006813 0.01 0.014678 Max_iter: 0.021544 2024-11-22 22:31:54 2024-11-22 22:31:54 0.046416 starting at 0.068129  Max_iter:starting at 2024-11-22 22:31:54 starting at 2024-11-22 22:31:54 starting at starting at Max_iter: starting at starting atMax_iter: 2024-11-22 22:31:54 starting at 2024-11-22 22:31:54 2024-11-22 22:31:54 starting at starting at starting at 1000starting at Max_iter: Max_iter: starting at 2024-11-22 22:31:54 starting at  10002024-11-22 22:31:54 Max_iter: 2024-11-22 22:31:54 Max_iter: 2024-11-22 22:31:54 2024-11-22 22:31:54 1000
2024-11-22 22:31:54  2024-11-22 22:31:541000Max_iter: 2024-11-22 22:31:54 Max_iter: Max_iter: 2024-11-22 22:31:54 2024-11-22 22:31:54 2024-11-22 22:31:54 
2024-11-22 22:31:54 100010002024-11-22 22:31:54 Max_iter: 2024-11-22 22:31:54 
Max_iter: 1000Max_iter: 1000Max_iter: Max_iter: Max_iter:  Max_iter:
1000Max_iter:10001000Max_iter:Max_iter:Max_iter:Max_iter:

Max_iter:1000Max_iter:1000
1000
1000
10001000 1000
 

     
 




100010001000
1000
1000
10001000



At iteration 209, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  210 ; minimum lost =  0.2110215425491333 ; diff loss =  2.980232238769531e-07 ; diff weight =  0.00020735178259201348
lambda is : 0.10000000000000002, cost : 50.902 min
==========
At iteration 261, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  262 ; minimum lost =  0.1854102909564972 ; diff loss =  9.5367431640625e-07 ; diff weight =  0.0006038506398908794
lambda is : 0.0681292069057962, cost : 62.614 min
==========
At iteration 266, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  267 ; minimum lost =  0.13178089261054993 ; diff loss =  9.08970832824707e-07 ; diff weight =  0.0007284347666427493
lambda is : 0.0316227766016838, cost : 65.05 min
==========
At iteration 265, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  266 ; minimum lost =  0.1586666852235794 ; diff loss =  1.4901161193847656e-08 ; diff weight =  7.415630534524098e-05
lambda is : 0.04641588833612786, cost : 65.206 min
==========
At iteration 319, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  320 ; minimum lost =  0.10721317678689957 ; diff loss =  5.289912223815918e-07 ; diff weight =  0.00040462100878357887
lambda is : 0.02154434690031885, cost : 76.584 min
==========
At iteration 323, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  324 ; minimum lost =  0.06828022748231888 ; diff loss =  7.152557373046875e-07 ; diff weight =  0.0010982470121234655
lambda is : 0.010000000000000004, cost : 78.241 min
==========
At iteration 332, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  333 ; minimum lost =  0.042922087013721466 ; diff loss =  6.034970283508301e-07 ; diff weight =  0.0013057364849373698
lambda is : 0.004641588833612781, cost : 80.396 min
==========
At iteration 365, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  366 ; minimum lost =  0.0857580155134201 ; diff loss =  3.501772880554199e-07 ; diff weight =  0.00045437272638082504
lambda is : 0.014677992676220709, cost : 87.998 min
==========
At iteration 393, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  394 ; minimum lost =  0.03405424579977989 ; diff loss =  9.685754776000977e-07 ; diff weight =  0.0029896977357566357
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.003162277660168382, cost : 93.398 min
==========
At iteration 400, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  401 ; minimum lost =  0.027258675545454025 ; diff loss =  9.238719940185547e-07 ; diff weight =  0.001964340452104807
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0021544346900318843, cost : 96.099 min
==========
At iteration 415, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  416 ; minimum lost =  0.054074615240097046 ; diff loss =  8.717179298400879e-07 ; diff weight =  0.001470048213377595
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.006812920690579613, cost : 98.578 min
==========
At iteration 415, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  416 ; minimum lost =  0.022019043564796448 ; diff loss =  9.834766387939453e-07 ; diff weight =  0.0022924281656742096
lambda is : 0.0014677992676220694, cost : 99.591 min
==========
At iteration 419, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  420 ; minimum lost =  0.01796879991889 ; diff loss =  9.872019290924072e-07 ; diff weight =  0.0024942283052951097
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0010000000000000002, cost : 100.079 min
==========
At iteration 422, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  423 ; minimum lost =  0.0148305743932724 ; diff loss =  9.424984455108643e-07 ; diff weight =  0.002131716813892126
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0006812920690579617, cost : 101.265 min
==========
At iteration 473, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  474 ; minimum lost =  0.010349515825510025 ; diff loss =  9.51811671257019e-07 ; diff weight =  0.0038087968714535236
lambda is : 0.00031622776601683783, cost : 111.124 min
==========
At iteration 474, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  475 ; minimum lost =  0.012331369332969189 ; diff loss =  9.34116542339325e-07 ; diff weight =  0.0032341957557946444
lambda is : 0.00046415888336127795, cost : 113.88 min
==========
At iteration 489, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  490 ; minimum lost =  0.00871848315000534 ; diff loss =  9.909272193908691e-07 ; diff weight =  0.004206873942166567
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 114.625 min
==========
At iteration 523, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  524 ; minimum lost =  0.007360500283539295 ; diff loss =  9.98377799987793e-07 ; diff weight =  0.004958749748766422
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00014677992676220703, cost : 123.29 min
==========
At iteration 543, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  544 ; minimum lost =  0.006168232765048742 ; diff loss =  9.98377799987793e-07 ; diff weight =  0.005572664551436901
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 9.999999999999991e-05, cost : 127.943 min
==========
At iteration 567, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  568 ; minimum lost =  0.005123003851622343 ; diff loss =  8.540228009223938e-07 ; diff weight =  0.006445357110351324
lambda is : 6.81292069057961e-05, cost : 129.653 min
==========
At iteration 585, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  586 ; minimum lost =  0.003526222426444292 ; diff loss =  7.180497050285339e-07 ; diff weight =  0.002618826925754547
lambda is : 3.16227766016838e-05, cost : 136.28 min
==========
At iteration 611, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  612 ; minimum lost =  0.0023182861041277647 ; diff loss =  6.253831088542938e-07 ; diff weight =  0.003497517202049494
At iteration 604, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  605 ; minimum lost =  0.0018896544352173805 ; diff loss =  4.3713953346014023e-07 ; diff weight =  0.004201479256153107
lambda is : 1.4677992676220687e-05, cost : 139.009 min
==========
lambda is : 9.999999999999997e-06, cost : 139.11 min
==========
At iteration 606, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  607 ; minimum lost =  0.002855184953659773 ; diff loss =  5.485489964485168e-07 ; diff weight =  0.004144890233874321
lambda is : 2.1544346900318854e-05, cost : 140.513 min
==========
At iteration 610, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  611 ; minimum lost =  0.004232416860759258 ; diff loss =  9.015202522277832e-07 ; diff weight =  0.003737780498340726
lambda is : 4.6415888336127784e-05, cost : 141.007 min
==========
*** Collecting results ***
Exporting result Dict
Leiden_11 Time elapsed: 141.07707260847093 minutes.
Shape: (71915, 25712)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for Leiden_12
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda:Lambda:Lambda:  Lambda:Lambda: Lambda:Lambda: Lambda: Lambda:1e-05 1.5e-05 Lambda: Lambda:Lambda:Lambda:3.2e-05  Lambda: 2.2e-05 Lambda:Lambda:6.8e-05 Lambda:Lambda: Lambda:Lambda:starting at Lambda:Lambda:starting at Lambda:Lambda:Lambda: 0.000147    starting at 4.6e-05  0.001 starting at   starting at   0.0001   2024-11-23 00:53:05   2024-11-23 00:53:05    0.000215 starting at 0.000316 0.000464 0.000681 2024-11-23 00:53:05 starting at 0.001468 starting at 2024-11-23 00:53:05 0.002154 0.003162 2024-11-23 00:53:05 0.004642 0.006813 starting at 0.01 0.014678 Max_iter: 0.021544 0.031623 Max_iter: 0.046416 0.068129 0.1 starting at 2024-11-23 00:53:05 starting at starting at starting at Max_iter: 2024-11-23 00:53:05 starting at 2024-11-23 00:53:05 Max_iter: starting at starting at Max_iter: starting at starting at 2024-11-23 00:53:05 starting at starting at 1000starting at starting at 1000starting at starting at starting at 2024-11-23 00:53:05 Max_iter: 2024-11-23 00:53:05 2024-11-23 00:53:05 2024-11-23 00:53:05 1000Max_iter: 2024-11-23 00:53:05 Max_iter: 10002024-11-23 00:53:05 2024-11-23 00:53:05 10002024-11-23 00:53:05 2024-11-23 00:53:05 Max_iter: 2024-11-23 00:53:05 2024-11-23 00:53:05
2024-11-23 00:53:05 2024-11-23 00:53:05 
2024-11-23 00:53:05 2024-11-23 00:53:05 2024-11-23 00:53:05 Max_iter: 1000Max_iter: Max_iter: Max_iter: 
1000Max_iter: 1000
Max_iter: Max_iter: 
Max_iter:Max_iter: 1000Max_iter: Max_iter:Max_iter:Max_iter:Max_iter:Max_iter:1000
100010001000
1000

10001000 10001000

 1000Max_iter:  1000 1000  1000 1000







1000

1000



At iteration 174, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  175 ; minimum lost =  0.19513805210590363 ; diff loss =  9.834766387939453e-07 ; diff weight =  0.00033346613054163754
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.10000000000000002, cost : 42.772 min
==========
At iteration 187, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  188 ; minimum lost =  0.17031361162662506 ; diff loss =  9.98377799987793e-07 ; diff weight =  0.00037243677070364356
lambda is : 0.0681292069057962, cost : 44.798 min
==========
At iteration 193, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  194 ; minimum lost =  0.13800504803657532 ; diff loss =  3.8743019104003906e-07 ; diff weight =  0.0003992051933892071
lambda is : 0.0316227766016838, cost : 46.239 min
==========
At iteration 226, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  227 ; minimum lost =  0.1240205466747284 ; diff loss =  8.866190910339355e-07 ; diff weight =  0.0009345635189674795
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.02154434690031885, cost : 54.245 min
==========
At iteration 225, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  226 ; minimum lost =  0.1519777476787567 ; diff loss =  4.76837158203125e-07 ; diff weight =  0.0003695889899972826
lambda is : 0.04641588833612786, cost : 54.474 min
==========
At iteration 313, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  314 ; minimum lost =  0.09117826074361801 ; diff loss =  9.164214134216309e-07 ; diff weight =  0.001341496710665524
lambda is : 0.010000000000000004, cost : 75.372 min
==========
At iteration 320, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  321 ; minimum lost =  0.07708665728569031 ; diff loss =  8.419156074523926e-07 ; diff weight =  0.0012516897404566407
lambda is : 0.006812920690579613, cost : 76.691 min
==========
At iteration 328, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  329 ; minimum lost =  0.10727294534444809 ; diff loss =  4.0978193283081055e-07 ; diff weight =  0.00044718306162394583
lambda is : 0.014677992676220709, cost : 78.084 min
==========
At iteration 343, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  344 ; minimum lost =  0.06547684967517853 ; diff loss =  9.98377799987793e-07 ; diff weight =  0.0014077251544222236
lambda is : 0.004641588833612781, cost : 81.288 min
==========
At iteration 352, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  353 ; minimum lost =  0.05595777928829193 ; diff loss =  8.158385753631592e-07 ; diff weight =  0.0012342171976342797
lambda is : 0.003162277660168382, cost : 85.28 min
==========
At iteration 366, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  367 ; minimum lost =  0.048252686858177185 ; diff loss =  8.754432201385498e-07 ; diff weight =  0.001717067207209766
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0021544346900318843, cost : 87.841 min
==========
At iteration 365, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  366 ; minimum lost =  0.04193906486034393 ; diff loss =  9.760260581970215e-07 ; diff weight =  0.0016090049175545573
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0014677992676220694, cost : 88.616 min
==========
At iteration 397, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  398 ; minimum lost =  0.03665323555469513 ; diff loss =  9.797513484954834e-07 ; diff weight =  0.0017650374211370945
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0010000000000000002, cost : 94.517 min
==========
At iteration 406, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  407 ; minimum lost =  0.032319385558366776 ; diff loss =  9.797513484954834e-07 ; diff weight =  0.0018071633530780673
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0006812920690579617, cost : 95.383 min
==========
At iteration 423, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  424 ; minimum lost =  0.02086063101887703 ; diff loss =  8.642673492431641e-07 ; diff weight =  0.0008496061200276017
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00014677992676220703, cost : 100.153 min
==========
At iteration 424, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  425 ; minimum lost =  0.01698790304362774 ; diff loss =  9.257346391677856e-07 ; diff weight =  0.0007873672875575721
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 6.81292069057961e-05, cost : 100.779 min
==========
At iteration 446, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  447 ; minimum lost =  0.010901515372097492 ; diff loss =  8.838251233100891e-07 ; diff weight =  0.0006095670978538692
lambda is : 1.4677992676220687e-05, cost : 103.641 min
==========
At iteration 445, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  446 ; minimum lost =  0.01869058422744274 ; diff loss =  7.580965757369995e-07 ; diff weight =  0.003733825171366334
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 9.999999999999991e-05, cost : 104.202 min
==========
At iteration 440, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  441 ; minimum lost =  0.012208547443151474 ; diff loss =  8.940696716308594e-07 ; diff weight =  0.0007474400335922837
At iteration 444, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  445 ; minimum lost =  0.02870093658566475 ; diff loss =  8.903443813323975e-07 ; diff weight =  0.002206838922575116
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00046415888336127795, cost : 104.507 min
==========
lambda is : 2.1544346900318854e-05, cost : 104.574 min
==========
At iteration 459, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  460 ; minimum lost =  0.025671591982245445 ; diff loss =  9.704381227493286e-07 ; diff weight =  0.0023815312888473272
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00031622776601683783, cost : 107.887 min
==========
At iteration 474, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  474 ; minimum lost =  0.015020536258816719 ; diff loss =  -8.530914783477783e-07 ; diff weight =  0.0067706117406487465
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 4.6415888336127784e-05, cost : 111.762 min
==========
At iteration 488, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  489 ; minimum lost =  0.008956320583820343 ; diff loss =  6.426125764846802e-07 ; diff weight =  0.0005264817737042904
At iteration 479, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  480 ; minimum lost =  0.0133899487555027 ; diff loss =  9.993091225624084e-07 ; diff weight =  0.0004900430212728679
lambda is : 9.999999999999997e-06, cost : 112.803 min
==========
lambda is : 3.16227766016838e-05, cost : 112.894 min
==========
At iteration 501, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  502 ; minimum lost =  0.022999916225671768 ; diff loss =  6.332993507385254e-07 ; diff weight =  0.0018765623681247234
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 117.713 min
==========
*** Collecting results ***
Exporting result Dict
Leiden_12 Time elapsed: 117.79253241618474 minutes.
Shape: (71915, 25712)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for Leiden_13
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda:Lambda: Lambda: Lambda: Lambda: Lambda:Lambda:1.5e-05 Lambda: Lambda:Lambda:Lambda:1e-05 Lambda: Lambda:3.2e-05 Lambda:Lambda:Lambda:4.6e-05  Lambda: Lambda:Lambda:starting at Lambda: Lambda:2.2e-05 Lambda: Lambda:  Lambda: Lambda:  starting at 0.000316  starting at    starting at 6.8e-05  0.002154   2024-11-23 02:50:59  0.0001  starting at  0.000147 0.046416 0.031623 0.068129 0.1 0.000215 2024-11-23 02:50:59 starting at 0.000464 2024-11-23 02:50:59 0.000681 0.001 0.001468 2024-11-23 02:50:59 starting at 0.003162 starting at 0.004642 0.006813 Max_iter: 0.01 starting at 0.014678 2024-11-23 02:50:59 0.021544 starting at starting at starting at starting at starting at starting at Max_iter: 2024-11-23 02:50:59 starting at Max_iter: starting at starting at starting at Max_iter: 2024-11-23 02:50:59 starting at 2024-11-23 02:50:59 starting at starting at 1000starting at 2024-11-23 02:50:59 starting at Max_iter: starting at 2024-11-23 02:50:59 2024-11-23 02:50:59 2024-11-23 02:50:59 2024-11-23 02:50:59 2024-11-23 02:50:59 2024-11-23 02:50:59 1000Max_iter: 2024-11-23 02:50:59 10002024-11-23 02:50:59 2024-11-23 02:50:59 2024-11-23 02:50:59 1000Max_iter: 2024-11-23 02:50:59 Max_iter: 2024-11-23 02:50:59 2024-11-23 02:50:59 
2024-11-23 02:50:59 Max_iter: 2024-11-23 02:50:59 10002024-11-23 02:50:59 Max_iter: Max_iter: Max_iter: Max_iter: Max_iter: Max_iter: 
1000Max_iter: 
Max_iter: Max_iter:Max_iter:
1000Max_iter: 1000Max_iter:Max_iter:Max_iter:1000Max_iter:
Max_iter: 100010001000100010001000
10001000
  1000
1000

 1000 1000 1000
 10001000






1000






At iteration 112, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  113 ; minimum lost =  0.18670134246349335 ; diff loss =  5.811452865600586e-07 ; diff weight =  0.00015146173245739192
lambda is : 0.10000000000000002, cost : 27.987 min
==========
At iteration 167, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  168 ; minimum lost =  0.1411193311214447 ; diff loss =  5.662441253662109e-07 ; diff weight =  0.00038449757266789675
lambda is : 0.04641588833612786, cost : 42.269 min
==========
At iteration 175, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  176 ; minimum lost =  0.12672236561775208 ; diff loss =  5.364418029785156e-07 ; diff weight =  0.00033822254044935107
lambda is : 0.0316227766016838, cost : 43.606 min
==========
At iteration 190, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  191 ; minimum lost =  0.1602230966091156 ; diff loss =  4.470348358154297e-08 ; diff weight =  5.916641021030955e-05
lambda is : 0.0681292069057962, cost : 46.893 min
==========
At iteration 265, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  266 ; minimum lost =  0.09326063096523285 ; diff loss =  6.48200511932373e-07 ; diff weight =  0.0005873373593203723
lambda is : 0.014677992676220709, cost : 66.794 min
==========
At iteration 281, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  282 ; minimum lost =  0.11138472706079483 ; diff loss =  8.642673492431641e-07 ; diff weight =  0.0009792629862204194
lambda is : 0.02154434690031885, cost : 68.467 min
==========
At iteration 365, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  366 ; minimum lost =  0.03376443311572075 ; diff loss =  7.711350917816162e-07 ; diff weight =  0.0016648174496367574
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0021544346900318843, cost : 88.698 min
==========
At iteration 371, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  372 ; minimum lost =  0.0769398882985115 ; diff loss =  9.685754776000977e-07 ; diff weight =  0.0014540418051183224
lambda is : 0.010000000000000004, cost : 88.779 min
==========
At iteration 380, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  381 ; minimum lost =  0.022750727832317352 ; diff loss =  8.195638656616211e-07 ; diff weight =  0.002398001030087471
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0010000000000000002, cost : 91.981 min
==========
At iteration 380, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  381 ; minimum lost =  0.05117537081241608 ; diff loss =  8.642673492431641e-07 ; diff weight =  0.0020813453011214733
lambda is : 0.004641588833612781, cost : 92.095 min
==========
At iteration 399, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  400 ; minimum lost =  0.06295382231473923 ; diff loss =  3.3527612686157227e-07 ; diff weight =  0.0005535489181056619
lambda is : 0.006812920690579613, cost : 94.323 min
==========
At iteration 411, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  412 ; minimum lost =  0.041492387652397156 ; diff loss =  9.94652509689331e-07 ; diff weight =  0.0018651437712833285
lambda is : 0.003162277660168382, cost : 97.667 min
==========
At iteration 415, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  416 ; minimum lost =  0.01885254681110382 ; diff loss =  9.052455425262451e-07 ; diff weight =  0.0025427849031984806
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0006812920690579617, cost : 99.216 min
==========
At iteration 427, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  428 ; minimum lost =  0.015771890059113503 ; diff loss =  9.499490261077881e-07 ; diff weight =  0.006136743817478418
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00046415888336127795, cost : 102.619 min
==========
At iteration 432, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  433 ; minimum lost =  0.02754574827849865 ; diff loss =  9.704381227493286e-07 ; diff weight =  0.0027987908106297255
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0014677992676220694, cost : 103.638 min
==========
At iteration 434, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  435 ; minimum lost =  0.013281254097819328 ; diff loss =  9.55536961555481e-07 ; diff weight =  0.0034554782323539257
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00031622776601683783, cost : 104.378 min
==========
At iteration 454, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  455 ; minimum lost =  0.011264711618423462 ; diff loss =  9.676441550254822e-07 ; diff weight =  0.0062461853958666325
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 109.048 min
==========
At iteration 460, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  461 ; minimum lost =  0.009616886265575886 ; diff loss =  9.527429938316345e-07 ; diff weight =  0.0032746128272265196
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00014677992676220703, cost : 109.236 min
==========
At iteration 496, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  497 ; minimum lost =  0.004607995506376028 ; diff loss =  8.675269782543182e-07 ; diff weight =  0.0025731814093887806
lambda is : 2.1544346900318854e-05, cost : 117.356 min
==========
At iteration 512, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  513 ; minimum lost =  0.006071454845368862 ; diff loss =  7.525086402893066e-07 ; diff weight =  0.0018851313507184386
lambda is : 4.6415888336127784e-05, cost : 119.75 min
==========
At iteration 510, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  511 ; minimum lost =  0.003282711608335376 ; diff loss =  8.956994861364365e-07 ; diff weight =  0.0023901157546788454
lambda is : 9.999999999999997e-06, cost : 119.954 min
==========
At iteration 528, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  529 ; minimum lost =  0.00523754907771945 ; diff loss =  5.387701094150543e-07 ; diff weight =  0.0010690497001633048
At iteration 521, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  522 ; minimum lost =  0.0081765903159976 ; diff loss =  9.480863809585571e-07 ; diff weight =  0.0062978435307741165
lambda is : 3.16227766016838e-05, cost : 123.576 min
==========
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 9.999999999999991e-05, cost : 123.621 min
==========
At iteration 522, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  523 ; minimum lost =  0.007016911171376705 ; diff loss =  5.820766091346741e-07 ; diff weight =  0.0014196374686434865
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 6.81292069057961e-05, cost : 124.02 min
==========
At iteration 548, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  549 ; minimum lost =  0.003660155227407813 ; diff loss =  2.2142194211483002e-07 ; diff weight =  0.00893444661051035
lambda is : 1.4677992676220687e-05, cost : 126.206 min
==========
*** Collecting results ***
Exporting result Dict
Leiden_13 Time elapsed: 126.28245935440063 minutes.
Shape: (71915, 25712)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for Leiden_14
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda:Lambda:Lambda:  Lambda:Lambda: Lambda: Lambda:Lambda:2.2e-05 Lambda:1e-05 Lambda: Lambda: Lambda:1.5e-05 Lambda:Lambda:Lambda:4.6e-05 Lambda:  Lambda: Lambda:Lambda:starting at Lambda: Lambda:Lambda:starting at Lambda:Lambda:Lambda:  3.2e-05  0.000316  starting at    starting at 0.003162 6.8e-05  0.0001   2024-11-23 04:57:23  0.000147   2024-11-23 04:57:23   0.1 0.000215 starting at 0.000464 starting at 0.000681 2024-11-23 04:57:23 0.001 0.001468 0.002154 2024-11-23 04:57:23 starting at starting at 0.004642 starting at 0.006813 0.01 Max_iter: 0.014678 starting at0.021544 0.031623 Max_iter: 0.046416 0.068129 starting at starting at 2024-11-23 04:57:23 starting at 2024-11-23 04:57:23 starting at Max_iter: starting at starting at starting at Max_iter: 2024-11-23 04:57:23 2024-11-23 04:57:23 starting at 2024-11-23 04:57:23 starting at starting at 1000starting at  2024-11-23 04:57:23starting at starting at 1000starting at starting at 2024-11-23 04:57:23 2024-11-23 04:57:23 Max_iter: 2024-11-23 04:57:23 Max_iter: 2024-11-23 04:57:23 10002024-11-23 04:57:23 2024-11-23 04:57:23 2024-11-23 04:57:23 1000Max_iter: Max_iter: 2024-11-23 04:57:23 Max_iter: 2024-11-23 04:57:23 2024-11-23 04:57:23 
2024-11-23 04:57:23  Max_iter:2024-11-23 04:57:23 2024-11-23 04:57:23 
2024-11-23 04:57:23 2024-11-23 04:57:23 Max_iter: Max_iter: 1000
Max_iter: 1000Max_iter: 
Max_iter: Max_iter: Max_iter: 
10001000Max_iter:1000Max_iter:Max_iter:Max_iter: Max_iter:Max_iter: Max_iter:Max_iter: 100010001000
1000100010001000

 1000
 1000 1000 10001000 10001000
 10001000














At iteration 104, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  105 ; minimum lost =  0.1631672978401184 ; diff loss =  1.1920928955078125e-07 ; diff weight =  0.0001012746652122587
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.10000000000000002, cost : 26.693 min
==========
At iteration 111, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  111 ; minimum lost =  0.11734216660261154 ; diff loss =  0.0 ; diff weight =  2.148747444152832e-05
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.04641588833612786, cost : 28.298 min
==========
At iteration 112, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  113 ; minimum lost =  0.13681049644947052 ; diff loss =  1.043081283569336e-07 ; diff weight =  0.00012034115934511647
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.0681292069057962, cost : 28.475 min
==========
At iteration 150, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  151 ; minimum lost =  0.10317789763212204 ; diff loss =  9.98377799987793e-07 ; diff weight =  0.00023748219246044755
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.0316227766016838, cost : 37.174 min
==========
At iteration 154, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  155 ; minimum lost =  0.09296217560768127 ; diff loss =  6.556510925292969e-07 ; diff weight =  0.000495951680932194
lambda is : 0.02154434690031885, cost : 38.342 min
==========
At iteration 260, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  261 ; minimum lost =  0.08099353313446045 ; diff loss =  6.631016731262207e-07 ; diff weight =  0.0006788962054997683
lambda is : 0.014677992676220709, cost : 64.508 min
==========
At iteration 275, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  276 ; minimum lost =  0.06868123263120651 ; diff loss =  7.227063179016113e-07 ; diff weight =  0.0009052943787537515
lambda is : 0.010000000000000004, cost : 67.468 min
==========
At iteration 279, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  280 ; minimum lost =  0.048032160848379135 ; diff loss =  8.381903171539307e-07 ; diff weight =  0.000863477645907551
lambda is : 0.004641588833612781, cost : 68.051 min
==========
At iteration 340, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  341 ; minimum lost =  0.057367537170648575 ; diff loss =  5.513429641723633e-07 ; diff weight =  0.00029528079903684556
lambda is : 0.006812920690579613, cost : 81.239 min
==========
At iteration 339, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  340 ; minimum lost =  0.034025512635707855 ; diff loss =  9.909272193908691e-07 ; diff weight =  0.002520994283258915
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0021544346900318843, cost : 82.565 min
==========
At iteration 348, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  349 ; minimum lost =  0.040178488940000534 ; diff loss =  6.407499313354492e-07 ; diff weight =  0.0014570786152034998
lambda is : 0.003162277660168382, cost : 84.507 min
==========
At iteration 357, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  358 ; minimum lost =  0.021477168425917625 ; diff loss =  8.288770914077759e-07 ; diff weight =  0.0039973389357328415
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0006812920690579617, cost : 85.792 min
==========
At iteration 370, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  371 ; minimum lost =  0.028932612389326096 ; diff loss =  9.797513484954834e-07 ; diff weight =  0.0028120381757616997
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0014677992676220694, cost : 89.88 min
==========
At iteration 387, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  388 ; minimum lost =  0.02478703483939171 ; diff loss =  8.773058652877808e-07 ; diff weight =  0.002594961319118738
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0010000000000000002, cost : 92.092 min
==========
At iteration 407, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  408 ; minimum lost =  0.01873565837740898 ; diff loss =  9.611248970031738e-07 ; diff weight =  0.0037220066878944635
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00046415888336127795, cost : 94.685 min
==========
At iteration 417, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  418 ; minimum lost =  0.010970257222652435 ; diff loss =  8.25151801109314e-07 ; diff weight =  0.0007178117521107197
lambda is : 6.81292069057961e-05, cost : 98.188 min
==========
At iteration 414, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  415 ; minimum lost =  0.012161326594650745 ; diff loss =  8.00006091594696e-07 ; diff weight =  0.0007934608729556203
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 9.999999999999991e-05, cost : 98.964 min
==========
At iteration 428, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  429 ; minimum lost =  0.00972252432256937 ; diff loss =  7.133930921554565e-07 ; diff weight =  0.0006123369676060975
lambda is : 4.6415888336127784e-05, cost : 100.792 min
==========
At iteration 432, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  433 ; minimum lost =  0.013368065468966961 ; diff loss =  3.9581209421157837e-07 ; diff weight =  0.0060634370893239975
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00014677992676220703, cost : 100.973 min
==========
At iteration 441, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  442 ; minimum lost =  0.007519247941672802 ; diff loss =  9.853392839431763e-07 ; diff weight =  0.00409260019659996
lambda is : 2.1544346900318854e-05, cost : 101.594 min
==========
At iteration 440, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  441 ; minimum lost =  0.01481863483786583 ; diff loss =  6.332993507385254e-07 ; diff weight =  0.0010348433861508965
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 103.987 min
==========
At iteration 450, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  451 ; minimum lost =  0.016548652201890945 ; diff loss =  9.94652509689331e-07 ; diff weight =  0.00496277492493391
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00031622776601683783, cost : 105.666 min
==========
At iteration 465, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  466 ; minimum lost =  0.005599022842943668 ; diff loss =  9.32253897190094e-07 ; diff weight =  0.0012157699093222618
lambda is : 9.999999999999997e-06, cost : 109.764 min
==========
At iteration 468, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  469 ; minimum lost =  0.008371034637093544 ; diff loss =  6.444752216339111e-07 ; diff weight =  0.0014614843530580401
lambda is : 3.16227766016838e-05, cost : 110.243 min
==========
At iteration 533, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  534 ; minimum lost =  0.005937461741268635 ; diff loss =  7.115304470062256e-07 ; diff weight =  0.006016450002789497
lambda is : 1.4677992676220687e-05, cost : 121.502 min
==========
*** Collecting results ***
Exporting result Dict
Leiden_14 Time elapsed: 121.57452254692713 minutes.
Shape: (71915, 25712)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for Leiden_15
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda:Lambda:Lambda:Lambda: Lambda:Lambda:  Lambda:Lambda: Lambda:Lambda:1e-05 Lambda: Lambda: Lambda:1.5e-05 Lambda:Lambda:2.2e-05  Lambda:Lambda: Lambda:3.2e-05 Lambda: Lambda: Lambda: Lambda:starting at Lambda: Lambda:Lambda:4.6e-05  6.8e-05  starting at   starting at 0.0001   0.000147  starting at  0.000215  0.000316 0.021544  2024-11-23 06:59:04  0.000464   starting at 0.000681 starting at 0.001 2024-11-23 06:59:04 0.001468 0.002154 2024-11-23 06:59:04 starting at 0.003162 0.004642 starting at 0.006813 2024-11-23 06:59:04 0.01 starting at 0.014678 starting at starting at 0.031623 Max_iter: 0.046416 starting at 0.068129 0.1 2024-11-23 06:59:04 starting at 2024-11-23 06:59:04 starting at Max_iter: starting at starting at Max_iter: 2024-11-23 06:59:04 starting at starting at 2024-11-23 06:59:04 starting at Max_iter: starting at 2024-11-23 06:59:04 starting at 2024-11-23 06:59:04 2024-11-23 06:59:04 starting at 1000starting at 2024-11-23 06:59:04 starting at starting at Max_iter: 2024-11-23 06:59:04 Max_iter: 2024-11-23 06:59:04 1000
2024-11-23 06:59:04 2024-11-23 06:59:04 1000Max_iter: 2024-11-23 06:59:04 2024-11-23 06:59:04 Max_iter: 2024-11-23 06:59:04 10002024-11-23 06:59:04 Max_iter: 2024-11-23 06:59:04 Max_iter: Max_iter: 2024-11-23 06:59:04 
2024-11-23 06:59:04 Max_iter: 2024-11-23 06:59:04 2024-11-23 06:59:04 1000Max_iter: 1000Max_iter: Max_iter: Max_iter:
1000Max_iter: Max_iter:1000Max_iter: 
Max_iter:1000Max_iter:10001000Max_iter:Max_iter:1000Max_iter:Max_iter:
1000
10001000 1000
1000
 
1000 1000
 

  
  



1000


1000
1000
1000
10001000

At iteration 146, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  147 ; minimum lost =  0.09401275217533112 ; diff loss =  9.760260581970215e-07 ; diff weight =  0.0007878975011408329
lambda is : 0.0316227766016838, cost : 35.814 min
==========
At iteration 175, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  176 ; minimum lost =  0.11031343042850494 ; diff loss =  4.991888999938965e-07 ; diff weight =  0.00022268180327955633
lambda is : 0.04641588833612786, cost : 42.861 min
==========
At iteration 191, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  192 ; minimum lost =  0.07304033637046814 ; diff loss =  8.195638656616211e-07 ; diff weight =  0.0017929133027791977
lambda is : 0.014677992676220709, cost : 46.809 min
==========
At iteration 208, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  209 ; minimum lost =  0.08185562491416931 ; diff loss =  9.98377799987793e-07 ; diff weight =  0.0010834226850420237
lambda is : 0.02154434690031885, cost : 50.614 min
==========
At iteration 206, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  207 ; minimum lost =  0.13246896862983704 ; diff loss =  3.427267074584961e-07 ; diff weight =  0.002555540530011058
lambda is : 0.0681292069057962, cost : 50.774 min
==========
At iteration 208, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  209 ; minimum lost =  0.06592311710119247 ; diff loss =  7.82310962677002e-07 ; diff weight =  0.0013915668241679668
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.010000000000000004, cost : 51.68 min
==========
At iteration 257, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  258 ; minimum lost =  0.05808842182159424 ; diff loss =  9.57399606704712e-07 ; diff weight =  0.0022733069490641356
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.006812920690579613, cost : 62.335 min
==========
At iteration 291, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  292 ; minimum lost =  0.03381988778710365 ; diff loss =  6.556510925292969e-07 ; diff weight =  0.0007502567605115473
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0014677992676220694, cost : 70.78 min
==========
At iteration 299, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  300 ; minimum lost =  0.05058910697698593 ; diff loss =  9.685754776000977e-07 ; diff weight =  0.0016936574829742312
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.004641588833612781, cost : 70.953 min
==========
At iteration 324, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  325 ; minimum lost =  0.03827701136469841 ; diff loss =  8.381903171539307e-07 ; diff weight =  0.0019929867703467607
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0021544346900318843, cost : 78.49 min
==========
At iteration 330, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  331 ; minimum lost =  0.16195902228355408 ; diff loss =  9.5367431640625e-07 ; diff weight =  0.00034371987567283213
lambda is : 0.10000000000000002, cost : 79.055 min
==========
At iteration 343, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  344 ; minimum lost =  0.020859286189079285 ; diff loss =  3.3155083656311035e-07 ; diff weight =  0.0005553878145292401
At iteration 337, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  338 ; minimum lost =  0.017809251323342323 ; diff loss =  7.133930921554565e-07 ; diff weight =  0.00042171802488155663
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 82.636 min
==========
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 9.999999999999991e-05, cost : 82.826 min
==========
At iteration 349, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  350 ; minimum lost =  0.02256365679204464 ; diff loss =  9.182840585708618e-07 ; diff weight =  0.0010633430210873485
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00031622776601683783, cost : 83.356 min
==========
At iteration 361, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  362 ; minimum lost =  0.027044253423810005 ; diff loss =  9.406358003616333e-07 ; diff weight =  0.002948101842775941
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0006812920690579617, cost : 86.178 min
==========
At iteration 360, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  361 ; minimum lost =  0.030036062002182007 ; diff loss =  6.984919309616089e-07 ; diff weight =  0.0023255222477018833
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0010000000000000002, cost : 86.235 min
==========
At iteration 353, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  354 ; minimum lost =  0.04384220018982887 ; diff loss =  5.699694156646729e-07 ; diff weight =  0.0011862947139889002
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.003162277660168382, cost : 86.51 min
==========
At iteration 358, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  359 ; minimum lost =  0.019189059734344482 ; diff loss =  9.201467037200928e-07 ; diff weight =  0.00032089330488815904
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00014677992676220703, cost : 87.062 min
==========
At iteration 363, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  364 ; minimum lost =  0.014698425307869911 ; diff loss =  8.810311555862427e-07 ; diff weight =  0.0005764389061369002
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 4.6415888336127784e-05, cost : 88.503 min
==========
At iteration 388, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  389 ; minimum lost =  0.01593734696507454 ; diff loss =  8.102506399154663e-07 ; diff weight =  0.005411242134869099
At iteration 383, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  384 ; minimum lost =  0.013219790533185005 ; diff loss =  6.854534149169922e-07 ; diff weight =  0.001953307306393981
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 6.81292069057961e-05, cost : 91.807 min
==========
lambda is : 3.16227766016838e-05, cost : 92.063 min
==========
At iteration 399, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  400 ; minimum lost =  0.02459137886762619 ; diff loss =  4.3958425521850586e-07 ; diff weight =  0.002447249833494425
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00046415888336127795, cost : 92.575 min
==========
At iteration 402, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  403 ; minimum lost =  0.008668944239616394 ; diff loss =  9.424984455108643e-07 ; diff weight =  0.0014541433192789555
lambda is : 9.999999999999997e-06, cost : 93.99 min
==========
At iteration 458, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  459 ; minimum lost =  0.009367008693516254 ; diff loss =  7.105991244316101e-07 ; diff weight =  0.013501633889973164
At iteration 462, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  463 ; minimum lost =  0.011146757751703262 ; diff loss =  8.977949619293213e-07 ; diff weight =  0.01653917133808136
lambda is : 1.4677992676220687e-05, cost : 105.874 min
==========
lambda is : 2.1544346900318854e-05, cost : 105.994 min
==========
*** Collecting results ***
Exporting result Dict
Leiden_15 Time elapsed: 106.07486068805059 minutes.
Shape: (71915, 25712)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for Leiden_16
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda:Lambda: Lambda: Lambda:1e-05 Lambda:Lambda:Lambda: Lambda:1.5e-05  Lambda:Lambda:Lambda:starting at Lambda:Lambda:  Lambda: 2.2e-05 Lambda: Lambda:Lambda:starting at Lambda:3.2e-05 Lambda: Lambda: Lambda: Lambda:Lambda:2024-11-23 08:45:15 Lambda: Lambda:  4.6e-05 6.8e-05  0.000681 starting at  0.0001   2024-11-23 08:45:15  starting at  0.000147  0.000215 0.000316   Max_iter:  0.000464 0.1 0.068129 starting at starting at 0.001 starting at 2024-11-23 08:45:15 0.001468 starting at 0.002154 0.003162 Max_iter: 0.004642 2024-11-23 08:45:15 0.006813 starting at 0.01  starting at0.014678starting at 0.021544 0.031623 10000.046416 starting at starting at starting at 2024-11-23 08:45:15 2024-11-23 08:45:15starting at 2024-11-23 08:45:15 Max_iter: starting at 2024-11-23 08:45:15 starting at starting at 1000starting at Max_iter: starting at 2024-11-23 08:45:15 starting at  2024-11-23 08:45:15 2024-11-23 08:45:15 starting at starting at 
starting at 2024-11-23 08:45:15 2024-11-23 08:45:15 2024-11-23 08:45:15 Max_iter:  Max_iter:2024-11-23 08:45:15 Max_iter: 10002024-11-23 08:45:15 Max_iter: 2024-11-23 08:45:15 2024-11-23 08:45:15 
2024-11-23 08:45:1510002024-11-23 08:45:15 Max_iter: 2024-11-23 08:45:15  Max_iter:starting at Max_iter: 2024-11-23 08:45:15 2024-11-23 08:45:15 2024-11-23 08:45:15 Max_iter: Max_iter: Max_iter: 1000 Max_iter:1000
Max_iter:1000Max_iter:Max_iter: 
Max_iter:1000Max_iter: 2024-11-23 08:45:151000Max_iter:Max_iter:Max_iter:100010001000
1000 
 
  1000Max_iter: 
 1000 
   



100010001000
 10001000
Max_iter: 100010001000


1000


1000



At iteration 97, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  98 ; minimum lost =  0.1460774838924408 ; diff loss =  1.7881393432617188e-07 ; diff weight =  0.00013610522728413343
lambda is : 0.10000000000000002, cost : 24.125 min
==========
At iteration 112, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  113 ; minimum lost =  0.11883273720741272 ; diff loss =  1.4901161193847656e-07 ; diff weight =  0.00032762496266514063
lambda is : 0.0681292069057962, cost : 27.958 min
==========
At iteration 124, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  125 ; minimum lost =  0.0983336865901947 ; diff loss =  2.086162567138672e-07 ; diff weight =  0.00015111597895156592
lambda is : 0.04641588833612786, cost : 30.519 min
==========
At iteration 124, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  125 ; minimum lost =  0.0719320997595787 ; diff loss =  9.611248970031738e-07 ; diff weight =  0.00017060121172107756
lambda is : 0.02154434690031885, cost : 30.697 min
==========
At iteration 141, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  142 ; minimum lost =  0.0580582320690155 ; diff loss =  9.98377799987793e-07 ; diff weight =  0.00043266487773507833
lambda is : 0.010000000000000004, cost : 34.317 min
==========
At iteration 143, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  144 ; minimum lost =  0.08311577141284943 ; diff loss =  9.685754776000977e-07 ; diff weight =  0.0012666956754401326
lambda is : 0.0316227766016838, cost : 34.887 min
==========
At iteration 146, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  147 ; minimum lost =  0.06386847048997879 ; diff loss =  9.313225746154785e-07 ; diff weight =  0.0010166614083573222
lambda is : 0.014677992676220709, cost : 36.112 min
==========
At iteration 248, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  249 ; minimum lost =  0.04752209782600403 ; diff loss =  9.797513484954834e-07 ; diff weight =  0.0016401582397520542
lambda is : 0.004641588833612781, cost : 58.859 min
==========
At iteration 279, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  280 ; minimum lost =  0.03850196301937103 ; diff loss =  9.909272193908691e-07 ; diff weight =  0.0021703368984162807
lambda is : 0.0021544346900318843, cost : 66.998 min
==========
At iteration 295, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  296 ; minimum lost =  0.03453059121966362 ; diff loss =  9.5367431640625e-07 ; diff weight =  0.003583332058042288
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0014677992676220694, cost : 70.405 min
==========
At iteration 303, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  304 ; minimum lost =  0.04275823384523392 ; diff loss =  9.201467037200928e-07 ; diff weight =  0.001659165951423347
lambda is : 0.003162277660168382, cost : 71.483 min
==========
At iteration 320, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  321 ; minimum lost =  0.027686497196555138 ; diff loss =  9.872019290924072e-07 ; diff weight =  0.002662609564140439
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0006812920690579617, cost : 76.236 min
==========
At iteration 342, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  343 ; minimum lost =  0.05279739573597908 ; diff loss =  8.903443813323975e-07 ; diff weight =  0.0030236593447625637
lambda is : 0.006812920690579613, cost : 79.009 min
==========
At iteration 332, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  333 ; minimum lost =  0.030842069536447525 ; diff loss =  9.55536961555481e-07 ; diff weight =  0.0035594983492046595
lambda is : 0.0010000000000000002, cost : 79.373 min
==========
At iteration 346, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  347 ; minimum lost =  0.024914497509598732 ; diff loss =  9.592622518539429e-07 ; diff weight =  0.0027337681967765093
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00046415888336127795, cost : 81.913 min
==========
At iteration 354, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  355 ; minimum lost =  0.014748282730579376 ; diff loss =  6.658956408500671e-07 ; diff weight =  0.0007461264613084495
At iteration 352, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  353 ; minimum lost =  0.013299401849508286 ; diff loss =  8.754432201385498e-07 ; diff weight =  0.0009053386165760458
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 6.81292069057961e-05, cost : 83.148 min
==========
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 4.6415888336127784e-05, cost : 83.394 min
==========
At iteration 353, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  354 ; minimum lost =  0.020213399082422256 ; diff loss =  5.345791578292847e-07 ; diff weight =  0.0009120613103732467
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 83.923 min
==========
At iteration 366, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  367 ; minimum lost =  0.022420255467295647 ; diff loss =  8.959323167800903e-07 ; diff weight =  0.0028052518609911203
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00031622776601683783, cost : 85.117 min
==========
At iteration 362, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  363 ; minimum lost =  0.011660276912152767 ; diff loss =  8.903443813323975e-07 ; diff weight =  0.000994761590845883
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 3.16227766016838e-05, cost : 85.78 min
==========
At iteration 381, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  382 ; minimum lost =  0.010045711882412434 ; diff loss =  7.497146725654602e-07 ; diff weight =  0.0009861491853371263
lambda is : 2.1544346900318854e-05, cost : 87.98 min
==========
At iteration 388, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  389 ; minimum lost =  0.018132546916604042 ; diff loss =  5.308538675308228e-07 ; diff weight =  0.005269244778901339
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00014677992676220703, cost : 89.485 min
==========
At iteration 381, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  382 ; minimum lost =  0.016272960230708122 ; diff loss =  8.959323167800903e-07 ; diff weight =  0.005197867285460234
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 9.999999999999991e-05, cost : 90.335 min
==========
At iteration 403, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  404 ; minimum lost =  0.007291353773325682 ; diff loss =  7.03614205121994e-07 ; diff weight =  0.0005839602090418339
lambda is : 9.999999999999997e-06, cost : 94.44 min
==========
At iteration 434, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  435 ; minimum lost =  0.008266342803835869 ; diff loss =  5.476176738739014e-07 ; diff weight =  0.0015640774508938193
lambda is : 1.4677992676220687e-05, cost : 100.067 min
==========
*** Collecting results ***
Exporting result Dict
Leiden_16 Time elapsed: 100.14057343800863 minutes.
Shape: (71915, 25712)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for Leiden_17
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda:Lambda:Lambda:Lambda:  Lambda: Lambda:Lambda: Lambda:Lambda:Lambda:1e-05 3.2e-05  Lambda: Lambda:1.5e-05 Lambda: Lambda: Lambda:2.2e-05  Lambda: Lambda:Lambda:Lambda: Lambda:Lambda:starting at Lambda:Lambda:starting at Lambda: Lambda:0.000215 0.000316  starting at  4.6e-05  6.8e-05  starting at 0.002154  0.0001    0.000147   2024-11-23 10:25:30   2024-11-23 10:25:30 0.068129  starting at starting at 0.000464 2024-11-23 10:25:30 0.000681 starting at 0.001 starting at 0.001468 2024-11-23 10:25:30 starting at 0.004642 starting at 0.003162 0.006813 0.01 starting at 0.014678 0.021544 Max_iter: 0.031623 0.046416 Max_iter: starting at 0.1 2024-11-23 10:25:30 2024-11-23 10:25:30 starting at Max_iter: starting at2024-11-23 10:25:30 starting at 2024-11-23 10:25:30 starting at Max_iter: 2024-11-23 10:25:30 starting at 2024-11-23 10:25:30 starting at starting at starting at 2024-11-23 10:25:30 starting at starting at 1000starting at starting at 10002024-11-23 10:25:30 starting at Max_iter: Max_iter: 2024-11-23 10:25:30 1000 2024-11-23 10:25:30Max_iter: 2024-11-23 10:25:30 Max_iter: 2024-11-23 10:25:30 1000Max_iter: 2024-11-23 10:25:30 Max_iter: 2024-11-23 10:25:30 2024-11-23 10:25:30 2024-11-23 10:25:30 Max_iter: 2024-11-23 10:25:30 2024-11-23 10:25:30 
2024-11-23 10:25:30 2024-11-23 10:25:30 
Max_iter: 2024-11-23 10:25:30 10001000Max_iter: 
 Max_iter:1000Max_iter: 1000Max_iter: 
1000Max_iter: 1000Max_iter:Max_iter: Max_iter:1000Max_iter:Max_iter:Max_iter:Max_iter:1000Max_iter:

1000 
1000
1000
1000
 10001000
 1000
    
 1000
1000




1000
10001000
1000



At iteration 101, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  102 ; minimum lost =  0.055378034710884094 ; diff loss =  5.62518835067749e-07 ; diff weight =  0.0004055287572555244
lambda is : 0.02154434690031885, cost : 26.01 min
==========
At iteration 117, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  118 ; minimum lost =  0.06731252372264862 ; diff loss =  4.917383193969727e-07 ; diff weight =  0.0002621001040097326
lambda is : 0.0316227766016838, cost : 29.907 min
==========
At iteration 127, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  128 ; minimum lost =  0.04632820188999176 ; diff loss =  9.872019290924072e-07 ; diff weight =  0.0007964387768879533
lambda is : 0.014677992676220709, cost : 32.164 min
==========
At iteration 137, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  138 ; minimum lost =  0.08308185636997223 ; diff loss =  4.3213367462158203e-07 ; diff weight =  0.0003872728848364204
lambda is : 0.04641588833612786, cost : 34.315 min
==========
At iteration 160, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  161 ; minimum lost =  0.1322641223669052 ; diff loss =  8.940696716308594e-08 ; diff weight =  0.00012841280840802938
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.10000000000000002, cost : 39.978 min
==========
At iteration 171, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  172 ; minimum lost =  0.10433866828680038 ; diff loss =  9.611248970031738e-07 ; diff weight =  0.0006968125817365944
lambda is : 0.0681292069057962, cost : 42.262 min
==========
At iteration 183, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  184 ; minimum lost =  0.03955104947090149 ; diff loss =  8.23289155960083e-07 ; diff weight =  0.002975117415189743
lambda is : 0.010000000000000004, cost : 45.643 min
==========
At iteration 210, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  211 ; minimum lost =  0.028997503221035004 ; diff loss =  9.126961231231689e-07 ; diff weight =  0.0011571869254112244
lambda is : 0.004641588833612781, cost : 51.251 min
==========
At iteration 217, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  218 ; minimum lost =  0.03391679376363754 ; diff loss =  9.350478649139404e-07 ; diff weight =  0.002019032370299101
lambda is : 0.006812920690579613, cost : 52.89 min
==========
At iteration 239, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  240 ; minimum lost =  0.024693980813026428 ; diff loss =  9.499490261077881e-07 ; diff weight =  0.0031487855594605207
lambda is : 0.003162277660168382, cost : 57.471 min
==========
At iteration 242, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  243 ; minimum lost =  0.02116994932293892 ; diff loss =  8.493661880493164e-07 ; diff weight =  0.0017587912734597921
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0021544346900318843, cost : 57.574 min
==========
At iteration 299, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  300 ; minimum lost =  0.018099650740623474 ; diff loss =  9.275972843170166e-07 ; diff weight =  0.0020993733778595924
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0014677992676220694, cost : 70.776 min
==========
At iteration 328, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  329 ; minimum lost =  0.013433417305350304 ; diff loss =  8.642673492431641e-07 ; diff weight =  0.0029682195745408535
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0006812920690579617, cost : 77.93 min
==========
At iteration 333, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  334 ; minimum lost =  0.015578130260109901 ; diff loss =  9.052455425262451e-07 ; diff weight =  0.0026537994854152203
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0010000000000000002, cost : 79.701 min
==========
At iteration 347, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  348 ; minimum lost =  0.011602319777011871 ; diff loss =  8.605420589447021e-07 ; diff weight =  0.0036333121825009584
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00046415888336127795, cost : 81.147 min
==========
At iteration 345, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  346 ; minimum lost =  0.0055638812482357025 ; diff loss =  5.73229044675827e-07 ; diff weight =  0.0021190540865063667
lambda is : 6.81292069057961e-05, cost : 81.855 min
==========
At iteration 350, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  351 ; minimum lost =  0.006421388126909733 ; diff loss =  9.778887033462524e-07 ; diff weight =  0.0009487149072811007
lambda is : 9.999999999999991e-05, cost : 82.549 min
==========
At iteration 359, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  360 ; minimum lost =  0.00997643917798996 ; diff loss =  8.670613169670105e-07 ; diff weight =  0.004391278140246868
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00031622776601683783, cost : 84.167 min
==========
At iteration 376, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  377 ; minimum lost =  0.007361048832535744 ; diff loss =  9.350478649139404e-07 ; diff weight =  0.002126753330230713
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00014677992676220703, cost : 87.269 min
==========
At iteration 382, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  383 ; minimum lost =  0.0033354712650179863 ; diff loss =  9.620562195777893e-07 ; diff weight =  0.0015977491857483983
lambda is : 2.1544346900318854e-05, cost : 87.493 min
==========
At iteration 372, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  373 ; minimum lost =  0.0040425206534564495 ; diff loss =  6.309710443019867e-07 ; diff weight =  0.001168802147731185
lambda is : 3.16227766016838e-05, cost : 87.688 min
==========
At iteration 387, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  388 ; minimum lost =  0.004571919329464436 ; diff loss =  6.398186087608337e-07 ; diff weight =  0.009163441136479378
lambda is : 4.6415888336127784e-05, cost : 89.963 min
==========
At iteration 393, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  394 ; minimum lost =  0.008550658822059631 ; diff loss =  9.937211871147156e-07 ; diff weight =  0.004383610095828772
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 91.048 min
==========
At iteration 407, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  408 ; minimum lost =  0.0026685893535614014 ; diff loss =  4.810281097888947e-07 ; diff weight =  0.0025640539824962616
lambda is : 1.4677992676220687e-05, cost : 93.438 min
==========
At iteration 442, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  443 ; minimum lost =  0.0022097760811448097 ; diff loss =  3.3923424780368805e-07 ; diff weight =  0.012447953224182129
lambda is : 9.999999999999997e-06, cost : 100.152 min
==========
*** Collecting results ***
Exporting result Dict
Leiden_17 Time elapsed: 100.2431932369868 minutes.
Shape: (71915, 25712)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for Leiden_18
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda:Lambda: Lambda: Lambda:Lambda:1.5e-05 Lambda: Lambda:Lambda:1e-05 Lambda: Lambda: Lambda:Lambda:Lambda:starting at  Lambda:Lambda:2.2e-05 Lambda:Lambda:  Lambda:Lambda:starting at Lambda: Lambda:3.2e-05 Lambda:Lambda: 0.000147 Lambda:Lambda:   2024-11-23 12:05:53 4.6e-05   starting at   6.8e-05 0.003162   2024-11-23 12:05:53  0.0001  starting at   0.000215 starting at   0.000316 0.046416 0.000464 Max_iter: starting at 0.001 0.000681 2024-11-23 12:05:53 0.001468 0.002154 starting at starting at 0.004642 0.006813 Max_iter: 0.01 starting at 0.014678 2024-11-23 12:05:53 0.021544 0.031623 starting at 2024-11-23 12:05:53 0.1 0.068129 starting at starting at starting at 10002024-11-23 12:05:53 starting at starting at Max_iter: starting at starting at 2024-11-23 12:05:53 2024-11-23 12:05:53 starting at starting at 1000starting at 2024-11-23 12:05:53 starting at Max_iter: starting at starting at 2024-11-23 12:05:53 Max_iter: starting at starting at 2024-11-23 12:05:53 2024-11-23 12:05:53 2024-11-23 12:05:53 
Max_iter: 2024-11-23 12:05:53 2024-11-23 12:05:53 10002024-11-23 12:05:53 2024-11-23 12:05:53 Max_iter: Max_iter: 2024-11-23 12:05:53 2024-11-23 12:05:53 
2024-11-23 12:05:53 Max_iter: 2024-11-23 12:05:53 10002024-11-23 12:05:53 2024-11-23 12:05:53 Max_iter: 10002024-11-23 12:05:53 2024-11-23 12:05:53 Max_iter: Max_iter: Max_iter:1000Max_iter: 1000
Max_iter:
Max_iter:Max_iter:10001000Max_iter:Max_iter:Max_iter:1000Max_iter:
Max_iter:Max_iter:1000
Max_iter:Max_iter:10001000 
  1000 1000

   
   
  

10001000

10001000
10001000
1000
1000
1000
1000




At iteration 140, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  141 ; minimum lost =  0.03903675451874733 ; diff loss =  6.742775440216064e-07 ; diff weight =  0.00016348839562851936
lambda is : 0.010000000000000004, cost : 33.85 min
==========
At iteration 150, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  151 ; minimum lost =  0.028221186250448227 ; diff loss =  9.126961231231689e-07 ; diff weight =  0.0012614944716915488
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.004641588833612781, cost : 36.147 min
==========
At iteration 152, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  153 ; minimum lost =  0.08338170498609543 ; diff loss =  9.164214134216309e-07 ; diff weight =  0.0005215391283854842
lambda is : 0.04641588833612786, cost : 37.116 min
==========
At iteration 156, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  157 ; minimum lost =  0.13282878696918488 ; diff loss =  9.387731552124023e-07 ; diff weight =  0.000401784956920892
lambda is : 0.10000000000000002, cost : 38.247 min
==========
At iteration 161, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  162 ; minimum lost =  0.05482121556997299 ; diff loss =  9.834766387939453e-07 ; diff weight =  0.0016147404676303267
lambda is : 0.02154434690031885, cost : 39.364 min
==========
At iteration 164, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  165 ; minimum lost =  0.04567040503025055 ; diff loss =  8.456408977508545e-07 ; diff weight =  0.0003683649701997638
lambda is : 0.014677992676220709, cost : 39.559 min
==========
At iteration 169, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  170 ; minimum lost =  0.10465648770332336 ; diff loss =  6.109476089477539e-07 ; diff weight =  0.00026729353703558445
lambda is : 0.0681292069057962, cost : 41.692 min
==========
At iteration 174, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  175 ; minimum lost =  0.06693165004253387 ; diff loss =  1.2665987014770508e-07 ; diff weight =  0.0002565456088632345
lambda is : 0.0316227766016838, cost : 41.827 min
==========
At iteration 206, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  207 ; minimum lost =  0.033013053238391876 ; diff loss =  7.525086402893066e-07 ; diff weight =  0.0013246601447463036
lambda is : 0.006812920690579613, cost : 49.188 min
==========
At iteration 235, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  236 ; minimum lost =  0.017304159700870514 ; diff loss =  9.480863809585571e-07 ; diff weight =  0.0027872812934219837
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0014677992676220694, cost : 56.685 min
==========
At iteration 239, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  240 ; minimum lost =  0.02350635826587677 ; diff loss =  1.3597309589385986e-07 ; diff weight =  0.0008018948719836771
lambda is : 0.003162277660168382, cost : 57.322 min
==========
At iteration 259, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  260 ; minimum lost =  0.014912940561771393 ; diff loss =  9.797513484954834e-07 ; diff weight =  0.003199527272954583
lambda is : 0.0010000000000000002, cost : 60.782 min
==========
At iteration 280, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  281 ; minimum lost =  0.019986506551504135 ; diff loss =  4.0978193283081055e-07 ; diff weight =  0.0008732540300115943
lambda is : 0.0021544346900318843, cost : 65.541 min
==========
At iteration 314, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  315 ; minimum lost =  0.010866925120353699 ; diff loss =  9.778887033462524e-07 ; diff weight =  0.0037903995253145695
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00046415888336127795, cost : 74.032 min
==========
At iteration 320, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  321 ; minimum lost =  0.012723252177238464 ; diff loss =  8.521601557731628e-07 ; diff weight =  0.002567775547504425
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0006812920690579617, cost : 76.222 min
==========
At iteration 343, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  344 ; minimum lost =  0.009228667244315147 ; diff loss =  9.98377799987793e-07 ; diff weight =  0.004262670874595642
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00031622776601683783, cost : 82.41 min
==========
At iteration 351, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  352 ; minimum lost =  0.006759929470717907 ; diff loss =  8.395873010158539e-07 ; diff weight =  0.004240886308252811
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00014677992676220703, cost : 82.728 min
==========
At iteration 361, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  362 ; minimum lost =  0.007890891283750534 ; diff loss =  9.51811671257019e-07 ; diff weight =  0.004108191933482885
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 83.768 min
==========
At iteration 366, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  367 ; minimum lost =  0.004978928714990616 ; diff loss =  3.986060619354248e-07 ; diff weight =  0.0005449994350783527
lambda is : 6.81292069057961e-05, cost : 86.201 min
==========
At iteration 376, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  377 ; minimum lost =  0.004212181083858013 ; diff loss =  8.414499461650848e-07 ; diff weight =  0.0005414577899500728
lambda is : 4.6415888336127784e-05, cost : 88.142 min
==========
At iteration 388, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  389 ; minimum lost =  0.002572461264207959 ; diff loss =  4.302710294723511e-07 ; diff weight =  0.02987210638821125
lambda is : 1.4677992676220687e-05, cost : 90.44 min
==========
At iteration 382, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  383 ; minimum lost =  0.003046297701075673 ; diff loss =  2.477318048477173e-07 ; diff weight =  0.020190810784697533
lambda is : 2.1544346900318854e-05, cost : 90.613 min
==========
At iteration 393, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  394 ; minimum lost =  0.0057319458574056625 ; diff loss =  4.814937710762024e-07 ; diff weight =  0.004827150609344244
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 9.999999999999991e-05, cost : 90.926 min
==========
At iteration 398, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  399 ; minimum lost =  0.003537601325660944 ; diff loss =  5.783513188362122e-07 ; diff weight =  0.010885898023843765
lambda is : 3.16227766016838e-05, cost : 91.313 min
==========
At iteration 397, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  398 ; minimum lost =  0.0021816117223352194 ; diff loss =  5.855690687894821e-07 ; diff weight =  0.008771605789661407
lambda is : 9.999999999999997e-06, cost : 91.7 min
==========
*** Collecting results ***
Exporting result Dict
Leiden_18 Time elapsed: 91.78143930832545 minutes.
Shape: (71915, 25712)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for Leiden_19
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda:Lambda: Lambda: Lambda: Lambda:1e-05  Lambda:1.5e-05 Lambda:Lambda:Lambda:3.2e-05 Lambda:Lambda: Lambda:starting at 2.2e-05 Lambda: Lambda:Lambda:Lambda:starting at Lambda: 0.0001Lambda:  Lambda:Lambda:starting at Lambda:  Lambda:4.6e-05  Lambda:2024-11-23 13:37:47 Lambda:Lambda:starting at  6.8e-05    2024-11-23 13:37:47   starting at 0.000147 0.000215   2024-11-23 13:37:47  0.000316 0.021544  starting at 0.000464  Max_iter:   2024-11-23 13:37:47 0.000681 starting at 0.001468 0.001 0.002154 Max_iter: 0.003162  2024-11-23 13:37:470.004642 starting at starting at 0.006813 0.01 Max_iter: 0.014678 starting at starting at 0.031623 2024-11-23 13:37:47 starting at 0.046416 10000.068129 0.1 Max_iter: starting at 2024-11-23 13:37:47 starting at starting at starting at 1000starting at  Max_iter:starting at 2024-11-23 13:37:47 2024-11-23 13:37:47 starting at starting at 1000starting at 2024-11-23 13:37:47 2024-11-23 13:37:47 starting at Max_iter: 2024-11-23 13:37:47 starting at 
starting at starting at 10002024-11-23 13:37:47 Max_iter: 2024-11-23 13:37:47 2024-11-23 13:37:47 2024-11-23 13:37:47 
2024-11-23 13:37:47  10002024-11-23 13:37:47 Max_iter: Max_iter: 2024-11-23 13:37:47 2024-11-23 13:37:47 
2024-11-23 13:37:47 Max_iter: Max_iter: 2024-11-23 13:37:47 1000Max_iter: 2024-11-23 13:37:47 2024-11-23 13:37:472024-11-23 13:37:47 
Max_iter:1000
Max_iter:Max_iter:Max_iter:Max_iter:
Max_iter:10001000Max_iter:Max_iter:Max_iter:10001000Max_iter:
1000Max_iter:  Max_iter:Max_iter:  1000 1000 1000 1000 1000 1000

   

 
1000 1000





1000
10001000
1000

1000


At iteration 94, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  95 ; minimum lost =  0.10243554413318634 ; diff loss =  1.6391277313232422e-07 ; diff weight =  0.00017853469762485474
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.0681292069057962, cost : 23.503 min
==========
At iteration 107, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  108 ; minimum lost =  0.03894554451107979 ; diff loss =  9.685754776000977e-07 ; diff weight =  0.001326805679127574
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.010000000000000004, cost : 26.255 min
==========
At iteration 108, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  109 ; minimum lost =  0.05382021516561508 ; diff loss =  3.427267074584961e-07 ; diff weight =  0.0004376079305075109
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.02154434690031885, cost : 26.647 min
==========
At iteration 110, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  111 ; minimum lost =  0.08122102916240692 ; diff loss =  8.791685104370117e-07 ; diff weight =  0.00018113925762008876
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.04641588833612786, cost : 27.17 min
==========
At iteration 111, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  112 ; minimum lost =  0.04522257298231125 ; diff loss =  5.066394805908203e-07 ; diff weight =  0.0010436283191666007
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.014677992676220709, cost : 27.44 min
==========
At iteration 111, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  112 ; minimum lost =  0.0654563456773758 ; diff loss =  7.525086402893066e-07 ; diff weight =  0.00028893628041259944
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.0316227766016838, cost : 27.553 min
==========
At iteration 116, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  117 ; minimum lost =  0.13120457530021667 ; diff loss =  9.08970832824707e-07 ; diff weight =  0.0002486667945049703
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.10000000000000002, cost : 28.287 min
==========
At iteration 119, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  120 ; minimum lost =  0.03432084247469902 ; diff loss =  9.685754776000977e-07 ; diff weight =  0.0020268945954740047
lambda is : 0.006812920690579613, cost : 29.187 min
==========
At iteration 153, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  154 ; minimum lost =  0.030940014868974686 ; diff loss =  8.698552846908569e-07 ; diff weight =  0.001180031569674611
lambda is : 0.004641588833612781, cost : 37.187 min
==========
At iteration 177, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  178 ; minimum lost =  0.026927489787340164 ; diff loss =  8.605420589447021e-07 ; diff weight =  0.0016611268511041999
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.003162277660168382, cost : 42.346 min
==========
At iteration 188, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  189 ; minimum lost =  0.02259918302297592 ; diff loss =  9.834766387939453e-07 ; diff weight =  0.0027716979384422302
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0021544346900318843, cost : 46.229 min
==========
At iteration 231, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  232 ; minimum lost =  0.01844601519405842 ; diff loss =  9.629875421524048e-07 ; diff weight =  0.0027465831954032183
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0014677992676220694, cost : 56.028 min
==========
At iteration 240, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  241 ; minimum lost =  0.011571098119020462 ; diff loss =  9.443610906600952e-07 ; diff weight =  0.003585872706025839
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0006812920690579617, cost : 56.724 min
==========
At iteration 245, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  246 ; minimum lost =  0.014650060795247555 ; diff loss =  9.98377799987793e-07 ; diff weight =  0.003643790492787957
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0010000000000000002, cost : 59.44 min
==========
At iteration 260, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  261 ; minimum lost =  0.009135964326560497 ; diff loss =  9.75094735622406e-07 ; diff weight =  0.00344165344722569
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00046415888336127795, cost : 61.441 min
==========
At iteration 261, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  262 ; minimum lost =  0.007287250831723213 ; diff loss =  8.763745427131653e-07 ; diff weight =  0.004414692055433989
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00031622776601683783, cost : 62.205 min
==========
At iteration 259, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  260 ; minimum lost =  0.004779609851539135 ; diff loss =  8.330680429935455e-07 ; diff weight =  0.0041844830848276615
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00014677992676220703, cost : 62.856 min
==========
At iteration 272, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  273 ; minimum lost =  0.005846424959599972 ; diff loss =  8.44709575176239e-07 ; diff weight =  0.01264302246272564
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 64.192 min
==========
At iteration 339, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  340 ; minimum lost =  0.0027663633227348328 ; diff loss =  9.327195584774017e-07 ; diff weight =  0.009610965847969055
lambda is : 4.6415888336127784e-05, cost : 77.69 min
==========
At iteration 344, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  345 ; minimum lost =  0.003918494563549757 ; diff loss =  8.605420589447021e-07 ; diff weight =  0.005430719815194607
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 9.999999999999991e-05, cost : 80.435 min
==========
At iteration 353, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  354 ; minimum lost =  0.0032953224144876003 ; diff loss =  9.392388164997101e-07 ; diff weight =  0.005856836214661598
lambda is : 6.81292069057961e-05, cost : 82.046 min
==========
At iteration 362, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  363 ; minimum lost =  0.002251851838082075 ; diff loss =  9.727664291858673e-07 ; diff weight =  0.009380688890814781
lambda is : 3.16227766016838e-05, cost : 83.808 min
==========
At iteration 360, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  361 ; minimum lost =  0.0018630358390510082 ; diff loss =  8.44709575176239e-07 ; diff weight =  0.00900398101657629
lambda is : 2.1544346900318854e-05, cost : 83.979 min
==========
At iteration 415, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  416 ; minimum lost =  0.001414886792190373 ; diff loss =  9.784707799553871e-07 ; diff weight =  0.0089181549847126
lambda is : 1.4677992676220687e-05, cost : 93.76 min
==========
At iteration 425, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  426 ; minimum lost =  0.0011241676984354854 ; diff loss =  9.097857400774956e-07 ; diff weight =  0.003504733322188258
lambda is : 9.999999999999997e-06, cost : 96.088 min
==========
*** Collecting results ***
Exporting result Dict
Leiden_19 Time elapsed: 96.17052175203959 minutes.
Shape: (71915, 25712)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for Leiden_2
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda:Lambda: Lambda:Lambda: Lambda:1e-05   Lambda:1.5e-05 Lambda:  Lambda:Lambda:Lambda: starting at Lambda:2.2e-05 Lambda:3.2e-05 Lambda:Lambda: Lambda: starting at Lambda: Lambda:0.0001 Lambda:4.6e-05Lambda:  Lambda:Lambda:0.000215 Lambda:2024-11-23 15:14:06 Lambda: Lambda:Lambda:starting at  starting at   6.8e-05 0.001468 2024-11-23 15:14:06 0.002154  starting at   starting at 0.006813 0.000147   starting at  Max_iter:  0.000316   2024-11-23 15:14:060.0004642024-11-23 15:14:06 0.001 0.000681 starting at starting at Max_iter: starting at 0.003162 2024-11-23 15:14:06 0.004642  2024-11-23 15:14:060.01 starting at starting at 0.014678 0.0215442024-11-23 15:14:06 0.03162310000.046416 starting at 0.068129 0.1  Max_iter: starting atMax_iter: starting at starting at 2024-11-23 15:14:06 2024-11-23 15:14:06 10002024-11-23 15:14:06 starting at Max_iter: starting at  Max_iter:starting at 2024-11-23 15:14:06 2024-11-23 15:14:06 starting at  Max_iter:  starting at
starting at 2024-11-23 15:14:06starting at starting at  2024-11-23 15:14:0610002024-11-23 15:14:06 2024-11-23 15:14:06 Max_iter: Max_iter: 
Max_iter: 2024-11-23 15:14:06 10002024-11-23 15:14:06  10002024-11-23 15:14:06 Max_iter: Max_iter: 2024-11-23 15:14:06 starting at 1000 2024-11-23 15:14:062024-11-23 15:14:06  Max_iter:2024-11-23 15:14:06  1000 Max_iter:
Max_iter: Max_iter: 100010001000Max_iter: 
Max_iter:
Max_iter: 10001000Max_iter: 2024-11-23 15:14:06 
 Max_iter:Max_iter:  Max_iter: 2024-11-23 15:14:06
 100010001000


1000 1000

1000
Max_iter:  10001000
1000
1000 Max_iter:



1000
1000


 1000

At iteration 309, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  310 ; minimum lost =  0.35412877798080444 ; diff loss =  8.344650268554688e-07 ; diff weight =  0.0002584876783657819
lambda is : 0.10000000000000002, cost : 79.724 min
==========
At iteration 330, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  331 ; minimum lost =  0.20722290873527527 ; diff loss =  8.940696716308594e-07 ; diff weight =  0.00047935830662027
lambda is : 0.0316227766016838, cost : 84.657 min
==========
At iteration 336, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  337 ; minimum lost =  0.24861687421798706 ; diff loss =  9.238719940185547e-07 ; diff weight =  0.00047632490168325603
lambda is : 0.04641588833612786, cost : 85.966 min
==========
At iteration 364, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  365 ; minimum lost =  0.2977774441242218 ; diff loss =  9.834766387939453e-07 ; diff weight =  0.0005097149405628443
lambda is : 0.0681292069057962, cost : 93.269 min
==========
At iteration 432, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  433 ; minimum lost =  0.17353278398513794 ; diff loss =  8.642673492431641e-07 ; diff weight =  0.0009932279353961349
lambda is : 0.02154434690031885, cost : 108.636 min
==========
At iteration 471, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  472 ; minimum lost =  0.1051807776093483 ; diff loss =  9.909272193908691e-07 ; diff weight =  0.0011657150462269783
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.006812920690579613, cost : 117.315 min
==========
At iteration 490, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  491 ; minimum lost =  0.14584214985370636 ; diff loss =  9.98377799987793e-07 ; diff weight =  0.0012969945091754198
lambda is : 0.014677992676220709, cost : 121.62 min
==========
At iteration 496, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  497 ; minimum lost =  0.12334632873535156 ; diff loss =  9.760260581970215e-07 ; diff weight =  0.002220569411292672
lambda is : 0.010000000000000004, cost : 122.658 min
==========
At iteration 496, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  497 ; minimum lost =  0.09035885334014893 ; diff loss =  9.387731552124023e-07 ; diff weight =  0.0013771879021078348
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.004641588833612781, cost : 122.865 min
==========
At iteration 536, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  537 ; minimum lost =  0.027621181681752205 ; diff loss =  9.927898645401e-07 ; diff weight =  0.0002915930235758424
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 3.16227766016838e-05, cost : 132.351 min
==========
At iteration 553, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  554 ; minimum lost =  0.06860814243555069 ; diff loss =  9.08970832824707e-07 ; diff weight =  0.0013858736492693424
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0021544346900318843, cost : 134.752 min
==========
At iteration 557, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  558 ; minimum lost =  0.036364637315273285 ; diff loss =  5.774199962615967e-07 ; diff weight =  0.00033634764258749783
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00014677992676220703, cost : 135.857 min
==========
At iteration 552, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  553 ; minimum lost =  0.07830773293972015 ; diff loss =  9.611248970031738e-07 ; diff weight =  0.001451217569410801
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.003162277660168382, cost : 136.086 min
==========
At iteration 568, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  569 ; minimum lost =  0.023070212453603745 ; diff loss =  9.760260581970215e-07 ; diff weight =  0.0006153823924250901
lambda is : 1.4677992676220687e-05, cost : 138.392 min
==========
At iteration 574, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  575 ; minimum lost =  0.04562972113490105 ; diff loss =  5.289912223815918e-07 ; diff weight =  0.0030081761069595814
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00046415888336127795, cost : 138.96 min
==========
At iteration 582, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  583 ; minimum lost =  0.06083200126886368 ; diff loss =  9.387731552124023e-07 ; diff weight =  0.001631642342545092
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0014677992676220694, cost : 141.175 min
==========
At iteration 583, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  584 ; minimum lost =  0.03891816735267639 ; diff loss =  3.427267074584961e-07 ; diff weight =  0.0005008718580938876
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 141.951 min
==========
At iteration 591, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  592 ; minimum lost =  0.03361854702234268 ; diff loss =  4.1350722312927246e-07 ; diff weight =  0.0002124731836374849
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 9.999999999999991e-05, cost : 142.374 min
==========
At iteration 591, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  592 ; minimum lost =  0.042022887617349625 ; diff loss =  9.126961231231689e-07 ; diff weight =  0.0002075634547509253
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00031622776601683783, cost : 143.221 min
==========
At iteration 588, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  589 ; minimum lost =  0.029101189225912094 ; diff loss =  8.866190910339355e-07 ; diff weight =  0.00048632617108523846
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 4.6415888336127784e-05, cost : 144.29 min
==========
At iteration 601, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  602 ; minimum lost =  0.05467365309596062 ; diff loss =  9.350478649139404e-07 ; diff weight =  0.001970897661522031
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0010000000000000002, cost : 145.152 min
==========
At iteration 595, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  596 ; minimum lost =  0.024494746699929237 ; diff loss =  9.462237358093262e-07 ; diff weight =  0.0004215282096993178
lambda is : 2.1544346900318854e-05, cost : 145.595 min
==========
At iteration 610, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  611 ; minimum lost =  0.049683816730976105 ; diff loss =  8.568167686462402e-07 ; diff weight =  0.0013401232426986098
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0006812920690579617, cost : 145.999 min
==========
At iteration 617, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  618 ; minimum lost =  0.031046368181705475 ; diff loss =  9.406358003616333e-07 ; diff weight =  0.004700458142906427
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 6.81292069057961e-05, cost : 149.286 min
==========
At iteration 684, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  685 ; minimum lost =  0.01811135560274124 ; diff loss =  5.997717380523682e-07 ; diff weight =  0.0122885936871171
lambda is : 9.999999999999997e-06, cost : 161.998 min
==========
*** Collecting results ***
Exporting result Dict
Leiden_2 Time elapsed: 162.11701054573058 minutes.
Shape: (71915, 25712)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for Leiden_20
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda:Lambda:Lambda: Lambda:Lambda: Lambda: Lambda:Lambda:1e-05 Lambda: Lambda:  Lambda:Lambda:2.2e-05 Lambda:Lambda: Lambda:Lambda:1.5e-05  Lambda: Lambda:Lambda:starting at Lambda: Lambda:Lambda:0.0001 Lambda: Lambda:0.000215 Lambda:3.2e-05   starting at   4.6e-05   starting at 6.8e-05  0.002154   2024-11-23 17:56:20  0.000147  starting at 0.031623  starting at  starting at 0.1 0.000316 2024-11-23 17:56:20 0.0004640.000681 starting at 0.0010.001468 2024-11-23 17:56:20 starting at 0.003162 starting at 0.0046420.006813Max_iter: 0.01 starting at0.014678 0.021544 2024-11-23 17:56:20 starting at 0.0464162024-11-23 17:56:20 0.068129 2024-11-23 17:56:20 starting at starting at Max_iter:  starting atstarting at 2024-11-23 17:56:20  starting at Max_iter: 2024-11-23 17:56:20 starting at 2024-11-23 17:56:20  starting at starting at1000  2024-11-23 17:56:20starting at starting at Max_iter: 2024-11-23 17:56:20  Max_iter: starting at Max_iter: 2024-11-23 17:56:20 2024-11-23 17:56:20 1000 2024-11-23 17:56:202024-11-23 17:56:20 Max_iter: starting at2024-11-23 17:56:20 1000Max_iter: 2024-11-23 17:56:20 Max_iter:  2024-11-23 17:56:20 2024-11-23 17:56:20
starting at  Max_iter:2024-11-23 17:56:20 2024-11-23 17:56:20 1000Max_iter: starting at 10002024-11-23 17:56:20 1000Max_iter: Max_iter: 
 Max_iter:Max_iter: 1000 Max_iter: 
1000Max_iter: 1000 Max_iter: Max_iter:2024-11-23 17:56:20  Max_iter: Max_iter: 
10002024-11-23 17:56:20
Max_iter:
10001000
 10001000
2024-11-23 17:56:20 1000

1000

  Max_iter:100010001000
  


Max_iter:10001000 1000


Max_iter: 1000
 1000


1000

At iteration 2, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  3 ; minimum lost =  0.020343968644738197 ; diff loss =  5.755573511123657e-07 ; diff weight =  0.0004068854032084346
At iteration 2, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  3 ; minimum lost =  0.020604431629180908 ; diff loss =  8.437782526016235e-07 ; diff weight =  0.0006967721856199205
At iteration 2, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  3 ; minimum lost =  0.020452100783586502 ; diff loss =  6.724148988723755e-07 ; diff weight =  0.000514930987264961
lambda is : 9.999999999999997e-06, cost : 2.487 min
==========
lambda is : 1.4677992676220687e-05, cost : 2.569 min
==========
lambda is : 2.1544346900318854e-05, cost : 2.599 min
==========
At iteration 96, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  97 ; minimum lost =  0.022418329492211342 ; diff loss =  5.997717380523682e-07 ; diff weight =  0.002639403101056814
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.003162277660168382, cost : 23.612 min
==========
At iteration 103, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  104 ; minimum lost =  0.020760012790560722 ; diff loss =  6.258487701416016e-07 ; diff weight =  0.0033787619322538376
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.0021544346900318843, cost : 25.84 min
==========
At iteration 117, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  118 ; minimum lost =  0.024788420647382736 ; diff loss =  8.121132850646973e-07 ; diff weight =  0.0023497017100453377
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.004641588833612781, cost : 28.526 min
==========
At iteration 120, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  121 ; minimum lost =  0.019496364519000053 ; diff loss =  9.760260581970215e-07 ; diff weight =  0.0029654293321073055
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0014677992676220694, cost : 29.038 min
==========
At iteration 124, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  125 ; minimum lost =  0.04781398922204971 ; diff loss =  9.126961231231689e-07 ; diff weight =  0.0012258701026439667
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.02154434690031885, cost : 29.746 min
==========
At iteration 140, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  141 ; minimum lost =  0.018398160114884377 ; diff loss =  9.890645742416382e-07 ; diff weight =  0.0028798801358789206
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0010000000000000002, cost : 33.41 min
==========
At iteration 147, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  148 ; minimum lost =  0.028119318187236786 ; diff loss =  9.08970832824707e-07 ; diff weight =  0.0037938738241791725
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.006812920690579613, cost : 35.143 min
==========
At iteration 153, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  154 ; minimum lost =  0.03903046250343323 ; diff loss =  9.685754776000977e-07 ; diff weight =  0.0020316042937338352
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.014677992676220709, cost : 36.613 min
==========
At iteration 156, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  157 ; minimum lost =  0.03265976160764694 ; diff loss =  7.636845111846924e-07 ; diff weight =  0.002412827219814062
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.010000000000000004, cost : 38.09 min
==========
At iteration 169, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  170 ; minimum lost =  0.12542825937271118 ; diff loss =  9.5367431640625e-07 ; diff weight =  0.00037967186653986573
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.10000000000000002, cost : 40.452 min
==========
At iteration 173, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  174 ; minimum lost =  0.05967060476541519 ; diff loss =  9.313225746154785e-07 ; diff weight =  0.0007710158242844045
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.0316227766016838, cost : 40.573 min
==========
At iteration 178, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  179 ; minimum lost =  0.0752754658460617 ; diff loss =  6.92903995513916e-07 ; diff weight =  0.0005443385452963412
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.04641588833612786, cost : 41.786 min
==========
At iteration 203, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  204 ; minimum lost =  0.09677571058273315 ; diff loss =  9.685754776000977e-07 ; diff weight =  0.00035369396209716797
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.0681292069057962, cost : 46.397 min
==========
At iteration 275, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  276 ; minimum lost =  0.017202941700816154 ; diff loss =  9.96515154838562e-07 ; diff weight =  0.0036391792818903923
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0006812920690579617, cost : 62.737 min
==========
At iteration 279, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  280 ; minimum lost =  0.0100904181599617 ; diff loss =  7.674098014831543e-07 ; diff weight =  0.011906293220818043
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 6.81292069057961e-05, cost : 63.52 min
==========
At iteration 282, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  283 ; minimum lost =  0.008864659816026688 ; diff loss =  5.150213837623596e-07 ; diff weight =  0.0008848090074025095
lambda is : 4.6415888336127784e-05, cost : 65.014 min
==========
At iteration 294, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  295 ; minimum lost =  0.016026537865400314 ; diff loss =  9.611248970031738e-07 ; diff weight =  0.004522237461060286
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00046415888336127795, cost : 66.454 min
==========
At iteration 296, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  297 ; minimum lost =  0.014763323590159416 ; diff loss =  9.052455425262451e-07 ; diff weight =  0.004703198559582233
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00031622776601683783, cost : 67.038 min
==========
At iteration 301, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  302 ; minimum lost =  0.00769985094666481 ; diff loss =  4.0885061025619507e-07 ; diff weight =  0.0014339799527078867
lambda is : 3.16227766016838e-05, cost : 67.914 min
==========
At iteration 306, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  307 ; minimum lost =  0.012383841909468174 ; diff loss =  9.257346391677856e-07 ; diff weight =  0.00637925136834383
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00014677992676220703, cost : 68.987 min
==========
At iteration 309, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  310 ; minimum lost =  0.013534541241824627 ; diff loss =  8.121132850646973e-07 ; diff weight =  0.0048408410511910915
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 69.288 min
==========
At iteration 316, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  317 ; minimum lost =  0.011095043271780014 ; diff loss =  9.033828973770142e-07 ; diff weight =  0.007553375791758299
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 9.999999999999991e-05, cost : 70.885 min
==========
*** Collecting results ***
Exporting result Dict
Leiden_20 Time elapsed: 70.9652463833491 minutes.
Shape: (71915, 25712)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for Leiden_21
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda:Lambda:Lambda:Lambda: Lambda:Lambda: Lambda: Lambda:Lambda: Lambda:Lambda:1e-05 Lambda: Lambda:  Lambda:1.5e-05 Lambda: Lambda:2.2e-05 Lambda:  Lambda:6.8e-05 Lambda: Lambda: Lambda:Lambda:starting at  Lambda:3.2e-05 Lambda:Lambda:0.000464 0.000316  starting at  4.6e-05  starting at  0.00010.003162  starting at  0.006813  0.000147   2024-11-23 19:07:25 0.000215  starting at   starting at starting at 0.000681 2024-11-23 19:07:25 0.001 starting at 0.001468 2024-11-23 19:07:25 0.002154  starting atstarting at 0.004642 2024-11-23 19:07:25 0.01 starting at 0.014678 starting at 0.021544 0.031623 Max_iter: starting at 0.046416 2024-11-23 19:07:25 0.1 0.068129 2024-11-23 19:07:25 2024-11-23 19:07:25 starting at Max_iter: starting at 2024-11-23 19:07:25 starting at Max_iter: starting at  2024-11-23 19:07:252024-11-23 19:07:25 Max_iter: starting at Max_iter: starting at 2024-11-23 19:07:25 starting at 2024-11-23 19:07:25 starting at starting at 10002024-11-23 19:07:25 starting at Max_iter: starting at starting at Max_iter: Max_iter: 2024-11-23 19:07:25 10002024-11-23 19:07:25 Max_iter: 2024-11-23 19:07:25 10002024-11-23 19:07:25  Max_iter:10002024-11-23 19:07:25 10002024-11-23 19:07:25 Max_iter: 2024-11-23 19:07:25 Max_iter: 2024-11-23 19:07:25 2024-11-23 19:07:25 
Max_iter: 2024-11-23 19:07:25 10002024-11-23 19:07:25 2024-11-23 19:07:25 10001000Max_iter: 
Max_iter: 1000Max_iter: 
Max_iter:  1000
Max_iter: 
Max_iter:1000Max_iter: 1000
Max_iter:Max_iter: 1000Max_iter: 
Max_iter: Max_iter: 

10001000
1000
1000
1000 
1000 1000

1000
10001000




1000
1000


At iteration 82, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  83 ; minimum lost =  0.017002813518047333 ; diff loss =  9.760260581970215e-07 ; diff weight =  0.002768994076177478
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.003162277660168382, cost : 20.308 min
==========
At iteration 85, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  86 ; minimum lost =  0.02304806187748909 ; diff loss =  7.711350917816162e-07 ; diff weight =  0.0015769910532981157
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.006812920690579613, cost : 20.837 min
==========
At iteration 89, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  90 ; minimum lost =  0.015111235901713371 ; diff loss =  6.416812539100647e-07 ; diff weight =  0.002261644694954157
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.0021544346900318843, cost : 21.793 min
==========
At iteration 95, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  96 ; minimum lost =  0.02784036099910736 ; diff loss =  6.537884473800659e-07 ; diff weight =  0.0003536485892254859
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.010000000000000004, cost : 23.381 min
==========
At iteration 109, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  110 ; minimum lost =  0.034312620759010315 ; diff loss =  5.103647708892822e-07 ; diff weight =  0.0013978021452203393
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.014677992676220709, cost : 25.981 min
==========
At iteration 118, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  119 ; minimum lost =  0.0195759404450655 ; diff loss =  8.959323167800903e-07 ; diff weight =  0.0017520791152492166
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.004641588833612781, cost : 28.548 min
==========
At iteration 130, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  131 ; minimum lost =  0.12231548130512238 ; diff loss =  2.60770320892334e-07 ; diff weight =  0.00021590504911728203
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.10000000000000002, cost : 31.013 min
==========
At iteration 129, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  130 ; minimum lost =  0.013697557151317596 ; diff loss =  9.71369445323944e-07 ; diff weight =  0.003096190979704261
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0014677992676220694, cost : 31.074 min
==========
At iteration 130, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  131 ; minimum lost =  0.09322865307331085 ; diff loss =  1.1920928955078125e-07 ; diff weight =  0.00013237516395747662
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.0681292069057962, cost : 31.353 min
==========
At iteration 146, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  147 ; minimum lost =  0.05521879717707634 ; diff loss =  9.052455425262451e-07 ; diff weight =  0.0007732956437394023
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.0316227766016838, cost : 34.511 min
==========
At iteration 145, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  146 ; minimum lost =  0.04321173205971718 ; diff loss =  5.476176738739014e-07 ; diff weight =  0.00072975002694875
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.02154434690031885, cost : 34.806 min
==========
At iteration 159, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  160 ; minimum lost =  0.07137711346149445 ; diff loss =  9.015202522277832e-07 ; diff weight =  0.0005110502243041992
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.04641588833612786, cost : 37.863 min
==========
At iteration 181, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  182 ; minimum lost =  0.011056472547352314 ; diff loss =  9.844079613685608e-07 ; diff weight =  0.0035617274697870016
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0006812920690579617, cost : 42.302 min
==========
At iteration 197, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  198 ; minimum lost =  0.012356705032289028 ; diff loss =  9.974464774131775e-07 ; diff weight =  0.002434259746223688
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0010000000000000002, cost : 45.92 min
==========
At iteration 239, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  240 ; minimum lost =  0.009779506362974644 ; diff loss =  9.387731552124023e-07 ; diff weight =  0.004805944859981537
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00046415888336127795, cost : 56.562 min
==========
At iteration 293, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  294 ; minimum lost =  0.008561672642827034 ; diff loss =  9.350478649139404e-07 ; diff weight =  0.006234182510524988
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00031622776601683783, cost : 68.52 min
==========
At iteration 322, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  323 ; minimum lost =  0.006425013765692711 ; diff loss =  7.664784789085388e-07 ; diff weight =  0.007999474182724953
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00014677992676220703, cost : 73.399 min
==========
At iteration 325, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  326 ; minimum lost =  0.00742663349956274 ; diff loss =  9.909272193908691e-07 ; diff weight =  0.006484192796051502
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 74.665 min
==========
At iteration 329, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  330 ; minimum lost =  0.002238854067400098 ; diff loss =  9.995419532060623e-07 ; diff weight =  0.008222980424761772
lambda is : 9.999999999999997e-06, cost : 75.369 min
==========
At iteration 335, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  336 ; minimum lost =  0.002896005753427744 ; diff loss =  4.0582381188869476e-07 ; diff weight =  0.0011002381797879934
lambda is : 2.1544346900318854e-05, cost : 77.51 min
==========
At iteration 352, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  353 ; minimum lost =  0.005498042330145836 ; diff loss =  8.670613169670105e-07 ; diff weight =  0.008564048446714878
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 9.999999999999991e-05, cost : 80.713 min
==========
At iteration 361, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  362 ; minimum lost =  0.0022417977452278137 ; diff loss =  5.029141902923584e-07 ; diff weight =  0.0025085110682994127
lambda is : 1.4677992676220687e-05, cost : 82.581 min
==========
At iteration 364, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  365 ; minimum lost =  0.0032890187576413155 ; diff loss =  4.409812390804291e-07 ; diff weight =  0.011817642487585545
lambda is : 3.16227766016838e-05, cost : 83.838 min
==========
At iteration 369, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  370 ; minimum lost =  0.003919692710042 ; diff loss =  9.685754776000977e-07 ; diff weight =  0.01023263018578291
lambda is : 4.6415888336127784e-05, cost : 84.06 min
==========
At iteration 379, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  380 ; minimum lost =  0.004643661435693502 ; diff loss =  6.444752216339111e-07 ; diff weight =  0.009895899333059788
lambda is : 6.81292069057961e-05, cost : 84.312 min
==========
*** Collecting results ***
Exporting result Dict
Leiden_21 Time elapsed: 84.38630105654399 minutes.
Shape: (71915, 25712)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for Leiden_22
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda:Lambda:Lambda: Lambda:Lambda: Lambda:Lambda: Lambda:Lambda: Lambda:1e-05 Lambda: Lambda: Lambda:1.5e-05 Lambda:  Lambda:2.2e-05 Lambda: Lambda:Lambda:0.0001 Lambda: Lambda:Lambda:starting at Lambda: Lambda:Lambda:3.2e-05 Lambda: 4.6e-05  starting at  0.000681 6.8e-05  starting at  0.002154   starting at  0.000147   2024-11-23 20:31:55  0.000215   starting at  0.000316 starting at 0.000464 2024-11-23 20:31:55 0.001 starting at starting at 0.001468 2024-11-23 20:31:55 0.003162 starting at 0.004642 0.006813 2024-11-23 20:31:55 0.01 starting at 0.014678 0.021544 Max_iter: 0.031623 starting at 0.046416 0.068129 2024-11-23 20:31:55 0.1 starting at 2024-11-23 20:31:55 starting at Max_iter: starting at 2024-11-23 20:31:55 2024-11-23 20:31:55 starting at Max_iter: starting at 2024-11-23 20:31:55 starting at starting at Max_iter: starting at 2024-11-23 20:31:55 starting at starting at 1000starting at 2024-11-23 20:31:55 starting at starting at Max_iter: starting at 2024-11-23 20:31:55 Max_iter: 2024-11-23 20:31:55 10002024-11-23 20:31:55 Max_iter: Max_iter: 2024-11-23 20:31:55 1000
2024-11-23 20:31:55 Max_iter: 2024-11-23 20:31:55 2024-11-23 20:31:55 10002024-11-23 20:31:55 Max_iter: 2024-11-23 20:31:55 2024-11-23 20:31:55 
2024-11-23 20:31:55 Max_iter: 2024-11-23 20:31:55 2024-11-23 20:31:55 10002024-11-23 20:31:55 Max_iter: 1000Max_iter: 
Max_iter: 10001000Max_iter:Max_iter:1000Max_iter:Max_iter:
Max_iter:1000Max_iter: Max_iter: Max_iter:1000Max_iter:Max_iter:
Max_iter: 1000
10001000


 1000 
   
10001000 
  1000


1000100010001000

1000
1000
1000





At iteration 63, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  64 ; minimum lost =  0.11730443686246872 ; diff loss =  9.685754776000977e-08 ; diff weight =  0.00010770836524898186
lambda is : 0.10000000000000002, cost : 15.662 min
==========
At iteration 79, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  80 ; minimum lost =  0.00894778873771429 ; diff loss =  3.259629011154175e-08 ; diff weight =  0.0005362578667700291
lambda is : 0.003162277660168382, cost : 19.836 min
==========
At iteration 81, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  82 ; minimum lost =  0.01167978160083294 ; diff loss =  9.778887033462524e-07 ; diff weight =  0.0029364689253270626
lambda is : 0.004641588833612781, cost : 20.542 min
==========
At iteration 93, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  94 ; minimum lost =  0.04865396395325661 ; diff loss =  8.158385753631592e-07 ; diff weight =  0.0007295051473192871
lambda is : 0.0316227766016838, cost : 22.973 min
==========
At iteration 100, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  101 ; minimum lost =  0.08768638968467712 ; diff loss =  7.450580596923828e-09 ; diff weight =  2.6180843633483164e-05
lambda is : 0.0681292069057962, cost : 24.66 min
==========
At iteration 106, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  107 ; minimum lost =  0.005518955644220114 ; diff loss =  6.551854312419891e-07 ; diff weight =  0.0034077444579452276
At iteration 105, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  106 ; minimum lost =  0.015283865854144096 ; diff loss =  5.867332220077515e-07 ; diff weight =  0.0021580855827778578
lambda is : 0.0014677992676220694, cost : 26.584 min
==========
lambda is : 0.006812920690579613, cost : 26.59 min
==========
At iteration 117, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  118 ; minimum lost =  0.006940603256225586 ; diff loss =  6.649643182754517e-07 ; diff weight =  0.0032286224886775017
lambda is : 0.0021544346900318843, cost : 29.934 min
==========
At iteration 128, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  129 ; minimum lost =  0.027116410434246063 ; diff loss =  4.991888999938965e-07 ; diff weight =  0.000228046890697442
lambda is : 0.014677992676220709, cost : 31.647 min
==========
At iteration 136, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  137 ; minimum lost =  0.020194001495838165 ; diff loss =  4.5821070671081543e-07 ; diff weight =  0.0005224900669418275
lambda is : 0.010000000000000004, cost : 32.543 min
==========
At iteration 139, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  140 ; minimum lost =  0.004420221317559481 ; diff loss =  9.746290743350983e-07 ; diff weight =  0.005963949486613274
lambda is : 0.0010000000000000002, cost : 33.724 min
==========
At iteration 148, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  149 ; minimum lost =  0.06530522555112839 ; diff loss =  9.685754776000977e-07 ; diff weight =  0.00042557716369628906
lambda is : 0.04641588833612786, cost : 35.852 min
==========
At iteration 170, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  171 ; minimum lost =  0.03623134270310402 ; diff loss =  9.313225746154785e-07 ; diff weight =  0.0009686021949164569
lambda is : 0.02154434690031885, cost : 41.533 min
==========
At iteration 197, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  198 ; minimum lost =  0.003672332037240267 ; diff loss =  9.923242032527924e-07 ; diff weight =  0.005738712381571531
lambda is : 0.0006812920690579617, cost : 46.052 min
==========
At iteration 199, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  200 ; minimum lost =  0.0031212414614856243 ; diff loss =  9.527429938316345e-07 ; diff weight =  0.0073984418995678425
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00046415888336127795, cost : 47.579 min
==========
At iteration 252, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  253 ; minimum lost =  0.0025107944384217262 ; diff loss =  9.57399606704712e-07 ; diff weight =  0.0008192298701032996
lambda is : 0.00031622776601683783, cost : 59.33 min
==========
At iteration 272, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  273 ; minimum lost =  0.00203497800976038 ; diff loss =  8.78702849149704e-07 ; diff weight =  0.008016269654035568
lambda is : 0.0002154434690031884, cost : 64.879 min
==========
At iteration 307, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  308 ; minimum lost =  0.0016100932843983173 ; diff loss =  8.438946679234505e-07 ; diff weight =  0.010723013430833817
lambda is : 0.00014677992676220703, cost : 72.772 min
==========
At iteration 349, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  350 ; minimum lost =  0.0012527392245829105 ; diff loss =  9.902287274599075e-07 ; diff weight =  0.012653369456529617
lambda is : 9.999999999999991e-05, cost : 80.589 min
==========
At iteration 359, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  360 ; minimum lost =  0.000996004557237029 ; diff loss =  7.012858986854553e-07 ; diff weight =  0.007697691675275564
lambda is : 6.81292069057961e-05, cost : 83.526 min
==========
At iteration 383, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  384 ; minimum lost =  0.0007706554024480283 ; diff loss =  9.840005077421665e-07 ; diff weight =  0.015308124013245106
lambda is : 4.6415888336127784e-05, cost : 86.651 min
==========
At iteration 407, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  408 ; minimum lost =  0.0005849698791280389 ; diff loss =  7.952912710607052e-07 ; diff weight =  0.019486209377646446
lambda is : 3.16227766016838e-05, cost : 91.817 min
==========
At iteration 422, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  423 ; minimum lost =  0.0002996748371515423 ; diff loss =  7.908383850008249e-07 ; diff weight =  0.01490633375942707
lambda is : 9.999999999999997e-06, cost : 93.94 min
==========
At iteration 427, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  428 ; minimum lost =  0.0004428877145983279 ; diff loss =  8.351053111255169e-07 ; diff weight =  0.026305636391043663
lambda is : 2.1544346900318854e-05, cost : 96.732 min
==========
At iteration 437, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  438 ; minimum lost =  0.00034306870657019317 ; diff loss =  9.137438610196114e-07 ; diff weight =  0.019194345921278
lambda is : 1.4677992676220687e-05, cost : 97.199 min
==========
*** Collecting results ***
Exporting result Dict
Leiden_22 Time elapsed: 97.29725101391475 minutes.
Shape: (71915, 25712)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for Leiden_23
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda:Lambda: Lambda:Lambda:Lambda: Lambda:Lambda:1e-05  Lambda:Lambda: Lambda: Lambda:1.5e-05 Lambda:Lambda: Lambda: Lambda: Lambda:starting at 2.2e-05  Lambda: Lambda:Lambda:0.0001 Lambda: 3.2e-05 Lambda:  Lambda:Lambda:starting at Lambda:Lambda:   4.6e-05  6.8e-05 0.001  2024-11-23 22:09:19 starting at 0.002154  0.000147   starting at  0.000215 starting at 0.014678 0.000316   2024-11-23 22:09:19  0.068129 0.0004640.1 starting at 0.000681 starting atstarting at 0.001468Max_iter: 2024-11-23 22:09:19 starting at 0.003162 starting at 0.004642 0.0068132024-11-23 22:09:19 0.01 starting at 2024-11-23 22:09:19 starting atstarting at0.021544 0.031623 Max_iter: 0.046416starting at starting at2024-11-23 22:09:19 starting at  2024-11-23 22:09:192024-11-23 22:09:19  starting at1000Max_iter: 2024-11-23 22:09:19 starting at 2024-11-23 22:09:19 starting at  Max_iter: starting at2024-11-23 22:09:19Max_iter:  2024-11-23 22:09:19 starting atstarting at1000  2024-11-23 22:09:19starting at 2024-11-23 22:09:19Max_iter: 2024-11-23 22:09:19  Max_iter:Max_iter:  
1000Max_iter: 2024-11-23 22:09:19 Max_iter:2024-11-23 22:09:19 starting at 1000 2024-11-23 22:09:19 Max_iter:1000 Max_iter:2024-11-23 22:09:19 2024-11-23 22:09:19 2024-11-23 22:09:19
starting at   Max_iter:1000Max_iter: 10002024-11-23 22:09:19
1000Max_iter: Max_iter:2024-11-23 22:09:19
  
   Max_iter:  Max_iter: 2024-11-23 22:09:19 
 10001000
 Max_iter:
 10001000
  Max_iter:Max_iter: 1000
1000
Max_iter:  1000Max_iter: 2024-11-23 22:09:19 1000 1000

 
1000
 100010001000

1000
Max_iter:
Max_iter: 
1000


 10001000

At iteration 91, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  92 ; minimum lost =  0.05139390751719475 ; diff loss =  8.67992639541626e-07 ; diff weight =  0.0010139555670320988
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.0316227766016838, cost : 22.45 min
==========
At iteration 103, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  104 ; minimum lost =  0.12676623463630676 ; diff loss =  8.791685104370117e-07 ; diff weight =  0.00041412501013837755
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.10000000000000002, cost : 25.352 min
==========
At iteration 106, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  107 ; minimum lost =  0.010949961841106415 ; diff loss =  9.248033165931702e-07 ; diff weight =  0.0003782425192184746
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.004641588833612781, cost : 26.01 min
==========
At iteration 107, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  108 ; minimum lost =  0.06968922168016434 ; diff loss =  8.121132850646973e-07 ; diff weight =  0.0004794377600774169
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.04641588833612786, cost : 26.175 min
==========
At iteration 107, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  108 ; minimum lost =  0.037627916783094406 ; diff loss =  5.21540641784668e-08 ; diff weight =  0.00021237533655948937
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.02154434690031885, cost : 26.57 min
==========
At iteration 117, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  118 ; minimum lost =  0.027562854811549187 ; diff loss =  4.284083843231201e-08 ; diff weight =  2.3351576601271518e-05
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.014677992676220709, cost : 28.564 min
==========
At iteration 126, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  127 ; minimum lost =  0.01479153148829937 ; diff loss =  8.21426510810852e-07 ; diff weight =  0.002409406006336212
lambda is : 0.006812920690579613, cost : 30.58 min
==========
At iteration 126, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  127 ; minimum lost =  0.09426625072956085 ; diff loss =  1.043081283569336e-07 ; diff weight =  0.00014692841796204448
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.0681292069057962, cost : 31.008 min
==========
At iteration 127, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  128 ; minimum lost =  0.008083251304924488 ; diff loss =  8.512288331985474e-07 ; diff weight =  0.003570144297555089
lambda is : 0.003162277660168382, cost : 31.334 min
==========
At iteration 131, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  132 ; minimum lost =  0.020186787471175194 ; diff loss =  4.805624485015869e-07 ; diff weight =  0.001136167673394084
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
lambda is : 0.010000000000000004, cost : 32.23 min
==========
At iteration 136, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  137 ; minimum lost =  0.006010827142745256 ; diff loss =  7.89295881986618e-07 ; diff weight =  0.0022859605960547924
lambda is : 0.0021544346900318843, cost : 33.234 min
==========
At iteration 180, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  181 ; minimum lost =  0.004536392167210579 ; diff loss =  7.832422852516174e-07 ; diff weight =  0.008129168301820755
lambda is : 0.0014677992676220694, cost : 44.348 min
==========
At iteration 196, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  197 ; minimum lost =  0.0034562982618808746 ; diff loss =  6.570480763912201e-07 ; diff weight =  0.0032636888790875673
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0010000000000000002, cost : 47.431 min
==========
At iteration 210, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  211 ; minimum lost =  0.0026936563663184643 ; diff loss =  8.032657206058502e-07 ; diff weight =  0.004186385776847601
lambda is : 0.0006812920690579617, cost : 49.161 min
==========
At iteration 229, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  230 ; minimum lost =  0.002129361731931567 ; diff loss =  9.159557521343231e-07 ; diff weight =  0.004689989611506462
lambda is : 0.00046415888336127795, cost : 55.351 min
==========
At iteration 254, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  255 ; minimum lost =  0.0017186549957841635 ; diff loss =  6.685731932520866e-07 ; diff weight =  0.003637376707047224
lambda is : 0.00031622776601683783, cost : 59.423 min
==========
At iteration 277, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  278 ; minimum lost =  0.0014104772126302123 ; diff loss =  6.094342097640038e-07 ; diff weight =  0.005593500565737486
lambda is : 0.0002154434690031884, cost : 64.236 min
==========
At iteration 316, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  317 ; minimum lost =  0.0011385027319192886 ; diff loss =  9.853392839431763e-07 ; diff weight =  0.009828545153141022
lambda is : 0.00014677992676220703, cost : 74.369 min
==========
At iteration 338, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  339 ; minimum lost =  0.0009092870750464499 ; diff loss =  8.803908713161945e-07 ; diff weight =  0.005879165604710579
lambda is : 9.999999999999991e-05, cost : 77.079 min
==========
At iteration 377, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  378 ; minimum lost =  0.0006932044052518904 ; diff loss =  6.995978765189648e-07 ; diff weight =  0.007348742801696062
lambda is : 6.81292069057961e-05, cost : 86.894 min
==========
At iteration 403, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  404 ; minimum lost =  0.0005240888567641377 ; diff loss =  9.273644536733627e-07 ; diff weight =  0.010788983665406704
lambda is : 4.6415888336127784e-05, cost : 92.375 min
==========
At iteration 416, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  417 ; minimum lost =  0.00040774198714643717 ; diff loss =  8.666538633406162e-07 ; diff weight =  0.012053785845637321
lambda is : 3.16227766016838e-05, cost : 93.964 min
==========
At iteration 441, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  442 ; minimum lost =  0.00025699398247525096 ; diff loss =  8.865608833730221e-07 ; diff weight =  0.012164855375885963
lambda is : 1.4677992676220687e-05, cost : 99.326 min
==========
At iteration 451, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  452 ; minimum lost =  0.00029179995181038976 ; diff loss =  9.70292603597045e-07 ; diff weight =  0.022567592561244965
lambda is : 2.1544346900318854e-05, cost : 101.169 min
==========
At iteration 453, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  454 ; minimum lost =  0.00020311048137955368 ; diff loss =  9.665818652138114e-07 ; diff weight =  0.012344135902822018
lambda is : 9.999999999999997e-06, cost : 102.116 min
==========
*** Collecting results ***
Exporting result Dict
Leiden_23 Time elapsed: 102.19044735034306 minutes.
Shape: (71915, 25712)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for Leiden_3
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda:Lambda:Lambda:Lambda: Lambda:  Lambda: Lambda:Lambda: Lambda:1e-05  Lambda: Lambda:1.5e-05 Lambda:2.2e-05 Lambda: Lambda:4.6e-05 Lambda: Lambda:Lambda:0.0001 Lambda: Lambda:Lambda:starting at Lambda:3.2e-05 Lambda:Lambda:Lambda:0.000215 Lambda: starting at  starting at  6.8e-05  starting at  0.001468   starting at  0.000147   2024-11-23 23:51:38  starting at    starting at  0.000316 2024-11-23 23:51:38 0.000464 2024-11-23 23:51:38 0.000681 starting at 0.001 2024-11-23 23:51:38 0.002154 starting at 0.003162 0.004642 2024-11-23 23:51:38 0.006813 starting at 0.01 0.014678 Max_iter: 0.021544 2024-11-23 23:51:38 0.031623 0.046416 0.068129 2024-11-23 23:51:38 0.1 starting at Max_iter: starting at Max_iter: starting at 2024-11-23 23:51:38 starting at Max_iter: starting at 2024-11-23 23:51:38 starting at starting at Max_iter: starting at 2024-11-23 23:51:38 starting at starting at 1000starting at Max_iter: starting at starting at starting at Max_iter: starting at 2024-11-23 23:51:38 10002024-11-23 23:51:38 10002024-11-23 23:51:38 Max_iter: 2024-11-23 23:51:38 10002024-11-23 23:51:38 Max_iter: 2024-11-23 23:51:38 2024-11-23 23:51:38 10002024-11-23 23:51:38 Max_iter: 2024-11-23 23:51:38 2024-11-23 23:51:38 
2024-11-23 23:51:38 10002024-11-23 23:51:38 2024-11-23 23:51:38 2024-11-23 23:51:38 10002024-11-23 23:51:38 Max_iter: 
Max_iter: 
Max_iter: 1000Max_iter:
Max_iter: 1000Max_iter: Max_iter: 
Max_iter: 1000Max_iter:Max_iter: Max_iter:
Max_iter: Max_iter: Max_iter: 
Max_iter: 1000
10001000

 1000

100010001000
 10001000 1000100010001000
1000




1000





At iteration 278, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  279 ; minimum lost =  0.34445515275001526 ; diff loss =  6.258487701416016e-07 ; diff weight =  0.00024202153144869953
lambda is : 0.10000000000000002, cost : 67.344 min
==========
At iteration 314, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  315 ; minimum lost =  0.3085681200027466 ; diff loss =  6.854534149169922e-07 ; diff weight =  0.0004766444908455014
lambda is : 0.0681292069057962, cost : 75.154 min
==========
At iteration 389, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  390 ; minimum lost =  0.2711678445339203 ; diff loss =  9.5367431640625e-07 ; diff weight =  0.0006329922471195459
lambda is : 0.04641588833612786, cost : 93.573 min
==========
At iteration 398, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  399 ; minimum lost =  0.17023292183876038 ; diff loss =  9.238719940185547e-07 ; diff weight =  0.0006794897490181029
lambda is : 0.014677992676220709, cost : 95.563 min
==========
At iteration 433, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  434 ; minimum lost =  0.20073725283145905 ; diff loss =  3.725290298461914e-07 ; diff weight =  0.0004402690101414919
lambda is : 0.02154434690031885, cost : 105.005 min
==========
At iteration 436, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  437 ; minimum lost =  0.11986907571554184 ; diff loss =  8.642673492431641e-07 ; diff weight =  0.0007875942392274737
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.006812920690579613, cost : 105.242 min
==========
At iteration 454, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  455 ; minimum lost =  0.23483127355575562 ; diff loss =  8.493661880493164e-07 ; diff weight =  0.000570625823456794
lambda is : 0.0316227766016838, cost : 109.484 min
==========
At iteration 461, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  462 ; minimum lost =  0.04890032857656479 ; diff loss =  5.960464477539062e-07 ; diff weight =  0.0009347480372525752
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0006812920690579617, cost : 110.825 min
==========
At iteration 488, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  489 ; minimum lost =  0.1008436530828476 ; diff loss =  9.760260581970215e-07 ; diff weight =  0.0012118254089727998
lambda is : 0.004641588833612781, cost : 115.922 min
==========
At iteration 489, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  489 ; minimum lost =  0.08559535443782806 ; diff loss =  -2.384185791015625e-07 ; diff weight =  0.00737637747079134
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.003162277660168382, cost : 116.672 min
==========
At iteration 489, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  490 ; minimum lost =  0.035186510533094406 ; diff loss =  6.891787052154541e-07 ; diff weight =  0.0004167270381003618
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 117.471 min
==========
At iteration 486, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  487 ; minimum lost =  0.05540517717599869 ; diff loss =  7.636845111846924e-07 ; diff weight =  0.0023856365587562323
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0010000000000000002, cost : 117.655 min
==========
At iteration 503, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  504 ; minimum lost =  0.14285530149936676 ; diff loss =  8.195638656616211e-07 ; diff weight =  0.0010301006259396672
lambda is : 0.010000000000000004, cost : 120.983 min
==========
At iteration 515, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  516 ; minimum lost =  0.0265304297208786 ; diff loss =  7.022172212600708e-07 ; diff weight =  0.007430500350892544
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 6.81292069057961e-05, cost : 123.204 min
==========
At iteration 517, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  518 ; minimum lost =  0.0732949823141098 ; diff loss =  9.98377799987793e-07 ; diff weight =  0.0012299565132707357
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0021544346900318843, cost : 123.779 min
==========
At iteration 520, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  520 ; minimum lost =  0.031502433121204376 ; diff loss =  -5.029141902923584e-07 ; diff weight =  0.004494690801948309
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00014677992676220703, cost : 124.249 min
==========
At iteration 526, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  527 ; minimum lost =  0.03871177136898041 ; diff loss =  4.507601261138916e-07 ; diff weight =  0.0006986272055655718
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00031622776601683783, cost : 125.816 min
==========
At iteration 529, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  530 ; minimum lost =  0.063385508954525 ; diff loss =  9.611248970031738e-07 ; diff weight =  0.001736478297971189
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0014677992676220694, cost : 126.258 min
==========
At iteration 530, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  531 ; minimum lost =  0.028651881963014603 ; diff loss =  6.332993507385254e-08 ; diff weight =  0.005067205522209406
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 9.999999999999991e-05, cost : 127.889 min
==========
At iteration 560, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  561 ; minimum lost =  0.043210092931985855 ; diff loss =  8.046627044677734e-07 ; diff weight =  0.00023948992020450532
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00046415888336127795, cost : 133.044 min
==========
At iteration 568, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  568 ; minimum lost =  0.023813748732209206 ; diff loss =  -1.825392246246338e-07 ; diff weight =  0.008217094466090202
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 4.6415888336127784e-05, cost : 135.269 min
==========
At iteration 569, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  569 ; minimum lost =  0.01628163829445839 ; diff loss =  -6.48200511932373e-07 ; diff weight =  0.02188301458954811
lambda is : 9.999999999999997e-06, cost : 135.686 min
==========
At iteration 580, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  581 ; minimum lost =  0.01773414947092533 ; diff loss =  8.139759302139282e-07 ; diff weight =  0.0003043576143682003
lambda is : 1.4677992676220687e-05, cost : 137.574 min
==========
At iteration 633, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  633 ; minimum lost =  0.021179454401135445 ; diff loss =  -9.51811671257019e-07 ; diff weight =  0.007828771136701107
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 3.16227766016838e-05, cost : 147.98 min
==========
At iteration 667, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  668 ; minimum lost =  0.019030388444662094 ; diff loss =  5.010515451431274e-07 ; diff weight =  0.0026441023219376802
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 2.1544346900318854e-05, cost : 153.476 min
==========
*** Collecting results ***
Exporting result Dict
Leiden_3 Time elapsed: 153.5543858249982 minutes.
Shape: (71915, 25712)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for Leiden_4
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda:Lambda:Lambda:Lambda:  Lambda: Lambda: Lambda: Lambda: 1e-05 Lambda:1.5e-05 Lambda:Lambda:4.6e-05 Lambda: Lambda:2.2e-05 Lambda: Lambda:3.2e-05 Lambda:Lambda:Lambda:Lambda:0.000147 Lambda:starting at Lambda:Lambda: Lambda:Lambda:Lambda:starting at   starting at  6.8e-05  starting at  0.0001  starting at     starting at  2024-11-24 02:25:17   0.000215    2024-11-24 02:25:17 0.000316 0.000464 2024-11-24 02:25:17 0.000681 starting at 0.001 2024-11-24 02:25:17 0.001468 starting at 0.002154 2024-11-24 02:25:17 0.003162 0.004642 0.006813 0.01 2024-11-24 02:25:17 0.014678 Max_iter: 0.021544 0.031623 starting at 0.046416 0.068129 0.1 Max_iter: starting at starting at Max_iter: starting at 2024-11-24 02:25:17 starting at Max_iter: starting at 2024-11-24 02:25:17 starting at Max_iter: starting at starting at starting at starting at Max_iter: starting at 1000starting at starting at 2024-11-24 02:25:17 starting at starting at starting at 10002024-11-24 02:25:17 2024-11-24 02:25:17 10002024-11-24 02:25:17 Max_iter: 2024-11-24 02:25:17 10002024-11-24 02:25:17 Max_iter: 2024-11-24 02:25:17 10002024-11-24 02:25:17 2024-11-24 02:25:17 2024-11-24 02:25:17 2024-11-24 02:25:17 10002024-11-24 02:25:17 
2024-11-24 02:25:17 2024-11-24 02:25:17 Max_iter: 2024-11-24 02:25:17 2024-11-24 02:25:17 2024-11-24 02:25:17 
Max_iter: Max_iter: 
Max_iter: 1000Max_iter: 
Max_iter:1000Max_iter: 
Max_iter: Max_iter: Max_iter: Max_iter:
Max_iter: Max_iter: Max_iter:1000Max_iter:Max_iter:Max_iter:100010001000
1000
 1000
1000
10001000
1000 10001000 
 1000 1000 





1000

1000


1000

At iteration 135, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  136 ; minimum lost =  0.2206694781780243 ; diff loss =  9.238719940185547e-07 ; diff weight =  0.00024753808975219727
lambda is : 0.10000000000000002, cost : 32.134 min
==========
At iteration 180, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  181 ; minimum lost =  0.17663230001926422 ; diff loss =  4.023313522338867e-07 ; diff weight =  0.0006175251328386366
lambda is : 0.04641588833612786, cost : 43.162 min
==========
At iteration 181, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  182 ; minimum lost =  0.1974681317806244 ; diff loss =  9.387731552124023e-07 ; diff weight =  0.00027870258782058954
lambda is : 0.0681292069057962, cost : 43.778 min
==========
At iteration 216, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  217 ; minimum lost =  0.1552610695362091 ; diff loss =  2.384185791015625e-07 ; diff weight =  0.0001375258289044723
lambda is : 0.0316227766016838, cost : 52.097 min
==========
At iteration 339, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  340 ; minimum lost =  0.13387885689735413 ; diff loss =  5.21540641784668e-07 ; diff weight =  0.0006533936830237508
lambda is : 0.02154434690031885, cost : 80.158 min
==========
At iteration 380, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  381 ; minimum lost =  0.09803079068660736 ; diff loss =  7.897615432739258e-07 ; diff weight =  0.0006483147153630853
lambda is : 0.010000000000000004, cost : 89.898 min
==========
At iteration 417, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  418 ; minimum lost =  0.11472476273775101 ; diff loss =  9.313225746154785e-07 ; diff weight =  0.0010359176667407155
lambda is : 0.014677992676220709, cost : 99.665 min
==========
At iteration 436, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  437 ; minimum lost =  0.083747997879982 ; diff loss =  3.725290298461914e-07 ; diff weight =  0.0008981286664493382
lambda is : 0.006812920690579613, cost : 104.412 min
==========
At iteration 432, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  433 ; minimum lost =  0.06145419180393219 ; diff loss =  9.834766387939453e-07 ; diff weight =  0.001739738741889596
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.003162277660168382, cost : 104.626 min
==========
At iteration 475, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  476 ; minimum lost =  0.07160799950361252 ; diff loss =  9.462237358093262e-07 ; diff weight =  0.0010902528883889318
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.004641588833612781, cost : 112.883 min
==========
At iteration 497, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  498 ; minimum lost =  0.046211086213588715 ; diff loss =  9.5367431640625e-07 ; diff weight =  0.0019969556014984846
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0014677992676220694, cost : 118.351 min
==========
At iteration 504, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  505 ; minimum lost =  0.022126078605651855 ; diff loss =  9.797513484954834e-07 ; diff weight =  0.0035477771889418364
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 9.999999999999991e-05, cost : 120.009 min
==========
At iteration 514, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  515 ; minimum lost =  0.029560532420873642 ; diff loss =  6.854534149169922e-07 ; diff weight =  0.0018126536160707474
At iteration 502, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  503 ; minimum lost =  0.052981145679950714 ; diff loss =  6.593763828277588e-07 ; diff weight =  0.00118928961455822
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00031622776601683783, cost : 120.135 min
==========
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0021544346900318843, cost : 120.15 min
==========
At iteration 518, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  519 ; minimum lost =  0.03640548512339592 ; diff loss =  9.98377799987793e-07 ; diff weight =  0.0025526066310703754
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0006812920690579617, cost : 123.63 min
==========
At iteration 528, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  529 ; minimum lost =  0.011934640817344189 ; diff loss =  5.681067705154419e-08 ; diff weight =  0.014864964410662651
lambda is : 9.999999999999997e-06, cost : 125.992 min
==========
At iteration 536, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  537 ; minimum lost =  0.019829269498586655 ; diff loss =  5.979090929031372e-07 ; diff weight =  0.003427050309255719
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 6.81292069057961e-05, cost : 126.92 min
==========
At iteration 544, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  545 ; minimum lost =  0.04079378396272659 ; diff loss =  9.98377799987793e-07 ; diff weight =  0.004060185048729181
At iteration 534, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  535 ; minimum lost =  0.03269936144351959 ; diff loss =  8.642673492431641e-07 ; diff weight =  0.0021311070304363966
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0010000000000000002, cost : 127.041 min
==========
At iteration 537, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  538 ; minimum lost =  0.01631728745996952 ; diff loss =  8.25151801109314e-07 ; diff weight =  0.008977203629910946
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00046415888336127795, cost : 127.107 min
==========
At iteration 549, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  550 ; minimum lost =  0.012485090643167496 ; diff loss =  8.847564458847046e-07 ; diff weight =  0.000268615287495777
lambda is : 3.16227766016838e-05, cost : 127.466 min
==========
lambda is : 1.4677992676220687e-05, cost : 127.729 min
==========
At iteration 551, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  552 ; minimum lost =  0.026720616966485977 ; diff loss =  7.748603820800781e-07 ; diff weight =  0.0013415837893262506
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 129.758 min
==========
At iteration 561, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  562 ; minimum lost =  0.02418329380452633 ; diff loss =  9.96515154838562e-07 ; diff weight =  0.00035136754740960896
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00014677992676220703, cost : 130.886 min
==========
At iteration 560, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  561 ; minimum lost =  0.01396674383431673 ; diff loss =  8.102506399154663e-07 ; diff weight =  0.00044829706894233823
lambda is : 2.1544346900318854e-05, cost : 132.075 min
==========
At iteration 565, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  566 ; minimum lost =  0.01751788705587387 ; diff loss =  4.861503839492798e-07 ; diff weight =  0.0003834108356386423
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 4.6415888336127784e-05, cost : 133.043 min
==========
*** Collecting results ***
Exporting result Dict
Leiden_4 Time elapsed: 133.12265697717666 minutes.
Shape: (71915, 25712)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for Leiden_5
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda:Lambda: Lambda: 1e-05  1.5e-05 starting at 2.2e-05 starting at 2024-11-24 04:38:32starting at 2024-11-24 04:38:32  Max_iter:2024-11-24 04:38:32 Max_iter:Lambda: 1000Lambda:Lambda:Max_iter:  1000Lambda: Lambda:Lambda:
 Lambda:Lambda:Lambda: Lambda:Lambda:1000
Lambda:Lambda:
Lambda: Lambda:Lambda:6.8e-05  Lambda: Lambda:Lambda:3.2e-05 Lambda:  Lambda:Lambda: 4.6e-05      0.0001   starting at0.000147 0.000215  starting at 0.000316 0.031623  0.000464starting at0.0006810.0010.0014680.0021540.003162starting at0.0046420.006813  0.01 0.0146780.021544 0.046416starting at 0.10.068129          2024-11-24 04:38:32starting at starting atstarting at  2024-11-24 04:38:32  starting at  starting at2024-11-24 04:38:32starting atstarting at starting atstarting atstarting at2024-11-24 04:38:32starting atstarting at    starting atstarting at starting at2024-11-24 04:38:32 2024-11-24 04:38:32starting atstarting at   2024-11-24 04:38:32      Max_iter:2024-11-24 04:38:322024-11-24 04:38:322024-11-24 04:38:32  Max_iter:     2024-11-24 04:38:32Max_iter:2024-11-24 04:38:32 2024-11-24 04:38:322024-11-24 04:38:322024-11-24 04:38:32Max_iter:2024-11-24 04:38:322024-11-24 04:38:32    2024-11-24 04:38:322024-11-24 04:38:32 2024-11-24 04:38:32Max_iter:Max_iter: 2024-11-24 04:38:322024-11-24 04:38:32   Max_iter:      1000Max_iter:Max_iter:Max_iter:  1000  1000  Max_iter: 1000Max_iter: Max_iter: Max_iter:Max_iter: 1000Max_iter:Max_iter: 
   Max_iter:Max_iter:
Max_iter:1000
Max_iter:Max_iter:1000
 10001000
1000 10001000

 1000100010001000   
  



1000



1000
1000
1000
10001000


At iteration 209, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  210 ; minimum lost =  0.293232262134552 ; diff loss =  9.5367431640625e-07 ; diff weight =  0.0003100595495197922
lambda is : 0.10000000000000002, cost : 52.696 min
==========
At iteration 228, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  229 ; minimum lost =  0.2684772312641144 ; diff loss =  4.172325134277344e-07 ; diff weight =  0.00018650066340342164
lambda is : 0.0681292069057962, cost : 56.706 min
==========
At iteration 366, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  367 ; minimum lost =  0.21610039472579956 ; diff loss =  9.834766387939453e-07 ; diff weight =  0.0006041978485882282
lambda is : 0.0316227766016838, cost : 91.28 min
==========
At iteration 374, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  375 ; minimum lost =  0.24104845523834229 ; diff loss =  9.834766387939453e-07 ; diff weight =  0.0004783972690347582
lambda is : 0.04641588833612786, cost : 91.796 min
==========
At iteration 401, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  402 ; minimum lost =  0.19222919642925262 ; diff loss =  3.725290298461914e-07 ; diff weight =  0.00028353332891128957
lambda is : 0.02154434690031885, cost : 99.256 min
==========
At iteration 414, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  415 ; minimum lost =  0.13507701456546783 ; diff loss =  9.98377799987793e-07 ; diff weight =  0.0012427179608494043
lambda is : 0.006812920690579613, cost : 101.292 min
==========
At iteration 414, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  415 ; minimum lost =  0.1707209348678589 ; diff loss =  7.599592208862305e-07 ; diff weight =  0.00045790255535393953
lambda is : 0.014677992676220709, cost : 101.674 min
==========
At iteration 433, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  434 ; minimum lost =  0.09907829761505127 ; diff loss =  9.685754776000977e-07 ; diff weight =  0.003253431059420109
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0021544346900318843, cost : 105.553 min
==========
At iteration 433, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  434 ; minimum lost =  0.12086603045463562 ; diff loss =  7.748603820800781e-07 ; diff weight =  0.002117566531524062
lambda is : 0.004641588833612781, cost : 105.862 min
==========
At iteration 437, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  438 ; minimum lost =  0.15169569849967957 ; diff loss =  7.450580596923828e-07 ; diff weight =  0.000556279846932739
lambda is : 0.010000000000000004, cost : 106.862 min
==========
At iteration 459, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  460 ; minimum lost =  0.10902167856693268 ; diff loss =  7.152557373046875e-07 ; diff weight =  0.0007897521136328578
lambda is : 0.003162277660168382, cost : 111.755 min
==========
At iteration 468, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  469 ; minimum lost =  0.09044285863637924 ; diff loss =  9.164214134216309e-07 ; diff weight =  0.0011211942182853818
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0014677992676220694, cost : 113.079 min
==========
At iteration 477, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  478 ; minimum lost =  0.08292412012815475 ; diff loss =  9.387731552124023e-07 ; diff weight =  0.001235370640642941
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0010000000000000002, cost : 114.695 min
==========
At iteration 482, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  483 ; minimum lost =  0.04901856929063797 ; diff loss =  8.791685104370117e-07 ; diff weight =  0.0003376878739800304
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 4.6415888336127784e-05, cost : 116.702 min
==========
At iteration 485, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  486 ; minimum lost =  0.03830339014530182 ; diff loss =  9.685754776000977e-07 ; diff weight =  0.00031133717857301235
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 9.999999999999997e-06, cost : 117.974 min
==========
At iteration 490, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  491 ; minimum lost =  0.05469111353158951 ; diff loss =  9.648501873016357e-07 ; diff weight =  0.002154767280444503
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 9.999999999999991e-05, cost : 118.696 min
==========
At iteration 493, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  494 ; minimum lost =  0.04612724483013153 ; diff loss =  7.003545761108398e-07 ; diff weight =  0.00020840729121118784
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 3.16227766016838e-05, cost : 120.103 min
==========
At iteration 493, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  494 ; minimum lost =  0.06590345501899719 ; diff loss =  3.427267074584961e-07 ; diff weight =  0.0016564300749450922
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00031622776601683783, cost : 120.407 min
==========
At iteration 498, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  499 ; minimum lost =  0.07068014144897461 ; diff loss =  6.109476089477539e-07 ; diff weight =  0.0002095252857543528
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00046415888336127795, cost : 120.905 min
==========
At iteration 502, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  503 ; minimum lost =  0.06164807453751564 ; diff loss =  2.5704503059387207e-07 ; diff weight =  0.0025866085197776556
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 121.282 min
==========
At iteration 500, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  501 ; minimum lost =  0.0579022578895092 ; diff loss =  3.725290298461914e-07 ; diff weight =  0.0003959500463679433
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00014677992676220703, cost : 121.611 min
==========
At iteration 512, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  512 ; minimum lost =  0.051564257591962814 ; diff loss =  -4.246830940246582e-07 ; diff weight =  0.0032118097878992558
At iteration 513, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  514 ; minimum lost =  0.07631032168865204 ; diff loss =  9.387731552124023e-07 ; diff weight =  0.0012415084056556225
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0006812920690579617, cost : 123.833 min
==========
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 6.81292069057961e-05, cost : 123.839 min
==========
At iteration 526, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  527 ; minimum lost =  0.04316439479589462 ; diff loss =  8.940696716308594e-07 ; diff weight =  0.00029801425989717245
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 2.1544346900318854e-05, cost : 126.329 min
==========
At iteration 552, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  553 ; minimum lost =  0.03990006446838379 ; diff loss =  5.62518835067749e-07 ; diff weight =  0.00034523161593824625
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 1.4677992676220687e-05, cost : 130.602 min
==========
*** Collecting results ***
Exporting result Dict
Leiden_5 Time elapsed: 130.72080665429434 minutes.
Shape: (71915, 25712)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for Leiden_6
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda:Lambda:Lambda:  Lambda: Lambda:Lambda:1e-05 Lambda:Lambda: Lambda:1.5e-05  2.2e-05Lambda:Lambda: Lambda:Lambda: Lambda:Lambda:starting at Lambda: Lambda:0.0001 Lambda: Lambda:Lambda:starting at Lambda:Lambda:3.2e-05  starting at Lambda: Lambda:Lambda:4.6e-05   6.8e-05   2024-11-24 06:49:24  0.000147  starting at  0.000215   2024-11-24 06:49:24   starting at  2024-11-24 06:49:240.014678  0.000316   starting at 0.068129 0.000681 starting at 0.000464 0.001 Max_iter: 0.001468 starting at 0.002154 2024-11-24 06:49:24 0.003162 starting at 0.004642 0.006813 Max_iter: 0.01 0.021544 2024-11-24 06:49:24  Max_iter:starting at 0.046416 starting at 0.031623 0.1 2024-11-24 06:49:24 starting at starting at 2024-11-24 06:49:24 starting at starting at 1000starting at 2024-11-24 06:49:24 starting at Max_iter: starting at 2024-11-24 06:49:24 starting at starting at 1000starting at starting at Max_iter:  10002024-11-24 06:49:24 starting at 2024-11-24 06:49:24 starting at starting at 2024-11-24 06:49:24 Max_iter: 2024-11-24 06:49:24 2024-11-24 06:49:24 Max_iter: 2024-11-24 06:49:24 2024-11-24 06:49:24 
2024-11-24 06:49:24 Max_iter: 2024-11-24 06:49:24 10002024-11-24 06:49:24 Max_iter: 2024-11-24 06:49:24 2024-11-24 06:49:24 
2024-11-24 06:49:242024-11-24 06:49:24 1000
Max_iter: 2024-11-24 06:49:24 Max_iter: 2024-11-24 06:49:24 Max_iter: 1000Max_iter: Max_iter: 1000Max_iter: Max_iter: Max_iter:1000Max_iter: 
Max_iter:1000Max_iter:Max_iter:  Max_iter:
1000Max_iter:1000Max_iter:1000
10001000
10001000
 
1000 
 1000Max_iter: 
 
 



1000

1000
1000

 10001000
1000
1000

At iteration 245, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  246 ; minimum lost =  0.26940375566482544 ; diff loss =  5.066394805908203e-07 ; diff weight =  0.0001857071474660188
lambda is : 0.10000000000000002, cost : 67.843 min
==========
At iteration 285, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  286 ; minimum lost =  0.23705990612506866 ; diff loss =  4.76837158203125e-07 ; diff weight =  0.0003656433545984328
lambda is : 0.0681292069057962, cost : 77.833 min
==========
At iteration 285, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  286 ; minimum lost =  0.19936984777450562 ; diff loss =  9.5367431640625e-07 ; diff weight =  0.0003845880855806172
lambda is : 0.04641588833612786, cost : 78.166 min
==========
At iteration 326, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  327 ; minimum lost =  0.08442927896976471 ; diff loss =  5.364418029785156e-07 ; diff weight =  0.0006149691180326045
lambda is : 0.010000000000000004, cost : 89.345 min
==========
At iteration 332, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  333 ; minimum lost =  0.16306054592132568 ; diff loss =  9.5367431640625e-07 ; diff weight =  0.0004116495547350496
lambda is : 0.0316227766016838, cost : 90.381 min
==========
At iteration 338, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  339 ; minimum lost =  0.10573378205299377 ; diff loss =  4.470348358154297e-08 ; diff weight =  0.00029089339659549296
lambda is : 0.014677992676220709, cost : 92.41 min
==========
At iteration 353, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  354 ; minimum lost =  0.13178622722625732 ; diff loss =  9.238719940185547e-07 ; diff weight =  0.0009902844903990626
lambda is : 0.02154434690031885, cost : 96.27 min
==========
At iteration 383, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  384 ; minimum lost =  0.06746695190668106 ; diff loss =  9.387731552124023e-07 ; diff weight =  0.0014928097371011972
lambda is : 0.006812920690579613, cost : 103.005 min
==========
At iteration 393, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  394 ; minimum lost =  0.043816372752189636 ; diff loss =  9.760260581970215e-07 ; diff weight =  0.0016797605203464627
lambda is : 0.003162277660168382, cost : 105.097 min
==========
At iteration 406, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  407 ; minimum lost =  0.03558124229311943 ; diff loss =  9.350478649139404e-07 ; diff weight =  0.0012144602369517088
lambda is : 0.0021544346900318843, cost : 108.224 min
==========
At iteration 419, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  420 ; minimum lost =  0.05420766770839691 ; diff loss =  8.158385753631592e-07 ; diff weight =  0.0012853613588958979
lambda is : 0.004641588833612781, cost : 110.465 min
==========
At iteration 438, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  439 ; minimum lost =  0.024376623332500458 ; diff loss =  6.854534149169922e-07 ; diff weight =  0.001890863524749875
lambda is : 0.0010000000000000002, cost : 115.089 min
==========
At iteration 453, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  454 ; minimum lost =  0.029246613383293152 ; diff loss =  9.96515154838562e-07 ; diff weight =  0.002159310970455408
lambda is : 0.0014677992676220694, cost : 118.006 min
==========
At iteration 471, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  472 ; minimum lost =  0.020541545003652573 ; diff loss =  8.884817361831665e-07 ; diff weight =  0.002677116310223937
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0006812920690579617, cost : 122.315 min
==========
At iteration 494, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  495 ; minimum lost =  0.017589449882507324 ; diff loss =  9.760260581970215e-07 ; diff weight =  0.0030319513753056526
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00046415888336127795, cost : 126.758 min
==========
At iteration 507, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  508 ; minimum lost =  0.015253614634275436 ; diff loss =  9.015202522277832e-07 ; diff weight =  0.0031742295250296593
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00031622776601683783, cost : 130.973 min
==========
At iteration 516, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  517 ; minimum lost =  0.013340226374566555 ; diff loss =  9.415671229362488e-07 ; diff weight =  0.007803510874509811
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 133.257 min
==========
At iteration 536, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  537 ; minimum lost =  0.011756033636629581 ; diff loss =  9.890645742416382e-07 ; diff weight =  0.003730439580976963
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00014677992676220703, cost : 135.834 min
==========
At iteration 573, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  574 ; minimum lost =  0.010389411821961403 ; diff loss =  8.391216397285461e-07 ; diff weight =  0.00429461058229208
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 9.999999999999991e-05, cost : 144.507 min
==========
At iteration 591, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  592 ; minimum lost =  0.00799309741705656 ; diff loss =  3.9301812648773193e-07 ; diff weight =  0.0007924835081212223
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 4.6415888336127784e-05, cost : 147.984 min
==========
At iteration 588, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  589 ; minimum lost =  0.009202459827065468 ; diff loss =  9.154900908470154e-07 ; diff weight =  0.0014013868058100343
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 6.81292069057961e-05, cost : 148.879 min
==========
At iteration 586, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  587 ; minimum lost =  0.005874074995517731 ; diff loss =  9.471550583839417e-07 ; diff weight =  0.0032075494527816772
lambda is : 2.1544346900318854e-05, cost : 149.171 min
==========
At iteration 595, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  596 ; minimum lost =  0.004149339161813259 ; diff loss =  7.636845111846924e-07 ; diff weight =  0.002336460631340742
lambda is : 9.999999999999997e-06, cost : 150.183 min
==========
At iteration 617, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  618 ; minimum lost =  0.004768839105963707 ; diff loss =  5.280598998069763e-07 ; diff weight =  0.012767148204147816
lambda is : 1.4677992676220687e-05, cost : 153.06 min
==========
At iteration 619, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  620 ; minimum lost =  0.006805233657360077 ; diff loss =  6.356276571750641e-07 ; diff weight =  0.000985618564300239
lambda is : 3.16227766016838e-05, cost : 153.889 min
==========
*** Collecting results ***
Exporting result Dict
Leiden_6 Time elapsed: 153.97734118700026 minutes.
Shape: (71915, 25712)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for Leiden_7
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda: Lambda:Lambda:Lambda:Lambda:Lambda:1e-05 Lambda:Lambda: Lambda:Lambda: Lambda:  Lambda:Lambda: Lambda: Lambda:Lambda:starting at  Lambda: Lambda:1.5e-05  Lambda:2.2e-05  Lambda:0.000215 3.2e-05 Lambda: Lambda:0.000316 Lambda:Lambda: Lambda:6.8e-05   2024-11-24 09:23:31 0.0001  4.6e-05   starting at0.003162 0.000147  starting at 0.006813  starting at starting at  0.014678  starting at   0.000464  starting at 0.000681 0.001 Max_iter: starting at 0.001468 starting at 0.002154  2024-11-24 09:23:31starting at starting at 0.004642 2024-11-24 09:23:31 starting at 0.01 2024-11-24 09:23:31 2024-11-24 09:23:31 0.021544 starting at 0.031623 2024-11-24 09:23:31 0.046416 0.068129starting at 0.1 2024-11-24 09:23:31 starting at starting at 10002024-11-24 09:23:31 starting at 2024-11-24 09:23:31 starting at  Max_iter:2024-11-24 09:23:31 2024-11-24 09:23:31 starting at Max_iter: 2024-11-24 09:23:31 starting at Max_iter: Max_iter: starting at 2024-11-24 09:23:31 starting at Max_iter: starting at  2024-11-24 09:23:31 starting at Max_iter: 2024-11-24 09:23:31 2024-11-24 09:23:31 
Max_iter: 2024-11-24 09:23:31 Max_iter: 2024-11-24 09:23:31  1000Max_iter:Max_iter: 2024-11-24 09:23:31 1000
Max_iter: 2024-11-24 09:23:31 1000
1000
2024-11-24 09:23:31Max_iter:2024-11-24 09:23:3110002024-11-24 09:23:31 starting atMax_iter: 2024-11-24 09:23:311000Max_iter:Max_iter: 1000Max_iter: 1000Max_iter:
 1000Max_iter: 1000Max_iter:   
Max_iter: 1000 
 1000
1000
 1000
1000
 Max_iter:1000Max_iter: 2024-11-24 09:23:31
Max_iter:1000

1000

1000 
 10001000  


1000

Max_iter: 1000
1000

At iteration 184, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  185 ; minimum lost =  0.2667325437068939 ; diff loss =  3.5762786865234375e-07 ; diff weight =  7.24785277270712e-05
lambda is : 0.10000000000000002, cost : 47.705 min
==========
At iteration 227, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  228 ; minimum lost =  0.24386656284332275 ; diff loss =  6.705522537231445e-07 ; diff weight =  0.00036007887683808804
lambda is : 0.0681292069057962, cost : 59.219 min
==========
At iteration 299, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  300 ; minimum lost =  0.2045077085494995 ; diff loss =  8.791685104370117e-07 ; diff weight =  0.0006341833504848182
lambda is : 0.0316227766016838, cost : 80.28 min
==========
At iteration 304, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  305 ; minimum lost =  0.22400103509426117 ; diff loss =  8.046627044677734e-07 ; diff weight =  0.0003669843717943877
lambda is : 0.04641588833612786, cost : 81.469 min
==========
At iteration 321, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  322 ; minimum lost =  0.18609844148159027 ; diff loss =  8.791685104370117e-07 ; diff weight =  0.0004875218728557229
lambda is : 0.02154434690031885, cost : 86.828 min
==========
At iteration 381, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  382 ; minimum lost =  0.1693013459444046 ; diff loss =  9.387731552124023e-07 ; diff weight =  0.0008413629839196801
lambda is : 0.014677992676220709, cost : 100.69 min
==========
At iteration 382, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  383 ; minimum lost =  0.15374164283275604 ; diff loss =  9.834766387939453e-07 ; diff weight =  0.0007993683102540672
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.010000000000000004, cost : 101.427 min
==========
At iteration 390, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  390 ; minimum lost =  0.10770820081233978 ; diff loss =  0.0 ; diff weight =  0.012852100655436516
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0021544346900318843, cost : 102.858 min
==========
At iteration 401, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  402 ; minimum lost =  0.07627025246620178 ; diff loss =  9.909272193908691e-07 ; diff weight =  0.0004282942973077297
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00031622776601683783, cost : 105.624 min
==========
At iteration 414, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  415 ; minimum lost =  0.12740392982959747 ; diff loss =  8.493661880493164e-07 ; diff weight =  0.0008870675810612738
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.004641588833612781, cost : 107.559 min
==========
At iteration 410, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  411 ; minimum lost =  0.13977651298046112 ; diff loss =  9.387731552124023e-07 ; diff weight =  0.0009058499708771706
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.006812920690579613, cost : 107.77 min
==========
At iteration 421, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  421 ; minimum lost =  0.05604618042707443 ; diff loss =  -2.3469328880310059e-07 ; diff weight =  0.006099933758378029
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 3.16227766016838e-05, cost : 110.076 min
==========
At iteration 431, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  432 ; minimum lost =  0.08619705587625504 ; diff loss =  9.909272193908691e-07 ; diff weight =  0.0015641875797882676
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0006812920690579617, cost : 111.915 min
==========
At iteration 435, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  436 ; minimum lost =  0.053080469369888306 ; diff loss =  8.381903171539307e-07 ; diff weight =  0.00013376481365412474
At iteration 437, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  438 ; minimum lost =  0.06180500239133835 ; diff loss =  8.866190910339355e-07 ; diff weight =  0.00012342266563791782
At iteration 438, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  439 ; minimum lost =  0.08076957613229752 ; diff loss =  6.780028343200684e-07 ; diff weight =  0.00017515654326416552
At iteration 440, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  441 ; minimum lost =  0.0923614352941513 ; diff loss =  7.003545761108398e-07 ; diff weight =  0.0010717937257140875
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 2.1544346900318854e-05, cost : 113.208 min
==========
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0010000000000000002, cost : 113.275 min
==========
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00046415888336127795, cost : 113.312 min
==========
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 6.81292069057961e-05, cost : 113.371 min
==========
At iteration 449, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  450 ; minimum lost =  0.11675043404102325 ; diff loss =  9.834766387939453e-07 ; diff weight =  0.0011298579629510641
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.003162277660168382, cost : 116.005 min
==========
At iteration 451, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  452 ; minimum lost =  0.09944161772727966 ; diff loss =  9.760260581970215e-07 ; diff weight =  0.001185487606562674
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0014677992676220694, cost : 116.452 min
==========
At iteration 450, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  451 ; minimum lost =  0.04939654842019081 ; diff loss =  8.903443813323975e-07 ; diff weight =  0.00023907692229840904
At iteration 455, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  455 ; minimum lost =  0.05833974480628967 ; diff loss =  -8.083879947662354e-07 ; diff weight =  0.0036716079339385033
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 4.6415888336127784e-05, cost : 116.812 min
==========
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 1.4677992676220687e-05, cost : 116.966 min
==========
At iteration 467, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  468 ; minimum lost =  0.06458307802677155 ; diff loss =  7.227063179016113e-07 ; diff weight =  0.00014420985826291144
At iteration 472, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  473 ; minimum lost =  0.07176660746335983 ; diff loss =  3.8743019104003906e-07 ; diff weight =  0.002284071873873472
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 9.999999999999991e-05, cost : 119.418 min
==========
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 119.589 min
==========
At iteration 475, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  475 ; minimum lost =  0.06796030700206757 ; diff loss =  -4.470348358154297e-07 ; diff weight =  0.0021288637071847916
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00014677992676220703, cost : 121.769 min
==========
At iteration 547, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  548 ; minimum lost =  0.04487652704119682 ; diff loss =  7.636845111846924e-07 ; diff weight =  0.00932261161506176
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 9.999999999999997e-06, cost : 135.59 min
==========
*** Collecting results ***
Exporting result Dict
Leiden_7 Time elapsed: 135.68537655671437 minutes.
Shape: (71915, 25712)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for Leiden_8
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda:Lambda:Lambda:Lambda: Lambda: Lambda:   Lambda:1e-05 Lambda: Lambda:Lambda:1.5e-05 Lambda:Lambda: Lambda:4.6e-05 Lambda:2.2e-05 Lambda:6.8e-05 Lambda: Lambda:Lambda:starting at Lambda: 3.2e-05 Lambda:  Lambda:Lambda:starting at Lambda: Lambda: Lambda:0.000464  starting at  starting at  starting at  0.0001   2024-11-24 11:39:20  0.000147 starting at  0.01 0.000215   2024-11-24 11:39:20  0.000316 0.1  starting at 0.0006812024-11-24 11:39:20 0.001 2024-11-24 11:39:20 0.001468 2024-11-24 11:39:20 0.002154 starting at 0.003162 0.004642Max_iter: 0.006813 starting at 2024-11-24 11:39:20 0.014678starting at starting at 0.021544 0.031623Max_iter: 0.046416starting at starting at 0.068129 2024-11-24 11:39:20  Max_iter: starting at Max_iter: starting at Max_iter: starting at 2024-11-24 11:39:20 starting at  starting at1000starting at 2024-11-24 11:39:20 Max_iter:  2024-11-24 11:39:20 2024-11-24 11:39:20 starting at  starting at1000 starting at2024-11-24 11:39:20 2024-11-24 11:39:20 starting at Max_iter: starting at 10002024-11-24 11:39:20 10002024-11-24 11:39:20 10002024-11-24 11:39:20 Max_iter: 2024-11-24 11:39:20  2024-11-24 11:39:20
2024-11-24 11:39:20 Max_iter: 1000starting atMax_iter: Max_iter: 2024-11-24 11:39:20  2024-11-24 11:39:20
 2024-11-24 11:39:20Max_iter: Max_iter: 2024-11-24 11:39:20 10002024-11-24 11:39:20 
Max_iter: 
Max_iter: 
Max_iter: 1000Max_iter: Max_iter:Max_iter:1000
 2024-11-24 11:39:201000
1000
Max_iter:  Max_iter: Max_iter:10001000
Max_iter: 
Max_iter:100010001000
   1000
 Max_iter:1000  1000
1000 1000


1000
1000
 1000
1000





At iteration 151, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  152 ; minimum lost =  0.2311767339706421 ; diff loss =  9.5367431640625e-07 ; diff weight =  0.0003017479320988059
lambda is : 0.10000000000000002, cost : 38.387 min
==========
At iteration 209, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  210 ; minimum lost =  0.20657211542129517 ; diff loss =  7.599592208862305e-07 ; diff weight =  3.893981920555234e-05
lambda is : 0.0681292069057962, cost : 53.462 min
==========
At iteration 215, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  216 ; minimum lost =  0.18624547123908997 ; diff loss =  7.897615432739258e-07 ; diff weight =  0.00037376699037849903
lambda is : 0.04641588833612786, cost : 53.955 min
==========
At iteration 316, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  317 ; minimum lost =  0.139745831489563 ; diff loss =  7.450580596923828e-07 ; diff weight =  0.0005953884683549404
lambda is : 0.02154434690031885, cost : 78.672 min
==========
At iteration 354, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  355 ; minimum lost =  0.08479747921228409 ; diff loss =  9.909272193908691e-07 ; diff weight =  0.0009428291232325137
lambda is : 0.006812920690579613, cost : 87.661 min
==========
At iteration 367, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  368 ; minimum lost =  0.16321587562561035 ; diff loss =  7.301568984985352e-07 ; diff weight =  0.0006098365411162376
lambda is : 0.0316227766016838, cost : 89.867 min
==========
At iteration 378, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  379 ; minimum lost =  0.11825381219387054 ; diff loss =  9.685754776000977e-07 ; diff weight =  0.0007774006808176637
lambda is : 0.014677992676220709, cost : 92.427 min
==========
At iteration 382, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  383 ; minimum lost =  0.09988029301166534 ; diff loss =  9.909272193908691e-07 ; diff weight =  0.0009188820258714259
lambda is : 0.010000000000000004, cost : 94.592 min
==========
At iteration 401, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  402 ; minimum lost =  0.06198107451200485 ; diff loss =  9.760260581970215e-07 ; diff weight =  0.0018737154314294457
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.003162277660168382, cost : 97.684 min
==========
At iteration 419, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  420 ; minimum lost =  0.07225996255874634 ; diff loss =  8.493661880493164e-07 ; diff weight =  0.0013221793342381716
lambda is : 0.004641588833612781, cost : 101.814 min
==========
At iteration 426, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  427 ; minimum lost =  0.046317845582962036 ; diff loss =  9.909272193908691e-07 ; diff weight =  0.002198826288804412
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0014677992676220694, cost : 102.767 min
==========
At iteration 430, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  431 ; minimum lost =  0.05344117060303688 ; diff loss =  9.98377799987793e-07 ; diff weight =  0.001875191810540855
lambda is : 0.0021544346900318843, cost : 103.494 min
==========
At iteration 439, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  440 ; minimum lost =  0.04032740741968155 ; diff loss =  8.568167686462402e-07 ; diff weight =  0.001940879737958312
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0010000000000000002, cost : 106.961 min
==========
At iteration 448, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  449 ; minimum lost =  0.03532085195183754 ; diff loss =  9.872019290924072e-07 ; diff weight =  0.0018482442246749997
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0006812920690579617, cost : 108.51 min
==========
At iteration 478, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  479 ; minimum lost =  0.016054527834057808 ; diff loss =  9.629875421524048e-07 ; diff weight =  0.0006511983810923994
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 4.6415888336127784e-05, cost : 116.267 min
==========
At iteration 483, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  484 ; minimum lost =  0.014455044642090797 ; diff loss =  9.406358003616333e-07 ; diff weight =  0.0004985735286027193
lambda is : 3.16227766016838e-05, cost : 117.197 min
==========
At iteration 486, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  487 ; minimum lost =  0.031132590025663376 ; diff loss =  9.611248970031738e-07 ; diff weight =  0.0025225523859262466
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00046415888336127795, cost : 117.57 min
==========
At iteration 487, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  488 ; minimum lost =  0.027659481391310692 ; diff loss =  9.927898645401e-07 ; diff weight =  0.0019135090988129377
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00031622776601683783, cost : 118.084 min
==========
At iteration 503, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  504 ; minimum lost =  0.022073931992053986 ; diff loss =  4.917383193969727e-07 ; diff weight =  0.0013715808745473623
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00014677992676220703, cost : 119.881 min
==========
At iteration 512, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  513 ; minimum lost =  0.019739937037229538 ; diff loss =  2.7939677238464355e-07 ; diff weight =  0.00277902465313673
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 9.999999999999991e-05, cost : 122.741 min
==========
At iteration 515, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  515 ; minimum lost =  0.012527784332633018 ; diff loss =  -5.979090929031372e-07 ; diff weight =  0.009965755976736546
lambda is : 2.1544346900318854e-05, cost : 123.707 min
==========
At iteration 527, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  528 ; minimum lost =  0.024628693237900734 ; diff loss =  5.606561899185181e-07 ; diff weight =  0.001830619527027011
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 124.208 min
==========
At iteration 519, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  520 ; minimum lost =  0.017615942284464836 ; diff loss =  3.781169652938843e-07 ; diff weight =  0.003547591855749488
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 6.81292069057961e-05, cost : 124.551 min
==========
At iteration 533, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  534 ; minimum lost =  0.009549057111144066 ; diff loss =  7.487833499908447e-07 ; diff weight =  0.01289546862244606
lambda is : 9.999999999999997e-06, cost : 128.048 min
==========
At iteration 624, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  624 ; minimum lost =  0.010296940803527832 ; diff loss =  -2.5890767574310303e-07 ; diff weight =  0.007266018074005842
lambda is : 1.4677992676220687e-05, cost : 145.204 min
==========
*** Collecting results ***
Exporting result Dict
Leiden_8 Time elapsed: 145.2964989423752 minutes.
Shape: (71915, 25712)
TYPE <class 'scipy.sparse._csr.csr_matrix'>
====================
Starting job for Leiden_9
[-11.51292546 -11.12916128 -10.7453971  -10.36163292  -9.97786874
  -9.59410455  -9.21034037  -8.82657619  -8.44281201  -8.05904783
  -7.67528364  -7.29151946  -6.90775528  -6.5239911   -6.14022691
  -5.75646273  -5.37269855  -4.98893437  -4.60517019  -4.221406
  -3.83764182  -3.45387764  -3.07011346  -2.68634928  -2.30258509]
[1.00000000e-05 1.46779927e-05 2.15443469e-05 3.16227766e-05
 4.64158883e-05 6.81292069e-05 1.00000000e-04 1.46779927e-04
 2.15443469e-04 3.16227766e-04 4.64158883e-04 6.81292069e-04
 1.00000000e-03 1.46779927e-03 2.15443469e-03 3.16227766e-03
 4.64158883e-03 6.81292069e-03 1.00000000e-02 1.46779927e-02
 2.15443469e-02 3.16227766e-02 4.64158883e-02 6.81292069e-02
 1.00000000e-01]
Alpha: 0.01
Loss tolerance: 1e-06
*** Start parallel lambda tuning ***
Lambda:Lambda: Lambda: 1e-05 Lambda: Lambda:Lambda:Lambda:1.5e-05Lambda:Lambda:Lambda:Lambda:starting at  Lambda:2.2e-05 Lambda:  Lambda: Lambda: starting atLambda:  Lambda: Lambda: Lambda:2024-11-24 14:04:46 Lambda:3.2e-05 Lambda: Lambda:Lambda:starting at Lambda:Lambda: 4.6e-05 0.000316  6.8e-05   2024-11-24 14:04:46 0.0001 0.002154  0.000147 0.004642  Max_iter:  starting at  0.000215   2024-11-24 14:04:46   0.000464 starting at starting at 0.000681 starting at 0.001  Max_iter:0.001468 starting at starting at 0.003162 starting at0.006813 starting at 0.01 10000.014678 2024-11-24 14:04:46 0.021544starting at 0.031623 0.046416 Max_iter: 0.10.068129 starting at 2024-11-24 14:04:46 2024-11-24 14:04:46 starting at 2024-11-24 14:04:46 starting at  1000starting at 2024-11-24 14:04:46 2024-11-24 14:04:46  starting at 2024-11-24 14:04:46starting at 2024-11-24 14:04:46 starting at 
starting at Max_iter:  starting at2024-11-24 14:04:46 starting at starting at 1000 starting atstarting at 2024-11-24 14:04:46 Max_iter: Max_iter: 2024-11-24 14:04:46 Max_iter: 2024-11-24 14:04:46 
2024-11-24 14:04:46 Max_iter: Max_iter:  2024-11-24 14:04:46 Max_iter:2024-11-24 14:04:46 Max_iter: 2024-11-24 14:04:46 2024-11-24 14:04:46 1000 2024-11-24 14:04:46Max_iter: 2024-11-24 14:04:46 2024-11-24 14:04:46 
 2024-11-24 14:04:462024-11-24 14:04:46 Max_iter: 10001000Max_iter: 1000Max_iter: Max_iter:10001000  Max_iter: 1000Max_iter: Max_iter:
 1000Max_iter: Max_iter:  Max_iter:Max_iter: 1000

1000
1000 1000

Max_iter:10001000
1000 Max_iter: 
10001000 1000



 


10001000

1000
1000



At iteration 205, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  206 ; minimum lost =  0.22247551381587982 ; diff loss =  1.9371509552001953e-07 ; diff weight =  8.552491635782644e-05
lambda is : 0.10000000000000002, cost : 52.507 min
==========
At iteration 271, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  271 ; minimum lost =  0.19893966615200043 ; diff loss =  0.0 ; diff weight =  9.299581870436668e-05
lambda is : 0.0681292069057962, cost : 68.69 min
==========
At iteration 293, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  294 ; minimum lost =  0.1816665530204773 ; diff loss =  8.493661880493164e-07 ; diff weight =  0.0008974595693871379
lambda is : 0.04641588833612786, cost : 74.112 min
==========
At iteration 305, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  306 ; minimum lost =  0.14290180802345276 ; diff loss =  7.152557373046875e-07 ; diff weight =  0.0015239506028592587
lambda is : 0.014677992676220709, cost : 76.481 min
==========
At iteration 309, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  310 ; minimum lost =  0.12935176491737366 ; diff loss =  9.98377799987793e-07 ; diff weight =  0.0005223373300395906
lambda is : 0.010000000000000004, cost : 77.243 min
==========
At iteration 361, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  362 ; minimum lost =  0.1169964075088501 ; diff loss =  9.98377799987793e-07 ; diff weight =  0.0006091106915846467
lambda is : 0.006812920690579613, cost : 90.396 min
==========
At iteration 364, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  365 ; minimum lost =  0.15687288343906403 ; diff loss =  6.556510925292969e-07 ; diff weight =  0.00041792046977207065
lambda is : 0.02154434690031885, cost : 90.889 min
==========
At iteration 370, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  371 ; minimum lost =  0.1691010445356369 ; diff loss =  9.98377799987793e-07 ; diff weight =  0.0008944837609305978
lambda is : 0.0316227766016838, cost : 91.876 min
==========
At iteration 399, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  399 ; minimum lost =  0.043525680899620056 ; diff loss =  -1.9744038581848145e-07 ; diff weight =  0.013102908618748188
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 9.999999999999991e-05, cost : 100.039 min
==========
At iteration 439, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  440 ; minimum lost =  0.04612920060753822 ; diff loss =  9.909272193908691e-07 ; diff weight =  0.0005336511530913413
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00014677992676220703, cost : 106.783 min
==========
At iteration 442, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  443 ; minimum lost =  0.09500762820243835 ; diff loss =  8.940696716308594e-07 ; diff weight =  0.001626373385079205
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.003162277660168382, cost : 109.003 min
==========
At iteration 443, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  444 ; minimum lost =  0.10552565008401871 ; diff loss =  7.972121238708496e-07 ; diff weight =  0.0023956072982400656
lambda is : 0.004641588833612781, cost : 109.047 min
==========
At iteration 451, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  452 ; minimum lost =  0.03378800302743912 ; diff loss =  8.977949619293213e-07 ; diff weight =  0.00015450271894223988
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 3.16227766016838e-05, cost : 110.491 min
==========
At iteration 458, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  459 ; minimum lost =  0.049331292510032654 ; diff loss =  9.015202522277832e-07 ; diff weight =  0.0002501373819541186
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0002154434690031884, cost : 111.908 min
==========
At iteration 462, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  463 ; minimum lost =  0.08561491966247559 ; diff loss =  7.674098014831543e-07 ; diff weight =  0.0016086767427623272
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0021544346900318843, cost : 112.176 min
==========
At iteration 462, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  463 ; minimum lost =  0.05814804881811142 ; diff loss =  5.513429641723633e-07 ; diff weight =  0.0018656300380825996
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00046415888336127795, cost : 112.383 min
==========
At iteration 462, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  463 ; minimum lost =  0.06991414725780487 ; diff loss =  5.289912223815918e-07 ; diff weight =  0.0004827047523576766
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0010000000000000002, cost : 114.257 min
==========
At iteration 484, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  485 ; minimum lost =  0.035702917724847794 ; diff loss =  8.381903171539307e-07 ; diff weight =  0.00039639833266846836
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 4.6415888336127784e-05, cost : 117.003 min
==========
At iteration 481, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  482 ; minimum lost =  0.07718357443809509 ; diff loss =  9.238719940185547e-07 ; diff weight =  0.0015837331302464008
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0014677992676220694, cost : 117.412 min
==========
At iteration 498, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  499 ; minimum lost =  0.06341623514890671 ; diff loss =  2.8312206268310547e-07 ; diff weight =  0.0002544475137256086
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.0006812920690579617, cost : 120.067 min
==========
At iteration 510, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  511 ; minimum lost =  0.05301225557923317 ; diff loss =  2.0489096641540527e-07 ; diff weight =  0.00023282533220481128
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 0.00031622776601683783, cost : 121.806 min
==========
At iteration 516, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  517 ; minimum lost =  0.028507636860013008 ; diff loss =  9.406358003616333e-07 ; diff weight =  0.0007121737580746412
At iteration 517, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  518 ; minimum lost =  0.03858643397688866 ; diff loss =  9.611248970031738e-07 ; diff weight =  0.0005451274337247014
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 2.1544346900318854e-05, cost : 125.119 min
==========
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 6.81292069057961e-05, cost : 125.554 min
==========
At iteration 593, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  594 ; minimum lost =  0.02538209781050682 ; diff loss =  9.499490261077881e-07 ; diff weight =  0.0007791934767737985
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 1.4677992676220687e-05, cost : 138.639 min
==========
At iteration 665, Convergence with loss difference, Device: cpu
Convergence with loss difference, Device: cpu
minimum epoch =  666 ; minimum lost =  0.021515777334570885 ; diff loss =  9.331852197647095e-07 ; diff weight =  0.00035763398045673966
/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
lambda is : 9.999999999999997e-06, cost : 154.15 min
==========
*** Collecting results ***
Exporting result Dict
Leiden_9 Time elapsed: 154.24493463039397 minutes.
***** Finished lambda tuning
====================
